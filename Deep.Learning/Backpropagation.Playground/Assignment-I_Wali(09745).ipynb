{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47484929",
   "metadata": {},
   "source": [
    "# Assignment-I (Backpropagation Implementation)\n",
    "## Deep Learning\n",
    "## Wali Ullah (09745)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de21761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from csv import reader\n",
    "from math import exp\n",
    "from sklearn.metrics import classification_report, confusion_matrix#for visualizing tree \n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eadd40",
   "metadata": {},
   "source": [
    "## data Generation and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d96a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(sample_size):\n",
    "    X,y = make_classification(n_samples=sample_size, n_features=4, n_informative=4, \n",
    "                    n_redundant=0, n_repeated=0, n_classes=4, n_clusters_per_class=2,\n",
    "                          class_sep=1.5,\n",
    "                   flip_y=0,weights=[0.5,0.5,0.5,0.5])\n",
    "    X = pd.DataFrame(X)\n",
    "    y = pd.DataFrame(y)\n",
    "    df= pd.concat([X, y], axis=1)\n",
    "    #data=df\n",
    "    df1= df.to_numpy()\n",
    "    data=df1.astype(int)\n",
    "    print('Sample data is generated')\n",
    "    return data\n",
    "\n",
    "def transformation(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data)\n",
    "    print('Min Max Transformation is applied to Sampled data')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c517c2",
   "metadata": {},
   "source": [
    "# Alg with Neurons and Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "060fae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alg with Neurons and Layers\n",
    "#number of neurons=n_hidden\n",
    "#number of layers=n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e44dda09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "# Initialize a network with n_hidden neurons and n_layers number of layers\n",
    "\n",
    "\n",
    "def initialize_network(n_inputs, n_hidden, n_layers, n_outputs):\n",
    "    network = list()\n",
    "    if n_layers==1:\n",
    "            hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "            network.append(hidden_layer)\n",
    "            output_layer = [{'weights':[random() for i in range(n_hidden)]} for i in range(n_outputs)]\n",
    "            network.append(output_layer)\n",
    "    else:\n",
    "        for i in range(n_layers):\n",
    "            if i==0:\n",
    "                hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "                network.append(hidden_layer)\n",
    "            else:\n",
    "                hidden_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_hidden)]\n",
    "                network.append(hidden_layer)\n",
    "        output_layer = [{'weights':[random() for i in range(n_hidden)]} for i in range(n_outputs)]\n",
    "        network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "\tactivation = weights[-1]\n",
    "\tfor i in range(len(weights)-1):\n",
    "\t\tactivation += weights[i] * inputs[i]\n",
    "\treturn activation\n",
    "\n",
    "\n",
    "\n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "\tinputs = row\n",
    "\tfor layer in network:\n",
    "\t\tnew_inputs = []\n",
    "\t\tfor neuron in layer:\n",
    "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
    "\t\t\tneuron['output'] = transfer(activation, fn)\n",
    "\t\t\tnew_inputs.append(neuron['output'])\n",
    "\t\tinputs = new_inputs\n",
    "\treturn inputs\n",
    "\n",
    "\n",
    "\n",
    "# Transfer neuron activation sigmoid\n",
    "def transfer(activation, fn):\n",
    "\tz=fn\n",
    "\tif z=='sigmoid':\n",
    "\t\tt= 1.0 / (1.0 + exp(-activation))\n",
    "\telse:\n",
    "\t\tt=(exp(activation)-exp(-activation))/(exp(activation)+exp(-activation))\n",
    "\treturn t\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the derivative of an neuron output sigmoind\n",
    "def transfer_derivative(output, fn):\n",
    "\tz=fn\n",
    "\tif z=='sigmoid':\n",
    "\t\tt_deri=output * (1.0 - output)\n",
    "\telse:\n",
    "\t\tt_deri=1-(output)**2\n",
    "\treturn t_deri\n",
    "\n",
    "\n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected,fn):\n",
    "\tfor i in reversed(range(len(network))):\n",
    "\t\tlayer = network[i]\n",
    "\t\terrors = list()\n",
    "\t\tif i != len(network)-1:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\terror = 0.0\n",
    "\t\t\t\tfor neuron in network[i + 1]:\n",
    "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
    "\t\t\t\terrors.append(error)\n",
    "\t\telse:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\tneuron = layer[j]\n",
    "\t\t\t\terrors.append(expected[j] - neuron['output'])\n",
    "\t\tfor j in range(len(layer)):\n",
    "\t\t\tneuron = layer[j]\n",
    "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'], fn)\n",
    "\n",
    "            \n",
    "            \n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "\tfor i in range(len(network)):\n",
    "\t\tinputs = row[:-1]\n",
    "\t\tif i != 0:\n",
    "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "\t\tfor neuron in network[i]:\n",
    "\t\t\tfor j in range(len(inputs)):\n",
    "\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']\n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0\n",
    "\t\tfor row in train:\n",
    "\t\t\toutputs = forward_propagate(network, row)\n",
    "\t\t\texpected = [0 for i in range(n_outputs)]\n",
    "\t\t\texpected[row[-1]] = 1\n",
    "\t\t\tsum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "\t\t\tbackward_propagate_error(network, expected, fn)\n",
    "\t\t\tupdate_weights(network, row, l_rate)\n",
    "\t\tprint('&gt;epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\n",
    "        \n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "\toutputs = forward_propagate(network, row)\n",
    "\treturn outputs.index(max(outputs))\n",
    "\n",
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(train, test, l_rate,  n_epoch, n_hidden):\n",
    "\tn_inputs = len(train[0]) - 1\n",
    "\tn_outputs = len(set([row[-1] for row in train]))\n",
    "\tnetwork = initialize_network(n_inputs, n_layers, n_hidden, n_outputs)\n",
    "\ttrain_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "\tpredictions = list()\n",
    "\tfor row in test:\n",
    "\t\tprediction = predict(network, row)\n",
    "\t\tpredictions.append(prediction)\n",
    "\treturn(predictions)\n",
    "\n",
    "# Split a dataset into a train and test set\n",
    "def train_test_split(dataset, split=0.60):\n",
    "\ttrain = list()\n",
    "\ttrain_size = split * len(dataset)\n",
    "\tdataset_copy = list(dataset)\n",
    "\twhile len(train) < train_size:\n",
    "\t\tindex = randrange(len(dataset_copy))\n",
    "\t\ttrain.append(dataset_copy.pop(index))\n",
    "\treturn train, dataset_copy\n",
    "    \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\t#print(\"Classification report - \\n\", classification_report(actual, predicted))\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0    \n",
    "    \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm,n_layers, *args):\n",
    "\ttrain, test = train_test_split(dataset)\n",
    "    #folds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\ttrain_set = list(train)\n",
    "\ttest_set = list()\n",
    "\tfor row in train:\n",
    "\t\trow_copy = list(row)\n",
    "\t\ttest_set.append(row_copy)\n",
    "\t\trow_copy[-1] = None\n",
    "\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\tactual = [row[-1] for row in test]\n",
    "\taccuracy = accuracy_metric(actual, predicted)\n",
    "\tscores.append(accuracy)\n",
    "\treturn scores\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb8a787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data is generated\n",
      "Min Max Transformation is applied to Sampled data\n",
      "&gt;epoch=0, lrate=10.000, error=3008.055\n",
      "&gt;epoch=1, lrate=10.000, error=2999.996\n",
      "&gt;epoch=2, lrate=10.000, error=2999.996\n",
      "&gt;epoch=3, lrate=10.000, error=2999.996\n",
      "&gt;epoch=4, lrate=10.000, error=2999.995\n",
      "&gt;epoch=5, lrate=10.000, error=2999.995\n",
      "&gt;epoch=6, lrate=10.000, error=2999.994\n",
      "&gt;epoch=7, lrate=10.000, error=2999.993\n",
      "&gt;epoch=8, lrate=10.000, error=2999.992\n",
      "&gt;epoch=9, lrate=10.000, error=2999.989\n",
      "&gt;epoch=10, lrate=10.000, error=2999.979\n",
      "&gt;epoch=11, lrate=10.000, error=3000.097\n",
      "&gt;epoch=12, lrate=10.000, error=2999.996\n",
      "&gt;epoch=13, lrate=10.000, error=2999.996\n",
      "&gt;epoch=14, lrate=10.000, error=2999.996\n",
      "&gt;epoch=15, lrate=10.000, error=2999.996\n",
      "&gt;epoch=16, lrate=10.000, error=2999.995\n",
      "&gt;epoch=17, lrate=10.000, error=2999.995\n",
      "&gt;epoch=18, lrate=10.000, error=2999.994\n",
      "&gt;epoch=19, lrate=10.000, error=2999.994\n",
      "&gt;epoch=20, lrate=10.000, error=2999.993\n",
      "&gt;epoch=21, lrate=10.000, error=2999.992\n",
      "&gt;epoch=22, lrate=10.000, error=2999.990\n",
      "&gt;epoch=23, lrate=10.000, error=2999.988\n",
      "&gt;epoch=24, lrate=10.000, error=2999.984\n",
      "&gt;epoch=25, lrate=10.000, error=2999.973\n",
      "&gt;epoch=26, lrate=10.000, error=3422.067\n",
      "&gt;epoch=27, lrate=10.000, error=5049.260\n",
      "&gt;epoch=28, lrate=10.000, error=5740.648\n",
      "&gt;epoch=29, lrate=10.000, error=3381.499\n",
      "&gt;epoch=30, lrate=10.000, error=2615.485\n",
      "&gt;epoch=31, lrate=10.000, error=2615.452\n",
      "&gt;epoch=32, lrate=10.000, error=2615.442\n",
      "&gt;epoch=33, lrate=10.000, error=2615.438\n",
      "&gt;epoch=34, lrate=10.000, error=2615.435\n",
      "&gt;epoch=35, lrate=10.000, error=2615.433\n",
      "&gt;epoch=36, lrate=10.000, error=2615.431\n",
      "&gt;epoch=37, lrate=10.000, error=2615.430\n",
      "&gt;epoch=38, lrate=10.000, error=2615.429\n",
      "&gt;epoch=39, lrate=10.000, error=2615.429\n",
      "&gt;epoch=40, lrate=10.000, error=2615.428\n",
      "&gt;epoch=41, lrate=10.000, error=2615.428\n",
      "&gt;epoch=42, lrate=10.000, error=2615.427\n",
      "&gt;epoch=43, lrate=10.000, error=2615.427\n",
      "&gt;epoch=44, lrate=10.000, error=2615.427\n",
      "&gt;epoch=45, lrate=10.000, error=2615.426\n",
      "&gt;epoch=46, lrate=10.000, error=2615.426\n",
      "&gt;epoch=47, lrate=10.000, error=2615.426\n",
      "&gt;epoch=48, lrate=10.000, error=2615.426\n",
      "&gt;epoch=49, lrate=10.000, error=2615.426\n",
      "Accuracy Scores: [23.45]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "# Test training backprop algorithm\n",
    "seed(1)\n",
    "\n",
    "sample_size=5000           #Choose sample size\n",
    "data_raw= data(sample_size)\n",
    "df1=transformation(data_raw)\n",
    "\n",
    "\n",
    "dataset = df1\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "\n",
    "#n_outputs=3\n",
    "\n",
    "fn= 'sigmoid'      #choose function either 'sigmoid' or 'tanh'\n",
    "l_rate = 0.9       #choose learning rate\n",
    "n_epoch = 50      #chhose epoch size\n",
    "n_hidden =5        #number of neurons in hidden layers\n",
    "n_layers= 10       #number of hidden layers\n",
    "scores = evaluate_algorithm(dataset,back_propagation, l_rate, n_layers, n_epoch, n_hidden)\n",
    "print('Accuracy Scores: %s' % scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d63706f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942d78f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
