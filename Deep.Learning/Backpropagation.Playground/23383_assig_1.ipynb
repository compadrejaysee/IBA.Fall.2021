{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate data with 4 features and 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate data \n",
    "# change the value of n_samples to change the number of inputs\n",
    "X, y = make_classification(n_samples=5000, n_features=4, n_informative=4, n_redundant=0, n_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change below parameters to your liking for the neural network along with some restrictions as defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = {\n",
    "    \"hidden_layers\": 1,        # can change to any value\n",
    "    \"num_neurons_hidden\" : 4,  # only 4 acceptable\n",
    "    \"num_neurons_output\" : 4,  # only 4 acceptable\n",
    "    \"activation_func\": \"sigmoid\", # use tanh or sigmoid\n",
    "    \"learning_rate\" : 0.35,          # must be between 0 and 1\n",
    "    \"train_test-split-ratio\": 0.80, # can be any ratio. 0.80 here means that 80% of the data will be used as training set\n",
    "    \"mini_batch_size\" : 500,  \n",
    "    \"epochs\" : 5\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below cell will define the neural network function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feedforward\n",
    "def neural_network(base_params, training_sample, testing_sample):\n",
    "\n",
    "# calculate layers\n",
    "    layers = [len(training_sample[0][0])] + [base_params[\"num_neurons_hidden\"]]*base_params[\"hidden_layers\"] + [base_params[\"num_neurons_output\"]]\n",
    "    \n",
    "    ## initialize weights & biases\n",
    "    weights = []\n",
    "    for i in range(len(layers)):\n",
    "        j = i+1\n",
    "        if j < len(layers):\n",
    "            weights.append(np.array(np.random.default_rng().standard_normal(layers[i]*layers[j])).reshape(layers[i],layers[i]))\n",
    "    weights = np.array(weights)\n",
    "    biases = np.array([np.random.default_rng().standard_normal(i) for i in layers[1:]])\n",
    "    \n",
    "    # for each epoch make mini batches\n",
    "    for epoch in range(base_params[\"epochs\"]):\n",
    "        np.random.shuffle(training_sample)\n",
    "        for batch in range(0,len(training_sample), base_params[\"mini_batch_size\"]):\n",
    "            training_sample[batch:batch + base_params[\"mini_batch_size\"]]\n",
    "            for x_values in range(len(training_sample)):\n",
    "                all_activations = feedforward(layers,weights, biases, training_sample[x_values][0], training_sample[x_values][1])\n",
    "                # take total deltas \n",
    "                if epoch == 0:\n",
    "                    all_deltas = backpropagate_error(layers,weights,all_activations, training_sample[x_values][1])\n",
    "                else:\n",
    "                    all_deltas = all_deltas + backpropagate_error(layers,weights,all_activations, training_sample[x_values][1])\n",
    "            weights, biases  = update_weights_biases(weights, biases, all_deltas, all_activations)\n",
    "        print(\"=========================================\")\n",
    "        print( \"epochs : \" + str(epoch + 1) + \" / \" + str(base_params[\"epochs\"]))\n",
    "        calc_accuracy(layers, testing_sample,weights, biases)\n",
    "        print(\"====================================\")\n",
    " \n",
    "def feedforward(layers,weights, biases, train_x, train_y):\n",
    "    all_activations = []\n",
    "    with_activation = train_x\n",
    "    all_activations.append(with_activation)\n",
    "    for layer in range(1,len(layers)):\n",
    "        all_activations.append( activation_func(np.dot(weights[layer-1],with_activation) + biases[layer-1], base_params[\"activation_func\"]))\n",
    "        with_activation=all_activations[layer-1]\n",
    "    all_activations = np.array(all_activations)\n",
    "    return all_activations\n",
    "\n",
    "def backpropagate_error(layers,weights, all_activations, train_y):\n",
    "    # last laeyer delta\n",
    "    deltas = []\n",
    "    deltas.append( np.multiply( derivative_of_cost(all_activations[-1],train_y), derivate_act_func(all_activations[-1], base_params[\"activation_func\"])))\n",
    "\n",
    "    # all layers delta\n",
    "    for delta_layer in range(len(weights),1, -1):\n",
    "\n",
    "        one_delta = np.multiply(np.dot(np.transpose(weights[delta_layer - 1]),deltas[len(weights) - delta_layer]), derivate_act_func(all_activations[delta_layer - 1],base_params[\"activation_func\"]))\n",
    "        deltas.append(one_delta)\n",
    "\n",
    "    # 0 index of deltas represent last layer delta\n",
    "    deltas = np.flipud(np.array(deltas))\n",
    "    return deltas\n",
    "\n",
    "def update_weights_biases(weights, biases, deltas, all_activations):\n",
    "    # updation of weights and biases\n",
    "    for i in range(len(weights)):\n",
    "        for j in range(len(weights[i])):\n",
    "            delta_weights = np.transpose(weights[i])\n",
    "            delta_weights[i][j] = delta_weights[i][j] - ( base_params[\"learning_rate\"] / base_params[\"mini_batch_size\"]) * (all_activations[i][j]*deltas[i][j])\n",
    "        weights[i] = np.transpose(delta_weights)\n",
    "    biases = biases - (base_params[\"learning_rate\"]/ base_params[\"mini_batch_size\"])*deltas\n",
    "    return weights, biases\n",
    "\n",
    "def calc_accuracy(layers,sample_to_check, weights, biases):\n",
    "    predicted_outputs = []\n",
    "    actual_outputs = []\n",
    "    for i in range(len(sample_to_check)):\n",
    "        predicted_outputs.append(np.argmax(feedforward(layers,weights, biases, sample_to_check[i][0], sample_to_check[i][1] )[-1]))\n",
    "        actual_outputs.append(np.argmax(sample_to_check[i][1]))\n",
    "    print(\" precision = \" + str(precision_score(predicted_outputs, actual_outputs, average='macro')))\n",
    "    print(\" recall = \" + str(recall_score(predicted_outputs, actual_outputs, average='macro')))\n",
    "    print(\" accuracy = \" + str(accuracy_score(predicted_outputs, actual_outputs)))\n",
    "\n",
    "def activation_func(z, func_to_use):\n",
    "    if func_to_use == \"sigmoid\":\n",
    "        return sigmoid_func(z)\n",
    "    else:\n",
    "        return tanh_func(z)\n",
    "\n",
    "def sigmoid_func(z):\n",
    "    return 1/(1+math.e**(-z))\n",
    "\n",
    "def tanh_func(z):\n",
    "    return ((math.e**(z)) - (math.e**(-z)))/ ((math.e**(z)) + (math.e**(-z)) )\n",
    "\n",
    "def derivate_act_func(output, func_to_use):\n",
    "    if func_to_use == \"sigmoid\":\n",
    "        return derivative_of_sigmoid(output)\n",
    "    else:\n",
    "        return derivative_of_tanh(output)\n",
    "\n",
    "# error at output layer\n",
    "def derivative_of_cost(output,target):\n",
    "    return output - target\n",
    "\n",
    "def derivative_of_sigmoid(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "def derivative_of_tanh(output):\n",
    "    return 1 - output**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run below cell to train your neural network and output accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "epochs : 1 / 5\n",
      " precision = 0.05025\n",
      " recall = 0.25\n",
      " accuracy = 0.201\n",
      "====================================\n",
      "=========================================\n",
      "epochs : 2 / 5\n",
      " precision = 0.038\n",
      " recall = 0.25\n",
      " accuracy = 0.152\n",
      "====================================\n",
      "=========================================\n",
      "epochs : 3 / 5\n",
      " precision = 0.03075\n",
      " recall = 0.25\n",
      " accuracy = 0.123\n",
      "====================================\n",
      "=========================================\n",
      "epochs : 4 / 5\n",
      " precision = 0.0135\n",
      " recall = 0.25\n",
      " accuracy = 0.054\n",
      "====================================\n",
      "=========================================\n",
      "epochs : 5 / 5\n",
      " precision = 0.0026666666666666666\n",
      " recall = 0.3333333333333333\n",
      " accuracy = 0.008\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "##data_preprocessing\n",
    "input_data_len = X.shape[0]\n",
    "y_classes = np.unique(y)\n",
    "size_train = int(base_params[\"train_test-split-ratio\"]*input_data_len)\n",
    "train_x, train_y , test_x, test_y = X[:size_train], y[:size_train], X[size_train:], y[size_train:]\n",
    "\n",
    "zero_vector = np.zeros((size_train, len(y_classes)))\n",
    "for i in range(len(train_y)):\n",
    "    zero_vector[i][train_y[i]] = 1\n",
    "# replace train y with vectors. the index have 1 represents the correct value\n",
    "train_y = zero_vector\n",
    "training_sample = np.array(list(zip(train_x, train_y)))\n",
    "testing_sample = np.array(list(zip(test_x, test_y)))\n",
    "\n",
    "\n",
    "neural_network(base_params, training_sample, testing_sample)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab300e542198f4740520b230a8686cef27802a0ff73f91e4975ff04eb92810d9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
