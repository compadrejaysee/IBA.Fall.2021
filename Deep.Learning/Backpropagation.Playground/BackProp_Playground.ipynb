{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BackProp Playground.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wqWOuPLFtfDg",
        "QulONz2UnCT-"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKCuE02atbSG"
      },
      "source": [
        "# Playground"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqWOuPLFtfDg"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB093boIe1EX"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.metrics import classification_report\n",
        "import math as mt\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kJcWbUstaOl"
      },
      "source": [
        "### Important Points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x651Nl6KpUIs"
      },
      "source": [
        "This is a playgorund for the training of neural network via backpropagation method.\n",
        "The user can define multiple parameters to test out different results.\n",
        "\n",
        "The list of parameters along with their constraints are as follows:\n",
        "\n",
        "**1- Network Architecture:** \n",
        "\n",
        "The user must define the architecture of the ``neural network``. The program will first ask for number of ``layers`` to be included into the model. There is no restriction on number of ``layers``. A user can add as many layers as desired. \n",
        "\n",
        "Furthermore, the program will ask for number of ``neurons`` to be included into each ``layer``. A user can add as many ``neurons`` as desired but the ``input layer`` must have ``4 neurons`` (as the dataset has 4 features) and either ``4 or 2 neurons`` in ``output layer``. The hidden layers can have any number of ``neurons``.\n",
        "\n",
        "**2- Activation Function:**\n",
        "\n",
        "A user can choose between either ``sigmoid`` or ``tanh`` as activation functions. \n",
        "\n",
        "**3- Batch Size:**\n",
        "\n",
        "A user can enter any value for the ``batch size``\n",
        "\n",
        "**4- Epochs:**\n",
        "\n",
        "User can enter any value for the number of ``epochs``\n",
        "\n",
        "**5- Instances:**\n",
        "\n",
        "A user can enter any number less than ``10,000`` as the number of instances of the data. \n",
        "\n",
        "**6- Split:**\n",
        "\n",
        "A user can enter a value between 0 and 1 to determine the ``test_train_split``. A value of ``0.3`` would mean that ``30%`` data would be used for ``testing`` purposes while ``70%`` of the data would be used for ``training`` purposes.\n",
        "\n",
        "**7- Learning Rate:**\n",
        "\n",
        "A user can enter a value between 0 and 1 to determine the ``learning rate`` for the model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJm8UtSqtuSI"
      },
      "source": [
        "### How to Run the Program?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXFMQLzbtR4Z"
      },
      "source": [
        "To run the program, kindly first run the **Functions** named cell. Then run the cell with the following line of code:\n",
        "\n",
        "``_main_()``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QulONz2UnCT-"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z60awnsVuALc"
      },
      "source": [
        "All the functions used for this playground are defined here. Hence, its imperitive that this cell is executed first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMss4FOfhCMh"
      },
      "source": [
        "\n",
        "# Main function\n",
        "def _main_():\n",
        "\n",
        "  layers, activation_func, instances, l_r, split, batch_size, epochs = input_()\n",
        "  data,__ = data_gen()\n",
        "  process(data,layers,activation_func, instances, l_r, split, batch_size, epochs)\n",
        "\n",
        "\n",
        "\n",
        "# Takes input from the user\n",
        "# Layers - Network Architecture eg: [4,3,3,4]\n",
        "# Activation Function - tanh or sigmoid\n",
        "# Instances - Number of records to be included into the dataset. Max: 10000\n",
        "# l_r - Learning Rate. Value between 0 and 1\n",
        "# Split - Train Test Split Ratio, eg: A value of 0.3 means 30% test set and 70% train set\n",
        "# Batch_size - Batch Size for Stochastic Gradient Descent\n",
        "# Epochs - Number of Epochs \n",
        "def input_():\n",
        "\n",
        "\n",
        "  layers = []\n",
        "  n = int(input(\"\\n\\nEnter number of layers for the Netwrok (eg: if entered 4 then it means 1 input layer, 2 hidden layers, and 1 output layer ) : \"))\n",
        "  for i in range(0, n):\n",
        "    ele = int(input(\"\\n\\nEnter number of neurons for each layer: \"))\n",
        "    layers.append(ele) \n",
        "  print(layers)\n",
        "  activation_func = str(input(\"\\n\\nEnter either 'tanh' or 'sigmoid' as activation function: \"))\n",
        "  instances = int(input(\"\\n\\nEnter the number of instances of the data (Any value less than or equal to 10K): \"))\n",
        "  l_r = float(input(\"\\n\\nEnter the learning rate value: \"))\n",
        "  split = float(input(\"\\n\\nEnter the test_train split ratio (eg: 0.1 ratio means 10% testing data and 90% training data): \"))\n",
        "  batch_size = int(input(\"\\n\\nEnter the batch size value: \"))\n",
        "  epochs = int(input(\"\\n\\nEnter the number of epochs: \"))\n",
        "  return (layers, activation_func, instances, l_r, split, batch_size, epochs)\n",
        "\n",
        "# Generates normally distributed data\n",
        "def data_gen():\n",
        "  data = []\n",
        "  data, labels = make_blobs(10000, 4, random_state = 0, centers = 4)\n",
        "  data = pd.DataFrame(data)\n",
        "  data[\"Output\"] = labels\n",
        "  return data,labels\n",
        "\n",
        "# Initialize weights and biases\n",
        "def init_w_b (layers):\n",
        "  biases = [np.random.randn(y, 1) for y in layers[1:]]\n",
        "  weights = [np.random.randn(y, x) for x, y in zip(layers[:-1], layers[1:])]\n",
        "  return (weights, biases)\n",
        "\n",
        "# Sigmoid\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "# Derivative of Sigmoid\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "\n",
        "# Derivative of Tanh\n",
        "def tanh_prime(z):\n",
        "  return (1-tanh_(z)**2)\n",
        "\n",
        "# Tanh\n",
        "def tanh_(z):\n",
        "  return ((np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z)))\n",
        "\n",
        "\n",
        "# Feed forward - Returns activation values and z values for each layer\n",
        "def feed_forward(x,y,weights,biases, activation_func):\n",
        "  \n",
        "  a_ = x\n",
        "  z_list = []\n",
        "  a_list = [x]\n",
        "\n",
        "  if activation_func == 'sigmoid':\n",
        "    for i in range(len(weights)):\n",
        "      weights_=weights[i]\n",
        "      z = np.dot(weights_,a_) + biases[i]\n",
        "      z_list.append(z)\n",
        "      #print(z)\n",
        "      a_ = sigmoid(z)\n",
        "      a_list.append(a_)\n",
        "\n",
        "  if activation_func == 'tanh':\n",
        "    for i in range(len(weights)):\n",
        "      weights_=weights[i]\n",
        "      z = np.dot(weights_,a_) + biases[i]\n",
        "      z_list.append(z)\n",
        "      #print(z)\n",
        "      a_ = tanh_(z)\n",
        "      a_list.append(a_)\n",
        "\n",
        "  return a_list, z_list\n",
        "#cost = a_list[-1] - y.flatten()\n",
        "#c_list.append(cost)\n",
        "\n",
        "# Mapping function for the output labels. Converts the integer values of 0,1,2,3 to respective arrays (works like one-hot encoding)\n",
        "def y_map(y, layers):\n",
        "\n",
        "  if layers[-1] == 4: # if output nuerons are 4\n",
        "    if y == 0: y_ = np.array([1,0,0,0]).reshape(4,1)\n",
        "    if y == 1: y_ = np.array([0,1,0,0]).reshape(4,1)\n",
        "    if y == 2: y_ = np.array([0,0,1,0]).reshape(4,1)\n",
        "    if y == 3: y_ = np.array([0,0,0,1]).reshape(4,1)\n",
        "\n",
        "  if layers[-1] == 2: # if output neurons are 2\n",
        "    if y == 0: y_ = np.array([0,0]).reshape(2,1)\n",
        "    if y == 1: y_ = np.array([0,1]).reshape(2,1)\n",
        "    if y == 2: y_ = np.array([1,0]).reshape(2,1)\n",
        "    if y == 3: y_ = np.array([1,1]).reshape(2,1)\n",
        "\n",
        "  return y_\n",
        "\n",
        "# Backprop - Back propagates the error and calculates partial derivatives for weights (pd_weights) and partial derivatives for biases (pd_biases) for each layer\n",
        "def backprop( x, y, biases, weights, a_list,z_list, activation_func,layers):\n",
        "        \n",
        "  pd_biases = [np.zeros(bias.shape) for bias in biases]\n",
        "  pd_weights = [np.zeros(weight.shape) for weight in weights]\n",
        "\n",
        "  if activation_func == 'sigmoid':\n",
        "    delta = (a_list[-1]-y) *  sigmoid_prime(z_list[-1]) # Calculates delta of the last/output layer\n",
        "    pd_biases[-1] = delta # put the value delta equal to the partial derivative of biases of the neurons of output layer\n",
        "    pd_weights[-1] = np.dot(delta,a_list[-2].transpose()) #Dot product of activation of second last layer and delta (put that euqual to partial derivatives of weights for last layer)\n",
        "\n",
        "    for l in range(2, len(layers)):\n",
        "      z = z_list[-l]\n",
        "      delta = np.dot(weights[-l+1].transpose(), delta) * sigmoid_prime(z)# calculates delta for second last, third last and so on layers by using z values for respective layers\n",
        "      pd_biases[-l] = delta # put the delta value equal to partial derivatives of biases for second last layer and so on\n",
        "      pd_weights[-l] = np.dot(delta, a_list[-l-1].transpose()) # take dot product of activations of previous layer with the delta recursively and store that patrial derivative value \n",
        "      \n",
        "# Description is same as for sigmoid\n",
        "  if activation_func == 'tanh':\n",
        "    delta = (a_list[-1]-y) *  tanh_prime(z_list[-1])\n",
        "    pd_biases[-1] = delta\n",
        "    pd_weights[-1] = np.dot(delta,a_list[-2].transpose())\n",
        "\n",
        "    for l in range(2, len(layers)):\n",
        "      z = z_list[-l]\n",
        "      delta = np.dot(weights[-l+1].transpose(), delta) * tanh_prime(z)\n",
        "      pd_biases[-l] = delta\n",
        "      pd_weights[-l] = np.dot(delta, a_list[-l-1].transpose())\n",
        "  \n",
        "  return (pd_weights, pd_biases) # return partial derivatives of weights and biases\n",
        "\n",
        "\n",
        "# This function is used for the case when there are 2 output neurons in the architecture instead of 4. \n",
        "# This function changes the prediction of the network of the form eg: [0.5,0.3] to a single integer label either 0,1,2 or 3 \n",
        "def y_p_2 (activations,layers):\n",
        "\n",
        "# Mapping - y = 0,1,2,3, then y_ = [0,0], [0,1], [1,0], [1,1]\n",
        "# The logic works on the principle of bits. If the output activation of prediction has maximum value on first bit then the only outcomes would be either [1,0] 2 OR [1,1] 3\n",
        "# If maximum value of the output activation lies on second bit then the only outcomes would be [0,1] 1 or [1,1] 3\n",
        "# The logic is further enhanced to incorporate the outputs of the form [same value, same value] eg [0.5,0.5] or [0.1,0.1]\n",
        "# A threshold value of 0.5 is taken to overcome all other possible outcomes\n",
        "\n",
        "  if np.argmax(activations[-1]) == 0:\n",
        "    if activations[-1][1] >= 0.5:\n",
        "      y_p = 3\n",
        "    else:\n",
        "      y_p = 2\n",
        "      \n",
        "  if np.argmax(activations[-1]) == 1:\n",
        "    if activations[-1][0] >= 0.5:\n",
        "      y_p = 3\n",
        "    else:\n",
        "      y_p = 1\n",
        "\n",
        "  if activations[-1][0] == activations[-1][1] and activations[-1][0] >= 0.5:\n",
        "    y_p = 3\n",
        "\n",
        "  if activations[-1][0] == activations[-1][1] and activations[-1][0] < 0.5:\n",
        "    y_p = 0\n",
        "\n",
        "  if activations[-1][0] >= 0.5 and activations[-1][1] >= 0.5:\n",
        "    y_p = 3\n",
        "\n",
        "  if activations[-1][0] < 0.5 and activations[-1][1] < 0.5:\n",
        "    y_p = 0\n",
        "\n",
        "  if activations[-1][0] >= 0.5 and activations[-1][1] < 0.5:\n",
        "    y_p = 2\n",
        "\n",
        "  if activations[-1][0] < 0.5 and activations[-1][1] >= 0.5:\n",
        "    y_p = 1\n",
        "\n",
        "  else:\n",
        "    y_p = 0\n",
        "\n",
        "  return y_p # returns the predicted value (0,1,2,3)\n",
        "\n",
        "# Returns accuracy of the training of the network (on testing dataset) after each epoch\n",
        "def evaluate_epoch (weights,biases, X_test,y_test,layers,activation_func):\n",
        "\n",
        "  y_pred = []\n",
        "  for i in range(0,len(X_test)):\n",
        "    x = np.array(X_test.values[i]).reshape(4,1)\n",
        "    y = y_test.values[i]\n",
        "\n",
        "    y_ = y_map(y,layers)\n",
        "\n",
        "    activations,_ = feed_forward(x,y_,weights,biases,activation_func)\n",
        "    if layers[-1] == 4:\n",
        "      y_p = np.argmax(activations[-1])\n",
        "      y_pred.append(y_p) \n",
        "\n",
        "    if layers[-1] == 2:\n",
        "      y_p = y_p_2 (activations, layers)\n",
        "      y_pred.append(y_p)\n",
        "\n",
        "  acc = 0\n",
        "  for y_actual, y_predicted in zip (y_test.values,y_pred):\n",
        "    if y_actual == y_predicted:\n",
        "      acc = acc + 1\n",
        "  return acc\n",
        "\n",
        "# Returns sum of all pd's for weights and biases in each layer for a single batch\n",
        "# This is the most important function. As it sums all partial derivatives of weights and biases calculated for each instance of the mini batch accross all layers of the network\n",
        "# The sum of all partial derivatives of weights and biases for a single layer accross all instances of mini batch are added together and stored\n",
        "# Suppose mini batch size is 64. Then you would have 64 different partial derivative matrices each for weights and biases.\n",
        "# Now you have to add all 64 partial derivatives of weights and biases for the first layer and store it into a different matrix at proper index\n",
        "# then add all 64 partial derivatives of weights and biases for the second layer and store it into the next index of the matrix\n",
        "# Iterate like that until you have a new matrices having layer by layer sum of all partial derivatives of weights and biases\n",
        "\n",
        "def pd_batch_sum (pd_weights_list,pd_biases_list, weights, biases):\n",
        "  w_a1_list = [] # an empty list to store all pd's for weights layer by layer\n",
        "  b_a1_list = [] # an empty list to store all pd's for wbiases layer by layer\n",
        "  batch_biases = [np.zeros(b.shape) for b in biases] # initialize an empty array of the same shape as of biases\n",
        "  batch_weights = [np.zeros(w.shape) for w in weights] # initialize an empty array of the same shape as of weights\n",
        "  for i in range(0,len(weights)): # loops accross all layers\n",
        "    sum = 0\n",
        "    w_a1_list = []\n",
        "    for j in range(0,len(pd_weights_list)): # pd_weights_list contains all partial derivatives of wieghts calculated at each instance of the mini batch\n",
        "\n",
        "      w_a1 = pd_weights_list[j][i]\n",
        "      #print(w_a1)\n",
        "      w_a1_list.append(w_a1)\n",
        "    #print(w_a1_list)\n",
        "\n",
        "    for k in range(0,len(w_a1_list)):\n",
        "      sum = sum + w_a1_list[k]\n",
        "    batch_weights[i]=sum\n",
        "#print(batch_weights)\n",
        "\n",
        "  for i in range(0,len(biases)):\n",
        "    sum = 0\n",
        "    b_a1_list = []\n",
        "    for j in range(0,len(pd_biases_list)):\n",
        "\n",
        "      b_a1 = pd_biases_list[j][i]\n",
        "      #print(w_a1)\n",
        "      b_a1_list.append(b_a1)\n",
        "    #print(w_a1_list)\n",
        "\n",
        "    for k in range(0,len(b_a1_list)):\n",
        "      sum = sum + b_a1_list[k]\n",
        "    batch_biases[i]=sum\n",
        "\n",
        "#print(batch_biases)\n",
        "  return batch_weights,batch_biases # summed partial derivatives of weights and biases\n",
        "\n",
        "# Returns total accuracy (calculated over testing data) of the model after complete training. Also, returns f1 score, precision, recall and support for each class \n",
        "def evaluate_training (weights,biases, X_test,y_test,layers,activation_func):\n",
        "\n",
        "  y_pred = []\n",
        "  for i in range(0,len(X_test)):\n",
        "    x = np.array(X_test.values[i]).reshape(4,1)\n",
        "    y = y_test.values[i]\n",
        "    y_ = y_map(y, layers)\n",
        "    activations,_ = feed_forward(x,y_,weights,biases,activation_func)\n",
        "\n",
        "    if layers[-1] == 4:\n",
        "      y_p = np.argmax(activations[-1])\n",
        "      y_pred.append(y_p) \n",
        "\n",
        "    if layers[-1] == 2:\n",
        "      y_p = y_p_2(activations,layers)\n",
        "      y_pred.append(y_p)\n",
        "\n",
        "  labels = [\"0\",\"1\",\"2\",\"3\"]\n",
        "\n",
        "  acc = accuracy_score(y_test.values,y_pred)\n",
        "  cls_rprt = classification_report(y_test.values, y_pred, target_names= labels)\n",
        "  return (cls_rprt,acc)\n",
        "\n",
        "\n",
        "# Updates the weights and biases after each batch\n",
        "def updater(batch_weights, batch_biases,weights,biases, l_r,batch_size):\n",
        "  new_weights = [np.zeros(bias.shape) for bias in biases]\n",
        "  new_biases = [np.zeros(weight.shape) for weight in weights]\n",
        "  new_weights = [weight-(l_r/batch_size)*batch_weight for weight, batch_weight in zip(weights, batch_weights)]\n",
        "  new_biases = [bias-(l_r/batch_size)*batch_bias for bias, batch_bias in zip(biases, batch_biases)]\n",
        "  return new_weights, new_biases\n",
        "\n",
        "\n",
        "# The core function that combines all other functions to train and test the network\n",
        "def process(data, layers, activation_func, instances, l_r, split, batch_size, epochs):\n",
        "\n",
        "  weights, biases = init_w_b(layers) # weights and biases are initialized\n",
        "  data = data.iloc[0:instances,:]\n",
        "  batch_size = batch_size\n",
        "  data_n = data.iloc[:,:len(data.columns)-1]\n",
        "  target = data.iloc[:,len(data.columns)-1]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(data_n, target,test_size = split ,random_state=42) # train-test split\n",
        "  #X_data  = pd.DataFrame(X_train,y_train)\n",
        "  X_train['Output'] = y_train\n",
        "  n_train = len(X_train)\n",
        "  acc_list = []\n",
        "  start = time.time()\n",
        "  for j in range(0,epochs):\n",
        "    #print('\\n\\nepoch',j)\n",
        "    X_train.sample(frac = 1)\n",
        "    mini_batches = [X_train[k:k+batch_size] for k in range(0, n_train, batch_size)]\n",
        "\n",
        "    for mb in mini_batches:\n",
        "      #print('\\n\\nminibatch')\n",
        "      X = mb.iloc[:,0:len(mb.columns)-1].values\n",
        "      Y = mb.iloc[:,len(mb.columns)-1].values\n",
        "      pd_weights_list = []\n",
        "      pd_biases_list = []\n",
        "\n",
        "      for i in range(0,len(mb)):\n",
        "        x = np.array(X[i]).reshape(len(X[i]),1)\n",
        "        y = Y[i]\n",
        "        y_ = y_map(y,layers) # label - array mapping\n",
        "        a_list, z_list = feed_forward(x,y_,weights,biases,activation_func) # feed foward\n",
        "\n",
        "        pd_weights, pd_biases = backprop(x,y_,biases,weights,a_list,z_list, activation_func,layers) # backpropagate\n",
        "\n",
        "        pd_weights_list.append(pd_weights) # a list containing all pd's for weights\n",
        "        pd_biases_list.append(pd_biases) # a list containing all pd's for biases\n",
        "\n",
        "      batch_weights, batch_biases = pd_batch_sum (pd_weights_list,pd_biases_list, weights,biases) # calculates a summed partial derivative array for weights and biases\n",
        "      weights_new, biases_new = updater(batch_weights, batch_biases,weights,biases, l_r,batch_size) # updates the weights and biases\n",
        "      weights = weights_new\n",
        "      biases = biases_new\n",
        "      #print('\\n\\nweights',weights)\n",
        "      #print('\\n\\nbiases',biases)\n",
        "    acc = evaluate_epoch (weights,biases, X_test,y_test, layers,activation_func)\n",
        "    acc_list.append(acc/len(y_test)*100)\n",
        "    print (\"Epoch {0}: Accuracy is: {1} % \".format(j, (acc/len(y_test)*100)))\n",
        "\n",
        "  cls_rprt, accuracy = evaluate_training(weights,biases, X_test,y_test,layers,activation_func)\n",
        "  stop = time.time()\n",
        "  print(\"\\n\\nNetwork: \", layers)\n",
        "  print(\"\\nData Instances: \", instances)\n",
        "  print(\"\\nTest_Train_Split: \", split)\n",
        "  print(\"\\nBatch Size: \", batch_size)\n",
        "  print(\"\\nlearning Rate: \", l_r)\n",
        "  print(\"\\nEpochs: \", epochs)\n",
        "  print(\"\\nActivation Function: \", activation_func)\n",
        "  print(\"\\nOutput Neurons: \", layers[-1])\n",
        "  print(\"\\nElapsed time is {0} seconds: \".format(stop-start))\n",
        "  plt.plot(range(0,epochs),acc_list)\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.title(\"Accuracy VS Epochs\")\n",
        "  plt.show\n",
        "  print(\"\\n\\nTesting Accuracy is {0}%: \".format(accuracy*100))\n",
        "  print(\"\\n\")\n",
        "  print(\"Classification Report for each class is: \")\n",
        "  print(cls_rprt)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "befSMK7X0edP"
      },
      "source": [
        "# ``_main_()``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_2Cx0iTuTtN"
      },
      "source": [
        "Now run ``_main_()``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "joccKQFCuW_2",
        "outputId": "2e05c5bc-03f6-4130-d752-7e2e8105a93c"
      },
      "source": [
        "# Run after the cell named Functions is done executing\n",
        "_main_()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Enter number of layers for the Netwrok (eg: if entered 4 then it means 1 input layer, 2 hidden layers, and 1 output layer ) : 4\n",
            "\n",
            "\n",
            "Enter number of neurons for each layer: 4\n",
            "\n",
            "\n",
            "Enter number of neurons for each layer: 3\n",
            "\n",
            "\n",
            "Enter number of neurons for each layer: 3\n",
            "\n",
            "\n",
            "Enter number of neurons for each layer: 4\n",
            "[4, 3, 3, 4]\n",
            "\n",
            "\n",
            "Enter either 'tanh' or 'sigmoid' as activation function: tanh\n",
            "\n",
            "\n",
            "Enter the number of instances of the data (Any value less than or equal to 10K): 10000\n",
            "\n",
            "\n",
            "Enter the learning rate value: 0.065\n",
            "\n",
            "\n",
            "Enter the test_train split ratio (eg: 0.1 ratio means 10% testing data and 90% training data): 0.3\n",
            "\n",
            "\n",
            "Enter the batch size value: 32\n",
            "\n",
            "\n",
            "Enter the number of epochs: 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:305: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Accuracy is: 75.06666666666668 % \n",
            "Epoch 1: Accuracy is: 75.06666666666668 % \n",
            "Epoch 2: Accuracy is: 75.06666666666668 % \n",
            "Epoch 3: Accuracy is: 71.89999999999999 % \n",
            "Epoch 4: Accuracy is: 74.83333333333333 % \n",
            "Epoch 5: Accuracy is: 74.83333333333333 % \n",
            "Epoch 6: Accuracy is: 74.83333333333333 % \n",
            "Epoch 7: Accuracy is: 74.83333333333333 % \n",
            "Epoch 8: Accuracy is: 74.83333333333333 % \n",
            "Epoch 9: Accuracy is: 74.83333333333333 % \n",
            "Epoch 10: Accuracy is: 74.83333333333333 % \n",
            "Epoch 11: Accuracy is: 74.83333333333333 % \n",
            "Epoch 12: Accuracy is: 74.83333333333333 % \n",
            "Epoch 13: Accuracy is: 74.83333333333333 % \n",
            "Epoch 14: Accuracy is: 74.83333333333333 % \n",
            "Epoch 15: Accuracy is: 74.83333333333333 % \n",
            "Epoch 16: Accuracy is: 74.83333333333333 % \n",
            "Epoch 17: Accuracy is: 74.83333333333333 % \n",
            "Epoch 18: Accuracy is: 74.83333333333333 % \n",
            "Epoch 19: Accuracy is: 74.83333333333333 % \n",
            "Epoch 20: Accuracy is: 74.83333333333333 % \n",
            "Epoch 21: Accuracy is: 74.83333333333333 % \n",
            "Epoch 22: Accuracy is: 74.83333333333333 % \n",
            "Epoch 23: Accuracy is: 74.83333333333333 % \n",
            "Epoch 24: Accuracy is: 74.83333333333333 % \n",
            "Epoch 25: Accuracy is: 74.83333333333333 % \n",
            "Epoch 26: Accuracy is: 74.83333333333333 % \n",
            "Epoch 27: Accuracy is: 74.83333333333333 % \n",
            "Epoch 28: Accuracy is: 74.83333333333333 % \n",
            "Epoch 29: Accuracy is: 74.83333333333333 % \n",
            "Epoch 30: Accuracy is: 74.83333333333333 % \n",
            "Epoch 31: Accuracy is: 74.83333333333333 % \n",
            "Epoch 32: Accuracy is: 74.83333333333333 % \n",
            "Epoch 33: Accuracy is: 74.83333333333333 % \n",
            "Epoch 34: Accuracy is: 74.83333333333333 % \n",
            "Epoch 35: Accuracy is: 74.83333333333333 % \n",
            "Epoch 36: Accuracy is: 74.83333333333333 % \n",
            "Epoch 37: Accuracy is: 74.83333333333333 % \n",
            "Epoch 38: Accuracy is: 74.83333333333333 % \n",
            "Epoch 39: Accuracy is: 74.83333333333333 % \n",
            "Epoch 40: Accuracy is: 74.83333333333333 % \n",
            "Epoch 41: Accuracy is: 74.83333333333333 % \n",
            "Epoch 42: Accuracy is: 74.83333333333333 % \n",
            "Epoch 43: Accuracy is: 74.83333333333333 % \n",
            "Epoch 44: Accuracy is: 74.83333333333333 % \n",
            "Epoch 45: Accuracy is: 74.83333333333333 % \n",
            "Epoch 46: Accuracy is: 74.83333333333333 % \n",
            "Epoch 47: Accuracy is: 74.83333333333333 % \n",
            "Epoch 48: Accuracy is: 74.83333333333333 % \n",
            "Epoch 49: Accuracy is: 74.83333333333333 % \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Network:  [4, 3, 3, 4]\n",
            "\n",
            "Data Instances:  10000\n",
            "\n",
            "Test_Train_Split:  0.3\n",
            "\n",
            "Batch Size:  32\n",
            "\n",
            "learning Rate:  0.065\n",
            "\n",
            "Epochs:  50\n",
            "\n",
            "Activation Function:  tanh\n",
            "\n",
            "Output Neurons:  4\n",
            "\n",
            "Elapsed time is 54.58755564689636: \n",
            "\n",
            "\n",
            "Testing Accuracy is 74.83333333333333%: \n",
            "\n",
            "\n",
            "Classification Report for each class is: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       755\n",
            "           1       1.00      1.00      1.00       767\n",
            "           2       0.50      1.00      0.66       748\n",
            "           3       1.00      1.00      1.00       730\n",
            "\n",
            "    accuracy                           0.75      3000\n",
            "   macro avg       0.62      0.75      0.67      3000\n",
            "weighted avg       0.62      0.75      0.66      3000\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZRkZZ3m8e+TS0WyKUslIFRBgSCICiXmYVwYWqRBCqWwx1HBDRdEu93wqLhju51D085A2zqtqAgqAqIUQzvIIuLSKkpRsgkiFE0JxVY0YCGQVbn85o/73swgKyIzMiJuZN3I53NOnIr73rg335sk7y/eXRGBmZnZVD1znQEzM9s8OUCYmVlNDhBmZlaTA4SZmdXkAGFmZjU5QJiZWU0OEGY2LUk/k3TCXOfDOs8BwuZUKnwekVSZ67y0m6RdJY1KemaNcyskfTG9P0bS9ZLWS3pI0k8l7VHnnmdL2ijpr1WvG4p+FpufHCBszkhaAvx3IIDlHf7ZfUX/jIhYC1wFvGnKz94eOAo4R9JewLeBDwJPB/YAvgKMTXPr0yJi66rXAYU8gM17DhA2l94MXAOcDRxffULSYkkXSVon6b8kfbnq3Dsk3SrpMUm3SDowpUcqcPPPnS3p8+n9SyXdI+kjku4HviVpO0k/Sj/jkfR+UdX120v6lqR70/mLU/rNko6u+lx/+ub//BrPeA5TAgRwLHBLRNwELAX+MyKuisxjEfHDiPjzbH+Zkpak38GJKc/3SfpQ1fmKpDPSuXvT+0rV+eqazGpJR1bdfndJv0q/8yskLUzXDEj6bvpv9KikayXtNNu82+bJAcLm0puBc9Pr5XnBIqkX+BGwBlgC7Aqcn869BvjHdO3TyGoe/9Xgz9sZ2B7YHTiR7O//W+l4N+BJ4MtVn/8OsCXwHGBH4PSU/m3gjVWfOwq4LyJ+X+NnrgAWSjq4Ku1NZIEDYBWwr6TTJR0qaesGn2U6hwJ7A0cAH5H0tyn9E8ALyYLSAcBBwCcBJB2UnuvDwLbAIcBdVfd8PfBWst/DAiAPPMeT1XwWAzsA7yL7PVo3iAi//Or4CzgYGAEWpuM/Ah9I718ErAP6alx3OfD+OvcMYK+q47OBz6f3LwU2AgPT5Gkp8Eh6/wxgHNiuxud2AR4DnpaOfwCcPM19vwGcmd7vnfKxY9X5FwLfT888nPK9dZ17nZ0+82jV65x0bkn6Hexb9fnTgG+m96uBo6rOvRy4K73/GnB6nZ/5M+CTVcf/AFyW3r8N+DWw/1z/TfnV/pdrEDZXjgeuiIiH0vH3mGxmWgysiYjRGtctJivomrEuIobzA0lbSvqapDWS1gO/ALZNNZjFwMMR8cjUm0TEvcCvgFdL2hZYRlYLqucc4DWSBshqD5dHxINV97smIl4bEYNkfTKHkH3br+eLEbFt1ev4Kefvrnq/hiygkf5dU+fcTL/X+6vePwHkNZ3vkAXt81Oz1WmS+qe5j5WIA4R1nKQtgNcCfyPp/tQn8AHgAEkHkBVwu9XpSL4b2GRUUPIEWZNQbucp56cuXfxBYB/gv0XE08gKZgCln7N9CgC1nEPWzPQa4DeRdUjX8x/Aw8Ax6Zpz6n0wIq4FLgKeO839ZrK46v1uwL3p/b1kzWm1zk33e60rIkYi4jMRsR/wYuCVZM1/1gUcIGwuvIpslM5+ZM06S4FnA78kK1x+B9wHnCppq9QR+pJ07TeAD0l6gTJ7ScoLveuB10vqTR2sfzNDPrYhay9/NI0s+nR+IiLuA34M/J/Umd0v6ZCqay8GDgTeT9Z2X1dERPrMP5G17/97fk7SwanTfcd0vC9Zv8o1M+R9Op9KtaPnkPUbXJDSzwM+KWkwdTKfAnw3nfsm8FZJh0nqUTZEd9+ZflDqN3leqnWtJ2s2HG8h77YZcYCwuXA88K2I+HNE3J+/yDqI30D2Df5oYC/gz8A9wOsAIuJC4AtkTVKPkRXU26f7vj9d92i6z8Uz5OMMYAvgIbIC+bIp599EVuD9EXgQOCk/ERFPAj8kG5Z6UQPP/G2yb+wXRMSGqvRHyQLCTZL+mvKwgqzvoJ6T9dR5EA9NOf9z4A6yIbZfjIgrUvrngZXAjcBNZB3kn0/P8zuyYHI68Jd0j92Z2c5kfTDrgVvTdd9p4DorAWVfbsxstiSdAjwrIt4444c7QNm8kv8E+uv035jNSuGThcy6UWqSejubznEw6xpuYjKbJUnvIOvU/XFE/GKu82NWFDcxmZlZTa5BmJlZTV3VB7Fw4cJYsmTJXGfDzKw0rrvuuofSJM1NdFWAWLJkCStXrpzrbJiZlYakNfXOuYnJzMxqcoAwM7OaHCDMzKwmBwgzM6vJAcLMzGpygDAzs5ocIMzMrKaumgfRrC9ddTujY7Nbwn7vnbbh6AN2mfmDZmYl5QABfPXnq3lyZKzhz0dApa/HAcLMulphAULSPkzuZAWwJ9kOVtsC7yDboB3g4xFxaY3rjwT+BegFvhERpxaV11s+e+SsPn/GT/7EGT+5nfHxoKdHBeXKzGxuFRYgIuI2sq0kSdsRriXbKeutwOkR8cV616bPfwU4nGw3sWslXRIRtxSV39mo9PUCsHFsnIGe3jnOjZlZMTrVSX0YsDoi6q75McVBwB0RcWdEbATOJ9vwfbNQ6ct+bRtGvPWumXWvTgWIY8k2TM+9R9KNks6StF2Nz+9KtiFL7p6UtglJJ0paKWnlunXran2k7Sr9KUCMNt5vYWZWNoUHCEkLyDZlvzAl/RvwTLLmp/uA/9XK/SPizIgYioihwcGaK9a2Xd7EtGHUNQgz616dqEEsA1ZFxAMAEfFARIxFxDjwdbLmpKnWAourjheltM3CRBOTaxBm1sU6ESCOo6p5SdIzqs79HXBzjWuuBfaWtEeqgRwLXFJoLmchDxDD7oMwsy5WaICQtBXZSKSLqpJPk3STpBuBQ4EPpM/uIulSgIgYBd4DXA7cCnw/Iv5QZF5no9LvJiYz636FTpSLiMeBHaakvanOZ+8Fjqo6vhTYZH7E5sBNTGY2H3gtpiZMBgjXIMysezlANGFiFJP7IMysizlANMHzIMxsPnCAaIKbmMxsPvBqrk2Y7US5JzeO8Yov/ZJ1j20oMltmNk/tsPUCfvbhQ9t+XweIJkw0MTW4RPiDjw1z50OPc8izBtlrcOsis2Zm89DWlWIWDXWAaMJsm5jyCXWvG1rMK/Z/xgyfNjPbPLgPogkLemdXg8g7swf6/es2s/JwidUESVT6emZdgxjo994RZlYeDhBNml2AGJu4xsysLFxiNanS39vwPIg8kLgGYWZl4gDRpIH+noZnUuc1CPdBmFmZuMRqUqWvt4kmJtcgzKw8HCCalPVBNNbENJwCScU1CDMrEZdYTZpNJ/WGiSYm1yDMrDwcIJpU6ettuA8iDyQexWRmZeISq0mV/lk0MY2MIU1OsDMzKwOXWE2aVRPT6DgDfb1IKjhXZmbt4wDRpNmOYvIQVzMrG5daTar09TS8FtPwyJiHuJpZ6ThANCnrg5hFE5NrEGZWMi61mjT7JibXIMysXAoLEJL2kXR91Wu9pJOqzn9QUkhaWOf6saprLykqn82a1US5kXEPcTWz0ilsw6CIuA1YCiCpF1gLrEjHi4EjgD9Pc4snI2JpUflrVaWvl5GxYGw86O2ZfnTShtExKq5BmFnJdOpr7WHA6ohYk45PB04GokM/v+3yZTM2NtDMNDwy7iYmMyudTgWIY4HzACQdA6yNiBtmuGZA0kpJ10h6VeE5nKXJbUdnbmbKRjG5icnMyqXwPaklLQCWAx+TtCXwcbLmpZnsHhFrJe0J/FTSTRGxusb9TwROBNhtt93amPPp5cNWG+mo3jjqGoSZlU8nvtYuA1ZFxAPAM4E9gBsk3QUsAlZJ2nnqRRGxNv17J/Az4Pm1bh4RZ0bEUEQMDQ4OFvMENeQ1guEG5kIMj4wx4BqEmZVMJ0qt40jNSxFxU0TsGBFLImIJcA9wYETcX32BpO0kVdL7hcBLgFs6kNeG5X0QjdQghkfHvdS3mZVOoaWWpK2Aw4GLGvjskKRvpMNnAysl3QBcDZwaEZtXgMibmBpY0XXDyBgDnkltZiVTaB9ERDwO7DDN+SVV71cCJ6T3vwaeV2TeWjWrTmr3QZhZCbndo0mTAWL6GsTI2Dhj4+FRTGZWOi61mpTXCGaqQQx7NzkzKykHiCZNdFLP0AeR1zC8WJ+ZlY1LrSY1Og8ir0F4qQ0zKxsHiCY12kk9POL9qM2snFxqNanRTuo8gLgPwszKxgGiSXmT0Ux9EHkNwgHCzMrGAaJJjTYx5duSuonJzMrGpVaT+npEjxppYnINwszKyQGiSZIa2nZ0ch6Ef9VmVi4utVpQ6e+ZaEKqZ3g0b2JyDcLMysUBogXZvtQzNDGNeKKcmZWTS60WzKqJyTUIMysZB4gWZDWImZqY0kQ51yDMrGRcarUg64NosInJNQgzKxkHiBY01MQ0OsaC3h56etShXJmZtYcDRAsaamIaGfMkOTMrJZdcLWhoFNPouFdyNbNScoBoQaWvd2KUUj3DI2Me4mpmpeSSqwWV/sbmQbiJyczKyCVXCyp9M49iymoQbmIys/JxgGhBNopphtVcR8cdIMyslBwgWjDQQBOT+yDMrKwKK7kk7SPp+qrXekknVZ3/oKSQtLDO9cdLuj29ji8qn61odB6EF+ozszLqK+rGEXEbsBRAUi+wFliRjhcDRwB/rnWtpO2BTwNDQADXSbokIh4pKr/NqPT1MDYejI6N09dbO9ZuGBl3DcLMSqlTJddhwOqIWJOOTwdOJiv8a3k5cGVEPJyCwpXAkcVnc3by9ZWmq0UMj455mQ0zK6VOBYhjgfMAJB0DrI2IG6b5/K7A3VXH96S0TUg6UdJKSSvXrVvXrvw2JG86mjZAjIx7oT4zK6XCSy5JC4DlwIWStgQ+DpzSrvtHxJkRMRQRQ4ODg+26bUMa2Zd6w4j7IMysnDrx1XYZsCoiHgCeCewB3CDpLmARsErSzlOuWQssrjpelNI2KxNNTNPMhRj2MFczK6lOBIjjSM1LEXFTROwYEUsiYglZ09GBEXH/lGsuB46QtJ2k7cg6tC/vQF5nZaYmpvHxYOOoZ1KbWTkVWnJJ2go4HLiogc8OSfoGQEQ8DHwOuDa9PpvSNiszNTFtHMu3G3UNwszKp7BhrgAR8TiwwzTnl1S9XwmcUHV8FnBWkflr1Uw1iIntRt1JbWYl5JKrBTP1QQyndHdSm1kZOUC0YKYmpjzdNQgzKyOXXC2YuYnJfRBmVl4OEC2YqQaR90F4FJOZlZFLrhbM3AeRNzG5BmFm5eMA0YKZmpjydPdBmFkZzVhySTpakku4GhpvYnINwszKp5GC/3XA7ZJOk7Rv0RkqkzxADNdrYnINwsxKbMaSKyLeCDwfWA2cLek3aQXVbQrP3Waur7eH3h7VH+bqGoSZlVhDX20jYj3wA+B84BnA35EtsvfeAvNWCpW+nvqd1KMe5mpm5dVIH8RySSuAnwH9wEERsQw4APhgsdnb/FX66u9LPVGDcBOTmZVQI2sxvRo4PSJ+UZ0YEU9Iensx2SqPgf7eaWZSpxqEm5jMrIQaCRD/CNyXH0jaAtgpIu6KiKuKylhZTFeDGB4Zo0fQ36sO58rMrHWNtH1cCFSXgGMpzcg6oKebKFfp60VygDCz8mkkQPRFxMb8IL1fUFyWyqXS3zNtE5OHuJpZWTVSeq2TtDw/kHQM8FBxWSqXmZqYPILJzMqqkT6IdwHnSvoyIOBu4M2F5qpEKn29PDlSbya1txs1s/KaMUBExGrghZK2Tsd/LTxXJVLp6+HRJzfWPLdh1DUIMyuvhrYclfQK4DnAQN7hGhGfLTBfpVHpn2ai3Mg4FQcIMyupRibKfZVsPab3kjUxvQbYveB8lUalr3faPgg3MZlZWTVSer04It4MPBIRnwFeBDyr2GyVR9ZJPd0oJtcgzKycGgkQw+nfJyTtAoyQrcdkNDCKyTUIMyupRvog/l3StsA/A6uAAL4+00WS9gEuqEraEzgF2AE4hmzy3YPAWyLi3hrXjwE3pcM/R8TyqZ/ZHFT660+U2zDqPggzK69pA0TaKOiqiHgU+KGkHwEDEfGXmW4cEbcBS9N9eoG1wAqypqpPpfT3kQWNd9W4xZMRsXQ2DzMX8iamiNhkxrRrEGZWZtOWXhExDnyl6nhDI8GhhsOA1RGxJi0dntuKrEZSWpW+HsYDRsc3fQz3QZhZmTXy9fYqSa9WawsKHQuclx9I+oKku4E3kNUgahmQtFLSNZJeVe/GafOilZJWrlu3roUsNme6famzmdSuQZhZOTVSer2TbHG+DZLWS3pM0vqZLspJWgAsp2qBv4j4REQsBs4F3lPn0t0jYgh4PXCGpGfW+lBEnBkRQxExNDg42Gi22ibf62HDlNnUETGxWJ+ZWRk1suXoNhHRExELIuJp6fhps/gZy4BVEfFAjXPnku03Uevnrk3/3km2WdHzZ/EzOyaf5zC1BjE6HoyH96M2s/KacRSTpENqpU/dQGgax/HU5qW9I+L2dHgM8McaP3M74ImI2CBpIfAS4LQGf15H5TWE4Sk1iPzYfRBmVlaNDHP9cNX7AeAg4DrgZTNdKGkr4HCyZqrcqWkI7DiwhjSCSdIQ8K6IOAF4NvA1SeNktZxTI+KWBvLacfVqEMNp6KtnUptZWTWyWN/R1ceSFgNnNHLziHicbN5DdVq9JqWVwAnp/a+B5zXyM+baRB/ElACRz672PAgzK6tmvt7eQ/YN36gaxbRJE1Paj9oBwsxKqpE+iH9lcq5CD9nkt1VFZqpMBurUIPI+CDcxmVlZNdIHsbLq/ShwXkT8qqD8lE69eRD5sWsQZlZWjQSIHwDDETEG2bIZkraMiCeKzVo5THZSP7WJKW9y8lIbZlZWDc2kBraoOt4C+Ekx2SmfyT6IKU1M7qQ2s5JrJEAMVG8zmt5vWVyWyqXuKKaJTmrXIMysnBopvR6XdGB+IOkFwJPFZalc6jUx5TWIAS+1YWYl1UgfxEnAhZLuJdtydGeyLUiN+p3UExPlXIMws5JqZKLctZL2BfZJSbdFxEix2SqPBXkNYmRqE5NrEGZWbjN+vZX0bmCriLg5Im4Gtpb0D8VnrRx6e0R/r2o0MXmYq5mVWyPtH+9IO8oBEBGPAO8oLkvlU+nr9UQ5M+s6jZRevdWbBaXtQxcUl6XyybcdrTY8Ms6C3h56elrZZ8nMbO400kl9GXCBpK+l43cCPy4uS+VT6evZtA9idMwd1GZWao0EiI8AJ5KW5QZuJBvJZEmlv1YTk/ejNrNya2RHuXHgt8BdZHtBvAy4tdhslUutJqYNI2PufzCzUqtbg5D0LLLd4I4DHgIuAIiIQzuTtfLIAsSmi/W5BmFmZTZdE9MfgV8Cr4yIOwAkfaAjuSqZSl/vpmsxjYx5mQ0zK7XpSrD/AdwHXC3p65IOI5tJbVNU+muMYhodm5hlbWZWRnUDRERcHBHHAvsCV5MtubGjpH+TdESnMlgGNZuYRsZdgzCzUmukk/rxiPhe2pt6EfB7spFNllT6eicmxuWGR8e8zIaZldqsvuJGxCMRcWZEHFZUhsqoVg1ieGTc8yDMrNRcgrVB1gex6UQ51yDMrMwKCxCS9pF0fdVrvaSTJH1O0o0p7QpJu9S5/nhJt6fX8UXlsx2yUUybLrXh3eTMrMwamUndlIi4DVgKE+s3rQVWAI9ExKdS+vuAU5icpU1K3x74NDAEBHCdpEvSQoGbnVo1iGFPlDOzkutUCXYYsDoi1kTE+qr0rcgCwFQvB66MiIdTULgSOLID+WxKvpprxOSjeKKcmZVdYTWIKY4FzssPJH0BeDPwF6DWzOxdgburju9JaZuQdCLZWlHstttubcru7OQ1hY1j41T6ehkfDzaOepirmZVb4SWYpAXAcuDCPC0iPhERi4Fzgfe0cv80qmooIoYGBwdby2yTJvelHn/Kv54oZ2Zl1omvuMuAVRHxQI1z5wKvrpG+FlhcdbwopW2W8s7ofLmNfFa1axBmVmadKMGO46nNS3tXnTuGbM2nqS4HjpC0naTtgCNS2mZpsgaRBYbhEW83amblV2gfhKStgMPJNhnKnSppH2AcWEMawSRpCHhXRJwQEQ9L+hxwbbrmsxHxcJF5bcXUJiZvN2pm3aDQABERjwM7TEmr1aRERKwETqg6Pgs4q8j8tUve15A3MQ1PNDG5BmFm5eWvuG2QL6mRNzFtmGhi8q/XzMrLJVgb1Gti8lIbZlZmDhBtMNHElAeIfJiraxBmVmIuwdpgogYxMvaUfz0PwszKzAGiDQb6pzQxjXqYq5mVnwNEG2zSxORhrmbWBVyCtcHUiXIbXIMwsy7gANEGU+dB5H0QHuZqZmXmEqwNKlP7INxJbWZdwAGiDRb0Zr/GPDBsGB2nR9Dfq7nMlplZSxwg2qCnRyzo7XlKDWKgvxfJAcLMyssBok0qfT1PWc3VI5jMrOxcirVJpb+3asOgMY9gMrPSc4Bok0pfz+RqriPej9rMys8Bok0q/dVNTGNuYjKz0nMp1iaVvt6nLLVRcQ3CzErOAaJNsk7qyYlyA65BmFnJuRRrk6wPIjUxuQZhZl3AAaJNnjKKyTUIM+sCLsXa5ClNTKMexWRm5ecA0SZPnSg35oX6zKz0XIq1SaWvt2oexJgX6jOz0usr6saS9gEuqEraEzgF2BU4GtgIrAbeGhGP1rj+LuAxYAwYjYihovLaDtk8iOomJsdeMyu3wkqxiLgtIpZGxFLgBcATwArgSuC5EbE/8CfgY9Pc5tB0j806OMBkE1NETCzWZ2ZWZp36mnsYsDoi1kTEFRExmtKvARZ1KA+FyifKjYwF4+HtRs2s/DpVih0LnFcj/W3Aj+tcE8AVkq6TdGK9G0s6UdJKSSvXrVvXhqw2p9LXw8bRcYZH893kXIMws3IrPEBIWgAsBy6ckv4JYBQ4t86lB0fEgcAy4N2SDqn1oYg4MyKGImJocHCwjTmfnXxXufVPjqRjBwgzK7dO1CCWAasi4oE8QdJbgFcCb4iIqHVRRKxN/z5I1ndxUPFZbV4+aukveYBwE5OZlVwnSrHjqGpeknQkcDKwPCKeqHWBpK0kbZO/B44Abu5AXpuWB4T1T2bdK25iMrOyKzRApML9cOCiquQvA9sAV0q6XtJX02d3kXRp+sxOwH9IugH4HfD/IuKyIvPaqjxA5DUIL7VhZmVX2DwIgIh4HNhhStpedT57L3BUen8ncECReWu3vM/BfRBm1i38NbdNXIMws27jUqxNNgkQrkGYWck5QLRJHhAmRjF5qQ0zKzmXYm2yaROTaxBmVm4OEG2Sz4NYP+wmJjPrDg4QbZI3KU32QfhXa2bl5lKsTaY2MXk/CDMrOweINploYkozqb3UhpmVnUuxNqlerG9BXw89PZrjHJmZtcYBok3yGsPGsXHXHsysK7gka5MFvZO/So9gMrNu4ADRJpImag4ewWRm3cAlWRvlAcIjmMysGzhAtFG+gqtrEGbWDVyStdFEE5NrEGbWBRwg2miiick1CDPrAi7J2ijve3ANwsy6gQNEG+U1Bw9zNbNu4ADRRpOjmPxrNbPyc0nWRnkTk/ejNrNu4ADRRp4oZ2bdxCVZG+U1B0+UM7Nu4ADRRq5BmFk3Kawkk7SPpOurXuslnSTpnyX9UdKNklZI2rbO9UdKuk3SHZI+WlQ+22nAo5jMrIsUFiAi4raIWBoRS4EXAE8AK4ArgedGxP7An4CPTb1WUi/wFWAZsB9wnKT9ispru0zOg3ANwszKr1Ml2WHA6ohYExFXRMRoSr8GWFTj8wcBd0TEnRGxETgfOKZDeW3a5Exq1yDMrPw6FSCOBc6rkf424Mc10ncF7q46vielbULSiZJWSlq5bt26ljPaiokahPsgzKwLFF6SSVoALAcunJL+CWAUOLeV+0fEmRExFBFDg4ODrdyqZRMzqT2Kycy6QF8HfsYyYFVEPJAnSHoL8ErgsIiIGtesBRZXHS9KaZs1L9ZnZt2kEyXZcVQ1L0k6EjgZWB4RT9S55lpgb0l7pBrIscAlhee0RV6sz8y6SaEBQtJWwOHARVXJXwa2Aa5Mw1+/mj67i6RLAVIn9nuAy4Fbge9HxB+KzGs7uJPazLpJoU1MEfE4sMOUtL3qfPZe4Kiq40uBS4vMX7vlTUterM/MuoFLsjY6eK+FvPNv9mTfnbeZ66yYmbWsE53U88a2Wy7gY8uePdfZMDNrC9cgzMysJgcIMzOryQHCzMxqcoAwM7OaHCDMzKwmBwgzM6vJAcLMzGpygDAzs5pUezHVcpK0DljT5OULgYfamJ2y8HPPL37u+aWR5949ImruldBVAaIVklZGxNBc56PT/Nzzi597fmn1ud3EZGZmNTlAmJlZTQ4Qk86c6wzMET/3/OLnnl9aem73QZiZWU2uQZiZWU0OEGZmVtO8DxCSjpR0m6Q7JH10rvNTJElnSXpQ0s1VadtLulLS7enf7eYyj+0mabGkqyXdIukPkt6f0rv6uQEkDUj6naQb0rN/JqXvIem36W/+AkkL5jqv7SapV9LvJf0oHXf9MwNIukvSTZKul7QypTX9tz6vA4SkXuArwDJgP+A4SfvNba4KdTZw5JS0jwJXRcTewFXpuJuMAh+MiP2AFwLvTv+Nu/25ATYAL4uIA4ClwJGSXgj8E3B62h/+EeDtc5jHorwfuLXqeD48c+7QiFhaNf+h6b/1eR0ggIOAOyLizojYCJwPHDPHeSpMRPwCeHhK8jHAOen9OcCrOpqpgkXEfRGxKr1/jKzQ2JUuf26AyPw1HfanVwAvA36Q0rvu2SUtAl4BfCMdiy5/5hk0/bc+3wPErsDdVcf3pLT5ZKeIuC+9vx/YaS4zUyRJS4DnA79lnjx3amq5HngQuBJYDTwaEaPpI934N38GcDIwno53oPufORfAFZKuk3RiSmv6b72v3bmz8oqIkNSV454lbQ38EDgpItZnXyoz3fzcETEGLJW0LbAC2HeOs1QoSa8EHoyI6yS9dK7zMwcOjoi1knYErpT0x+qTs/1bn+81iLXA4qrjRSltPnlA0jMA0r8PznF+2k5SP1lwOAna8HoAAAMGSURBVDciLkrJXf/c1SLiUeBq4EXAtpLyL4fd9jf/EmC5pLvImoxfBvwL3f3MEyJibfr3QbIvBAfRwt/6fA8Q1wJ7pxEOC4BjgUvmOE+ddglwfHp/PPB/5zAvbZfan78J3BoR/7vqVFc/N4CkwVRzQNIWwOFkfTBXA/8zfayrnj0iPhYRiyJiCdn/zz+NiDfQxc+ck7SVpG3y98ARwM208Lc+72dSSzqKrM2yFzgrIr4wx1kqjKTzgJeSLQH8APBp4GLg+8BuZEulvzYipnZkl5akg4FfAjcx2Sb9cbJ+iK59bgBJ+5N1SvaSfRn8fkR8VtKeZN+utwd+D7wxIjbMXU6LkZqYPhQRr5wPz5yecUU67AO+FxFfkLQDTf6tz/sAYWZmtc33JiYzM6vDAcLMzGpygDAzs5ocIMzMrCYHCDMzq8kBwmwGksbS6pj5q20L+0laUr26rtnmxEttmM3syYhYOteZMOs01yDMmpTW3j8trb//O0l7pfQlkn4q6UZJV0naLaXvJGlF2p/hBkkvTrfqlfT1tGfDFWnWM5Lel/axuFHS+XP0mDaPOUCYzWyLKU1Mr6s695eIeB7wZbIZ+QD/CpwTEfsD5wJfSulfAn6e9mc4EPhDSt8b+EpEPAd4FHh1Sv8o8Px0n3cV9XBm9XgmtdkMJP01IraukX4X2YY8d6YFAe+PiB0kPQQ8IyJGUvp9EbFQ0jpgUfUSD2kJ8ivTZi5I+gjQHxGfl3QZ8Fey5VAurtrbwawjXIMwa03UeT8b1WsCjTHZN/gKsh0PDwSurVqN1KwjHCDMWvO6qn9/k97/mmwlUYA3kC0WCNl2j38PExv5PL3eTSX1AIsj4mrgI8DTgU1qMWZF8jcSs5ltkXZly10WEflQ1+0k3UhWCzgupb0X+JakDwPrgLem9PcDZ0p6O1lN4e+B+6itF/huCiICvpT2dDDrGPdBmDUp9UEMRcRDc50XsyK4icnMzGpyDcLMzGpyDcLMzGpygDAzs5ocIMzMrCYHCDMzq8kBwszMavr/1xwE1faLJccAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}