{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea6f0b2",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0fae5f",
   "metadata": {},
   "source": [
    "## Insructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d788d546",
   "metadata": {},
   "source": [
    "To use this playground, follow these steps:\n",
    "\n",
    "1. Run all the cells till the cell titled \"Data\"\n",
    "2. The data cell can be use to create data as per need. You can vary the following parameters\n",
    "    - classes - decides number of output classes\n",
    "    - features - decides number of input features\n",
    "    - size - decides the size of the dataset\n",
    "    \n",
    "   The dataset is divided into 3 parts automatically:\n",
    "    1. Train 70%\n",
    "    2. Test 20%\n",
    "    3. Validation 10%\n",
    "    \n",
    "   The data is in a zipped format which has to be unzipped using the following code:\n",
    "   ```\n",
    "   X,Y = zip(*T_train)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "   ```\n",
    "    \n",
    "3. The next cell title \"Create Model and Train\" is used to create the neural network and train it. Use the following code to create the network. As can be seen, features size is already set as input layer size and classes is set as output layer size.\n",
    "    ```\n",
    "    n = NeuralNet()\n",
    "    n.add(Input(features))\n",
    "    n.add(Hidden(5,TanH()))\n",
    "    n.add(Hidden(10,TanH()))\n",
    "    n.add(Hidden(5,TanH()))\n",
    "    n.add(Output(classes,Sigmoid()))\n",
    "    n.train(X,Y,batch_size=50,learning_rate=1,epochs=100,val_set=T_val)\n",
    "    ```\n",
    "4. Once the model is trained, you can use it to predict new data. Use the following code to do so:\n",
    "    ```\n",
    "    X_test,Y_test = zip(*T_test)\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "\n",
    "    Y_pred = n.predict(X_test)\n",
    "    comp= np.sum(np.all(np.equal(Y_pred, Y_test), axis=1))/len(Y_test)\n",
    "    print(comp)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a76c6d",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46085497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from IPython.display import display\n",
    "from math import floor\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa0f36b",
   "metadata": {},
   "source": [
    "## Classes and Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "000e6da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        Activation.__init__(self)\n",
    "        \n",
    "    def map(self,x):\n",
    "        z = np.exp(-x)\n",
    "        sig = 1 / (1 + z)\n",
    "\n",
    "        return sig\n",
    "    \n",
    "class TanH(Activation):\n",
    "    def __init__(self):\n",
    "        Activation.__init__(self)\n",
    "        \n",
    "    def map(self,x):\n",
    "        z1 = np.exp(x)\n",
    "        z2 = np.exp(-x)\n",
    "        tanh = (z1 - z2)/ (z1 + z2)\n",
    "\n",
    "        return tanh\n",
    "    \n",
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        Activation.__init__(self)\n",
    "    \n",
    "    def map(self,x):\n",
    "        return [(n if n > 0 else 0) for n in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "982c6932",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self,size,act: Activation):\n",
    "        self.size = size\n",
    "        self.activation = act\n",
    "        self.next = None\n",
    "        self.prev = None\n",
    "        self.x = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.a = None\n",
    "    \n",
    "class Input(Layer):\n",
    "    def __init__(self,size):\n",
    "        Layer.__init__(self,size,None)\n",
    "    \n",
    "class Output(Layer):\n",
    "    def __init__(self,size,act: Activation):\n",
    "        Layer.__init__(self,size,act)\n",
    "    \n",
    "class Hidden(Layer):\n",
    "    def __init__(self,size,act: Activation):\n",
    "        Layer.__init__(self,size,act)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5bd3988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add(self,layer: Layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "        if(self.__depth() > 1):\n",
    "            last = self.layers[-2]\n",
    "            layer.prev = last\n",
    "            last.next = layer\n",
    "            \n",
    "            last.weights = np.random.rand(layer.size,last.size)\n",
    "            \n",
    "        layer.bias = np.random.rand(1,layer.size)\n",
    "        layer.a = np.zeros(layer.size)\n",
    "             \n",
    "    def __depth(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "    def __parameters(self):\n",
    "        total = 0\n",
    "        for i in range(0,self.__depth()-1):\n",
    "            total = total + self.layers[i].size * self.layers[i+1].size + self.layers[i+1].size\n",
    "            \n",
    "        return total\n",
    "    \n",
    "    def summary(self):\n",
    "        print(\"The neural network is {} layers deep and has {} parameters.\".format(self.__depth(), self.__parameters()))\n",
    "    \n",
    "    def train(self,X,Y,learning_rate=0.001,batch_size=100,epochs=10,val_set=None):\n",
    "        self.__validate(X,Y)\n",
    "        error = 0\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            start = time.process_time()\n",
    "            dh = display('Epoch {}'.format(j+1),display_id=True)\n",
    "            \n",
    "            for i in range(0,len(X),batch_size):\n",
    "                error,gradient = self.__forward_pass(X[i:i+batch_size],Y[i:i+batch_size])\n",
    "                self.__backward_pass(learning_rate,gradient)\n",
    "                dh.update(\"Epoch {}  Error:{} Processed: {} out of {}\".format(j+1,round(error,3),i+batch_size,len(X)))\n",
    "            \n",
    "            if(val_set is not None):\n",
    "                x_val,y_val = zip(*val_set)\n",
    "                x_val = np.array(x_val)\n",
    "                y_val = np.array(y_val)\n",
    "                \n",
    "                y_pred = self.predict(x_val)\n",
    "                comp= np.sum(np.all(np.equal(y_pred, y_val), axis=1))/len(y_val)\n",
    " \n",
    "                start = round(time.process_time() - start,2)\n",
    "                dh.update(\"Epoch {}  Error:{} Accuracy: {} Execution time: {}s\".format(j+1,round(error,3),comp,start))\n",
    "            else:\n",
    "                dh.update(\"Epoch {}  Error:{} Processed: {} out of {}\".format(j+1,round(error,3),i+batch_size,len(X)))\n",
    "            \n",
    "    def __forward_pass(self,X,Y):\n",
    "        error = 0\n",
    "        gradient = np.zeros(len(Y[0]))\n",
    "        \n",
    "        for (x,y) in zip(X,Y):\n",
    "            for n,l in enumerate(self.layers):\n",
    "                if(n == 0):\n",
    "                    l.a = x\n",
    "                elif (n == self.__depth()- 1):\n",
    "                    l.x = l.prev.a\n",
    "                    z = np.add(np.dot(l.prev.weights,l.prev.a),l.bias)[0]\n",
    "                    l.a = l.activation.map(z)\n",
    "                    error = error + self.__cost(y,l.a)\n",
    "                    gradient = (gradient + l.a - y) * self.__derivative(l.a,l.activation)\n",
    "                else:\n",
    "                    l.x = l.prev.a\n",
    "                    z = np.add(np.dot(l.prev.weights,l.prev.a),l.bias)[0]\n",
    "                    l.a = l.activation.map(z)\n",
    "            \n",
    "        error = error / len(X)\n",
    "        return error,gradient\n",
    "    \n",
    "    def __backward_pass(self,eta,gradient):\n",
    "        delta = []\n",
    "        for n,l in enumerate(self.layers[::-1]):\n",
    "            if(n == self.__depth()- 1):\n",
    "                pass\n",
    "            elif (n == 0):\n",
    "                delta.append(gradient)\n",
    "            else:\n",
    "                delta.append(np.sum(delta[-1] * l.weights.T,axis=1) * self.__derivative(l.a,l.activation))\n",
    "                \n",
    "        delta.reverse()\n",
    "        \n",
    "        for n,l in enumerate(self.layers):\n",
    "            if(n == self.__depth()- 1):\n",
    "                pass\n",
    "            else:\n",
    "                d = eta*delta[n]*(np.ones(l.weights.shape) * l.a).T\n",
    "                l.weights -= d.T\n",
    "                l.next.bias -= eta*delta[n]\n",
    "                \n",
    "    \n",
    "    def __cost(self,actual,predicted):\n",
    "        cost = np.sum(np.power(actual - predicted,2))/2\n",
    "        return cost\n",
    "    \n",
    "    def __derivative(self,val,act):\n",
    "        if(type(act).__name__ == \"Sigmoid\"):\n",
    "            return val * (1 - val)\n",
    "        \n",
    "        if(type(act).__name__ == \"TanH\"):\n",
    "            return (1 - val) * (1 + val)\n",
    "        \n",
    "        if(type(act).__name__ == \"ReLU\"):\n",
    "            return [ n > 0 for n in val]\n",
    "        \n",
    "    \n",
    "    def __validate(self,X,Y):\n",
    "        if(self.__depth() < 3):\n",
    "            raise Exception(\"Error: Layers not sufficient to train neural network\")\n",
    "        \n",
    "        if(not(isinstance(self.layers[0],Input))):\n",
    "            raise Exception(\"Error: First layer of the network should be an input layer\")\n",
    "            \n",
    "        if(not(isinstance(self.layers[-1],Output))):\n",
    "            raise Exception(\"Error: Last layer of the network should be an output layer\")\n",
    "           \n",
    "        assert Y.shape[0] == X.shape[0],\"Error: X and Y must have equal number of training instances\"\n",
    "        \n",
    "        shape = np.shape(X) \n",
    "        assert shape[0] > 0,\"Error: Input data array is empty\"\n",
    "        assert shape[1] == self.layers[0].size,\"Error: Input data dimensions must match with input layer dimension\"\n",
    "        \n",
    "        shape = np.shape(Y) \n",
    "        assert shape[0] > 0,\"Error: Output data array is empty\"\n",
    "        assert shape[1] == self.layers[-1].size,\"Error: Output data dimensions must match with output layer dimension\"            \n",
    "        \n",
    "    def predict(self,X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            for n,l in enumerate(self.layers):\n",
    "                if(n == 0):\n",
    "                    l.a = x\n",
    "                elif (n == self.__depth()- 1):\n",
    "                    l.x = l.prev.a\n",
    "                    z = np.add(np.dot(l.prev.weights,l.prev.a),l.bias)[0]\n",
    "                    l.a = l.activation.map(z)\n",
    "                    y_pred.append(l.activation.map(z))\n",
    "                else:\n",
    "                    l.x = l.prev.a\n",
    "                    z = np.add(np.dot(l.prev.weights,l.prev.a),l.bias)[0]\n",
    "                    l.a = l.activation.map(z)\n",
    "        \n",
    "        y_pred = np.array(y_pred)\n",
    "        temp = np.zeros_like(y_pred)\n",
    "        temp[np.arange(len(y_pred)), y_pred.argmax(1)] = 1\n",
    "        y_pred = temp           \n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c2f11",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5a7a15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 54 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classes = 4\n",
    "size = 10000\n",
    "features = 4\n",
    "\n",
    "X, Y = make_blobs(n_samples=size, centers=classes, n_features=features)\n",
    "temp = np.zeros([len(Y),classes])\n",
    "temp[np.arange(len(Y)), Y] = 1\n",
    "Y = temp\n",
    "T = list(zip(X,Y))\n",
    "random.shuffle(T)\n",
    "\n",
    "T_val = T[:floor(0.1*size)]\n",
    "T_test = T[-floor(0.2*size):]\n",
    "T_train = T[floor(0.1*size):floor(0.8*size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3656b819",
   "metadata": {},
   "source": [
    "## Create Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a5eae357",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Epoch 1  Error:0.668 Accuracy: 0.511 Execution time: 2.69s'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Epoch 2  Error:0.379 Accuracy: 0.65 Execution time: 2.64s'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Epoch 3  Error:0.279 Accuracy: 0.751 Execution time: 2.7s'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Epoch 4  Error:0.224 Accuracy: 0.751 Execution time: 2.67s'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Epoch 5  Error:0.155 Accuracy: 0.91 Execution time: 2.61s'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Epoch 6  Error:0.069 Accuracy: 0.973 Execution time: 2.67s'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Epoch 7  Error:0.051 Accuracy: 0.975 Execution time: 2.59s'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Epoch 8  Error:0.04 Accuracy: 0.98 Execution time: 2.69s'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Epoch 9  Error:0.032 Accuracy: 0.982 Execution time: 2.67s'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Epoch 10  Error:0.027 Accuracy: 0.986 Execution time: 2.81s'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X,Y = zip(*T_train)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "n = NeuralNet()\n",
    "n.add(Input(features))\n",
    "n.add(Hidden(10,TanH()))\n",
    "n.add(Hidden(10,TanH()))\n",
    "n.add(Output(classes,Sigmoid()))\n",
    "n.train(X,Y,batch_size=50,learning_rate=0.05,epochs=10,val_set=T_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65cd22d",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea130224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 99.1%\n",
      "\n",
      "Confusion Matrix\n",
      "\n",
      "[[497   0   0   0]\n",
      " [  0 481   0   0]\n",
      " [  0  11 491   0]\n",
      " [  0   0   7 513]]\n",
      "Wall time: 354 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test,Y_test = zip(*T_test)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "Y_pred = n.predict(X_test)\n",
    "acc= np.sum(np.all(np.equal(Y_pred, Y_test), axis=1))/len(Y_test)\n",
    "print(\"Accuracy {}%\".format(acc*100))\n",
    "print(\"\\nConfusion Matrix\\n\")\n",
    "print(confusion_matrix(Y_test.argmax(1), Y_pred.argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559baec",
   "metadata": {},
   "source": [
    "## Network Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1a6a8cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The neural network is 4 layers deep and has 204 parameters.\n"
     ]
    }
   ],
   "source": [
    "n.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
