{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### This assignment makes use of PIMA healthcare dataset. The problem revolves around predicting the degree of diabetes found in patients. Firstly the data is cleaned, encoded and then scaled via data pre-processing class. Later Pytorch is used to create the VAE model. It is then trained and the synthetic dataset is created at the end. "
      ],
      "metadata": {
        "id": "UianVqLiBhEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "id": "TCjbktMov266",
        "outputId": "8768296d-5144-4973-b8b4-94e3ee581c23"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdt\n",
            "  Downloading rdt-0.6.2-py2.py3-none-any.whl (35 kB)\n",
            "Collecting psutil<6,>=5.7\n",
            "  Downloading psutil-5.9.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 25.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas<2,>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from rdt) (1.1.5)\n",
            "Collecting scikit-learn<1,>=0.24\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml<6,>=5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 60.8 MB/s \n",
            "\u001b[?25hCollecting scipy<2,>=1.5.4\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting numpy<2,>=1.20.0\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 322 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>=1.1.3->rdt) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>=1.1.3->rdt) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas<2,>=1.1.3->rdt) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1,>=0.24->rdt) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1,>=0.24->rdt) (3.0.0)\n",
            "Installing collected packages: numpy, scipy, scikit-learn, pyyaml, psutil, rdt\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.5 psutil-5.9.0 pyyaml-5.4.1 rdt-0.6.2 scikit-learn-0.24.2 scipy-1.7.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDpm-tALPzjw",
        "outputId": "030f535d-b59c-4f18-988b-92a533a7a3a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rdt.transformers import OneHotEncodingTransformer\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "class DataProcessing(): # making a class for preprocessing of the data. \n",
        "  def __init__(self, df,disc_name,cont_name):\n",
        "        super(DataProcessing, self).__init__()\n",
        "        self.data=df[disc_name+cont_name]\n",
        "        self.disc_colmns=disc_name\n",
        "        self.cont_colmns=cont_name\n",
        "  def show(self): \n",
        "    print(self.data)\n",
        "  def transformData(self): # one hotencoding of the data\n",
        "    new_data=[]\n",
        "    for i in range(len(self.disc_colmns)):\n",
        "      ohe = OneHotEncodingTransformer()\n",
        "      fit_data = pd.DataFrame(self.data, columns=[self.disc_colmns[i]])\n",
        "      ohe.fit(fit_data, self.disc_colmns[i])\n",
        "      num_categories = len(ohe.dummies)\n",
        "      print(num_categories)\n",
        "      if i==0:\n",
        "        new_data=ohe.transform(fit_data).to_numpy()\n",
        "      else:\n",
        "        new_data=np.concatenate((new_data, ohe.transform(fit_data).to_numpy()), axis=1)\n",
        "    self.transformations=[]\n",
        "    for i in range(len(self.cont_colmns)): # std scaling of the data\n",
        "      scaler = preprocessing.StandardScaler()\n",
        "      fit_data = pd.DataFrame(self.data, columns=[self.cont_colmns[i]]).to_numpy()\n",
        "      scaler.fit(fit_data)\n",
        "      new_data=np.concatenate((new_data, scaler.transform(fit_data)), axis=1)\n",
        "      self.transformations.append((scaler.mean_,scaler.scale_))\n",
        "    print('Before Transformation: ')\n",
        "    print(self.data.head(5))\n",
        "    print('After Transformation: ')\n",
        "    df=pd.DataFrame(new_data,columns=['Outcomes1','Outcomes2','Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigree','Age'])\n",
        "    print(df.head(5))\n",
        "    return new_data\n",
        "  def transformBack(self,data):\n",
        "    clmns=columns=['Outcomes1','Outcomes2','Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigree','Age']\n",
        "    df_prev=pd.DataFrame(data,columns=clmns)\n",
        "    new_data=[]\n",
        "    new_data.append(np.argmax(data[0,0:2]))\n",
        "    print(len(self.cont_colmns))\n",
        "    for i in range(len(self.cont_colmns)):\n",
        "      m_,v_=self.transformations[i]\n",
        "      new_data.append(np.rint(v_*data[0,1+i]+m_))\n",
        "    new_data=[new_data]\n",
        "    print('Synthesized Data (Normed): ')\n",
        "    print(df_prev.head(5))\n",
        "    print('Synthesized Data (De-Normed): ')\n",
        "    df=pd.DataFrame(new_data,columns=['Outcomes','Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigree','Age'])\n",
        "    print(df.head(5))\n",
        "def GHData():\n",
        "  df=pd.read_csv('/content/drive/MyDrive/DL/diabetes.csv')\n",
        "  dp=DataProcessing(df,['Outcomes'],['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigree','Age'])\n",
        "  transform_data=dp.transformData()\n",
        "  print('Dimension of Dataset: ', transform_data.shape)\n",
        "  return transform_data,dp  "
      ],
      "metadata": {
        "id": "soQet-eWtGzI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vv,dp=GHData()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r72dcUBaWuKz",
        "outputId": "0f4ed6b1-f2f7-4fa3-f809-9b86452bcb17"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "Before Transformation: \n",
            "   Outcomes  Pregnancies  Glucose  ...   BMI  DiabetesPedigree  Age\n",
            "0         1            6      148  ...  33.6             0.627   50\n",
            "1         0            1       85  ...  26.6             0.351   31\n",
            "2         1            8      183  ...  23.3             0.672   32\n",
            "3         0            1       89  ...  28.1             0.167   21\n",
            "4         1            0      137  ...  43.1             2.288   33\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "After Transformation: \n",
            "   Outcomes1  Outcomes2  Pregnancies  ...       BMI  DiabetesPedigree       Age\n",
            "0        1.0        0.0     0.639947  ...  0.204013          0.468492  1.425995\n",
            "1        0.0        1.0    -0.844885  ... -0.684422         -0.365061 -0.190672\n",
            "2        1.0        0.0     1.233880  ... -1.103255          0.604397 -0.105584\n",
            "3        0.0        1.0    -0.844885  ... -0.494043         -0.920763 -1.041549\n",
            "4        1.0        0.0    -1.141852  ...  1.409746          5.484909 -0.020496\n",
            "\n",
            "[5 rows x 10 columns]\n",
            "Dimension of Dataset:  (768, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch as T\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = T.device(\"cpu\") \n",
        "\n",
        "#####################  Dataset ################################################\n",
        "\n",
        "class PIMADataset(T.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, data, n_rows=None):\n",
        "    self.x_data = T.tensor(data, dtype=T.float32).to(device) \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x_data[idx]\n",
        "\n",
        "################### Variational Autoencoders ###################################\n",
        "\n",
        "class VAE(T.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(VAE, self).__init__()\n",
        "\n",
        "    self.input_dim = 10\n",
        "    self.latent_dim = 256\n",
        "    self.dec1=64\n",
        "    self.dec2=64\n",
        "    self.enc1=256\n",
        "    self.enc2=128\n",
        "\n",
        "    self.fc1 = T.nn.Linear(self.input_dim, self.enc1)\n",
        "    self.fc11 = T.nn.Linear(self.enc1,self.enc2)\n",
        "    self.fc12 = T.nn.Linear(self.enc2, 64)\n",
        "    self.fc2a = T.nn.Linear(64, self.latent_dim) \n",
        "    self.fc2b = T.nn.Linear(64, self.latent_dim) \n",
        "    self.fc3 = T.nn.Linear(self.latent_dim, self.dec1)\n",
        "    self.fc31 = T.nn.Linear(self.dec1, self.dec2)\n",
        "    self.fc41 = T.nn.Linear(self.dec1, 2)\n",
        "    self.fc5 = T.nn.Linear(self.dec1, 8)\n",
        "\n",
        "  def encode(self, x):\n",
        "    z = T.relu(self.fc1(x))    \n",
        "    z = T.relu(self.fc11(z))    \n",
        "    z = T.relu(self.fc12(z))    \n",
        "    mean = T.sigmoid(self.fc2a(z))\n",
        "    logvar = T.sigmoid(self.fc2b(z))           \n",
        "    return (mean, logvar)\n",
        "\n",
        "  def decode(self, z): \n",
        "    z = T.relu(self.fc3(z)) \n",
        "    z = T.relu(self.fc31(z))  \n",
        "    z11 = T.softmax(self.fc41(z),1)  \n",
        "    z2 = self.fc5(z) \n",
        "    return (z11,z2)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    (mean, logvar) = self.encode(x) \n",
        "    stdev = T.exp(0.5 * logvar)\n",
        "    noise = T.randn_like(stdev)\n",
        "    inpt = mean + (noise * stdev)   \n",
        "    z11,z2 = self.decode(inpt)     \n",
        "    return (z11,z2, mean, logvar)\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def cus_loss_func(z1,z2, x, mean, logvar, beta=1.0):\n",
        "  bce = T.nn.functional.cross_entropy(z1, x[:,0:2])\n",
        "  bmse = T.nn.functional.mse_loss(z2, x[:,2:10])\n",
        "  kld = -0.5 * T.sum(1 + logvar - T.pow(mean, 2) - \\\n",
        "    T.exp(logvar))\n",
        "  return bce + bmse+(beta * kld)  # beta weights KLD component\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def train(vae, ds, bs, me, le, lr, beta):\n",
        "\n",
        "  vae.train()\n",
        "  data_ldr = T.utils.data.DataLoader(ds, batch_size=bs,\n",
        "    shuffle=True)\n",
        "  opt = T.optim.Adam(vae.parameters(), lr=lr)\n",
        "  print(\"\\nStarting training\")\n",
        "  for epoch in range(0, me):\n",
        "    epoch_loss = 0.0\n",
        "    for (bat_idx, batch) in enumerate(data_ldr):\n",
        "      opt.zero_grad()\n",
        "      (z1,z2, mean, logvar) = vae(batch)\n",
        "      loss_val = cus_loss_func(z1,z2, batch, mean, \\\n",
        "        logvar, beta)\n",
        "      loss_val.backward()\n",
        "      epoch_loss += loss_val.item() \n",
        "      opt.step()\n",
        "    if epoch % le == 0:\n",
        "      print(\"epoch = %4d   loss = %0.4f\" % (epoch, epoch_loss))\n",
        "  print(\"Done \")\n",
        "\n",
        "################################# Main Function ############################### \n",
        "\n",
        "def main():\n",
        "  # Set seed to reproduce results\n",
        "  T.manual_seed(1)\n",
        "  np.random.seed(1)\n",
        "  np.set_printoptions(linewidth=36)\n",
        "\n",
        "  # Create dataset \n",
        "  print(\"Creating PIMA Dataset \")\n",
        "  data,dp=GHData()\n",
        "  data_ds = PIMADataset(data) \n",
        "\n",
        "  # Create Model \n",
        "  vae = VAE().to(device)\n",
        "\n",
        "  # Train Model\n",
        "  bat_size = 64\n",
        "  max_epochs = 500\n",
        "  log_interval = 2\n",
        "  lrn_rate = 0.0001\n",
        "  beta = 1.0 \n",
        "\n",
        "  print(\"\\nbat_size = %3d \" % bat_size)\n",
        "  print(\"loss = custom BCE plus (beta * KLD) \")\n",
        "  print(\"loss beta = %0.2f \" % beta)\n",
        "  print(\"optimizer = Adam\")\n",
        "  print(\"max_epochs = %3d \" % max_epochs)\n",
        "  print(\"lrn_rate = %0.3f \" % lrn_rate)\n",
        "\n",
        "  train(vae, data_ds, bat_size, max_epochs, \n",
        "    log_interval, lrn_rate, beta)\n",
        "\n",
        "##########################Evaluation#####################################\n",
        "\n",
        "  # create Synthetic Data \n",
        "  vae.eval()\n",
        "  np.set_printoptions(linewidth=36)\n",
        "  for i in range(1):\n",
        "    rinpt = T.randn(1, vae.latent_dim).to(device)  # Gaussian\n",
        "    with T.no_grad():\n",
        "      z1,z2= vae.decode(rinpt)\n",
        "    si = np.concatenate((z1,z2),1)\n",
        "    print(si.shape)\n",
        "    dp.transformBack(si)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc1e_iNzmAko",
        "outputId": "ce3b9aae-0711-4831-ff38-51dc65799a4c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating PIMA Dataset \n",
            "2\n",
            "Before Transformation: \n",
            "   Outcomes  Pregnancies  Glucose  ...   BMI  DiabetesPedigree  Age\n",
            "0         1            6      148  ...  33.6             0.627   50\n",
            "1         0            1       85  ...  26.6             0.351   31\n",
            "2         1            8      183  ...  23.3             0.672   32\n",
            "3         0            1       89  ...  28.1             0.167   21\n",
            "4         1            0      137  ...  43.1             2.288   33\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "After Transformation: \n",
            "   Outcomes1  Outcomes2  Pregnancies  ...       BMI  DiabetesPedigree       Age\n",
            "0        1.0        0.0     0.639947  ...  0.204013          0.468492  1.425995\n",
            "1        0.0        1.0    -0.844885  ... -0.684422         -0.365061 -0.190672\n",
            "2        1.0        0.0     1.233880  ... -1.103255          0.604397 -0.105584\n",
            "3        0.0        1.0    -0.844885  ... -0.494043         -0.920763 -1.041549\n",
            "4        1.0        0.0    -1.141852  ...  1.409746          5.484909 -0.020496\n",
            "\n",
            "[5 rows x 10 columns]\n",
            "Dimension of Dataset:  (768, 10)\n",
            "\n",
            "bat_size =  64 \n",
            "loss = custom BCE plus (beta * KLD) \n",
            "loss beta = 1.00 \n",
            "optimizer = Adam\n",
            "max_epochs = 500 \n",
            "lrn_rate = 0.000 \n",
            "\n",
            "Starting training\n",
            "epoch =    0   loss = 39246.8120\n",
            "epoch =    2   loss = 38328.4585\n",
            "epoch =    4   loss = 35734.8284\n",
            "epoch =    6   loss = 29194.4263\n",
            "epoch =    8   loss = 18629.3021\n",
            "epoch =   10   loss = 9101.6110\n",
            "epoch =   12   loss = 3943.5699\n",
            "epoch =   14   loss = 1794.1510\n",
            "epoch =   16   loss = 919.2524\n",
            "epoch =   18   loss = 538.7409\n",
            "epoch =   20   loss = 356.4877\n",
            "epoch =   22   loss = 257.0698\n",
            "epoch =   24   loss = 195.9364\n",
            "epoch =   26   loss = 156.5826\n",
            "epoch =   28   loss = 128.7827\n",
            "epoch =   30   loss = 108.9926\n",
            "epoch =   32   loss = 93.9091\n",
            "epoch =   34   loss = 82.4813\n",
            "epoch =   36   loss = 73.3732\n",
            "epoch =   38   loss = 66.0622\n",
            "epoch =   40   loss = 60.0397\n",
            "epoch =   42   loss = 55.1743\n",
            "epoch =   44   loss = 51.0589\n",
            "epoch =   46   loss = 47.5500\n",
            "epoch =   48   loss = 44.6135\n",
            "epoch =   50   loss = 42.0927\n",
            "epoch =   52   loss = 39.8642\n",
            "epoch =   54   loss = 38.0017\n",
            "epoch =   56   loss = 36.3623\n",
            "epoch =   58   loss = 34.9404\n",
            "epoch =   60   loss = 33.6076\n",
            "epoch =   62   loss = 32.5126\n",
            "epoch =   64   loss = 31.5029\n",
            "epoch =   66   loss = 30.6538\n",
            "epoch =   68   loss = 29.8321\n",
            "epoch =   70   loss = 29.0772\n",
            "epoch =   72   loss = 28.4241\n",
            "epoch =   74   loss = 27.8245\n",
            "epoch =   76   loss = 27.2976\n",
            "epoch =   78   loss = 26.8270\n",
            "epoch =   80   loss = 26.3909\n",
            "epoch =   82   loss = 25.9710\n",
            "epoch =   84   loss = 25.6015\n",
            "epoch =   86   loss = 25.2418\n",
            "epoch =   88   loss = 24.9696\n",
            "epoch =   90   loss = 24.6479\n",
            "epoch =   92   loss = 24.4525\n",
            "epoch =   94   loss = 24.1543\n",
            "epoch =   96   loss = 23.8964\n",
            "epoch =   98   loss = 23.7150\n",
            "epoch =  100   loss = 23.4807\n",
            "epoch =  102   loss = 23.3210\n",
            "epoch =  104   loss = 23.1137\n",
            "epoch =  106   loss = 23.0076\n",
            "epoch =  108   loss = 22.8363\n",
            "epoch =  110   loss = 22.7418\n",
            "epoch =  112   loss = 22.5284\n",
            "epoch =  114   loss = 22.4379\n",
            "epoch =  116   loss = 22.3167\n",
            "epoch =  118   loss = 22.2043\n",
            "epoch =  120   loss = 22.0913\n",
            "epoch =  122   loss = 21.9918\n",
            "epoch =  124   loss = 21.9106\n",
            "epoch =  126   loss = 21.7698\n",
            "epoch =  128   loss = 21.7297\n",
            "epoch =  130   loss = 21.6139\n",
            "epoch =  132   loss = 21.5314\n",
            "epoch =  134   loss = 21.4829\n",
            "epoch =  136   loss = 21.3778\n",
            "epoch =  138   loss = 21.2800\n",
            "epoch =  140   loss = 21.2380\n",
            "epoch =  142   loss = 21.1892\n",
            "epoch =  144   loss = 21.0963\n",
            "epoch =  146   loss = 21.0327\n",
            "epoch =  148   loss = 20.9414\n",
            "epoch =  150   loss = 20.9779\n",
            "epoch =  152   loss = 20.8687\n",
            "epoch =  154   loss = 20.8263\n",
            "epoch =  156   loss = 20.7381\n",
            "epoch =  158   loss = 20.7394\n",
            "epoch =  160   loss = 20.6578\n",
            "epoch =  162   loss = 20.6422\n",
            "epoch =  164   loss = 20.5839\n",
            "epoch =  166   loss = 20.5275\n",
            "epoch =  168   loss = 20.5170\n",
            "epoch =  170   loss = 20.4818\n",
            "epoch =  172   loss = 20.4520\n",
            "epoch =  174   loss = 20.4104\n",
            "epoch =  176   loss = 20.4079\n",
            "epoch =  178   loss = 20.3726\n",
            "epoch =  180   loss = 20.3110\n",
            "epoch =  182   loss = 20.2898\n",
            "epoch =  184   loss = 20.2539\n",
            "epoch =  186   loss = 20.2611\n",
            "epoch =  188   loss = 20.2204\n",
            "epoch =  190   loss = 20.2130\n",
            "epoch =  192   loss = 20.1729\n",
            "epoch =  194   loss = 20.1689\n",
            "epoch =  196   loss = 20.1293\n",
            "epoch =  198   loss = 20.0989\n",
            "epoch =  200   loss = 20.1029\n",
            "epoch =  202   loss = 20.0677\n",
            "epoch =  204   loss = 20.0861\n",
            "epoch =  206   loss = 20.0685\n",
            "epoch =  208   loss = 20.0467\n",
            "epoch =  210   loss = 20.0228\n",
            "epoch =  212   loss = 20.0091\n",
            "epoch =  214   loss = 19.9982\n",
            "epoch =  216   loss = 19.9831\n",
            "epoch =  218   loss = 19.9924\n",
            "epoch =  220   loss = 20.0166\n",
            "epoch =  222   loss = 19.9371\n",
            "epoch =  224   loss = 20.0101\n",
            "epoch =  226   loss = 19.9798\n",
            "epoch =  228   loss = 19.9636\n",
            "epoch =  230   loss = 19.9582\n",
            "epoch =  232   loss = 19.9165\n",
            "epoch =  234   loss = 19.9458\n",
            "epoch =  236   loss = 19.9110\n",
            "epoch =  238   loss = 19.9310\n",
            "epoch =  240   loss = 19.9420\n",
            "epoch =  242   loss = 19.9382\n",
            "epoch =  244   loss = 19.9032\n",
            "epoch =  246   loss = 19.9326\n",
            "epoch =  248   loss = 19.9026\n",
            "epoch =  250   loss = 19.8867\n",
            "epoch =  252   loss = 19.8844\n",
            "epoch =  254   loss = 19.9007\n",
            "epoch =  256   loss = 19.9011\n",
            "epoch =  258   loss = 19.8996\n",
            "epoch =  260   loss = 19.8827\n",
            "epoch =  262   loss = 19.8800\n",
            "epoch =  264   loss = 19.8970\n",
            "epoch =  266   loss = 19.8607\n",
            "epoch =  268   loss = 19.8451\n",
            "epoch =  270   loss = 19.8697\n",
            "epoch =  272   loss = 19.8470\n",
            "epoch =  274   loss = 19.8717\n",
            "epoch =  276   loss = 19.8348\n",
            "epoch =  278   loss = 19.8569\n",
            "epoch =  280   loss = 19.8572\n",
            "epoch =  282   loss = 19.8444\n",
            "epoch =  284   loss = 19.8550\n",
            "epoch =  286   loss = 19.8445\n",
            "epoch =  288   loss = 19.8198\n",
            "epoch =  290   loss = 19.8204\n",
            "epoch =  292   loss = 19.8214\n",
            "epoch =  294   loss = 19.8184\n",
            "epoch =  296   loss = 19.8414\n",
            "epoch =  298   loss = 19.8478\n",
            "epoch =  300   loss = 19.8409\n",
            "epoch =  302   loss = 19.8286\n",
            "epoch =  304   loss = 19.8046\n",
            "epoch =  306   loss = 19.8291\n",
            "epoch =  308   loss = 19.8470\n",
            "epoch =  310   loss = 19.8224\n",
            "epoch =  312   loss = 19.8223\n",
            "epoch =  314   loss = 19.7969\n",
            "epoch =  316   loss = 19.8021\n",
            "epoch =  318   loss = 19.8166\n",
            "epoch =  320   loss = 19.8086\n",
            "epoch =  322   loss = 19.8154\n",
            "epoch =  324   loss = 19.7740\n",
            "epoch =  326   loss = 19.8128\n",
            "epoch =  328   loss = 19.8266\n",
            "epoch =  330   loss = 19.8162\n",
            "epoch =  332   loss = 19.8126\n",
            "epoch =  334   loss = 19.7992\n",
            "epoch =  336   loss = 19.8210\n",
            "epoch =  338   loss = 19.8356\n",
            "epoch =  340   loss = 19.8204\n",
            "epoch =  342   loss = 19.8304\n",
            "epoch =  344   loss = 19.7752\n",
            "epoch =  346   loss = 19.7873\n",
            "epoch =  348   loss = 19.8055\n",
            "epoch =  350   loss = 19.7936\n",
            "epoch =  352   loss = 19.8005\n",
            "epoch =  354   loss = 19.8094\n",
            "epoch =  356   loss = 19.7987\n",
            "epoch =  358   loss = 19.8126\n",
            "epoch =  360   loss = 19.8284\n",
            "epoch =  362   loss = 19.7929\n",
            "epoch =  364   loss = 19.8249\n",
            "epoch =  366   loss = 19.7850\n",
            "epoch =  368   loss = 19.8145\n",
            "epoch =  370   loss = 19.7716\n",
            "epoch =  372   loss = 19.7966\n",
            "epoch =  374   loss = 19.8031\n",
            "epoch =  376   loss = 19.7867\n",
            "epoch =  378   loss = 19.8187\n",
            "epoch =  380   loss = 19.8059\n",
            "epoch =  382   loss = 19.8062\n",
            "epoch =  384   loss = 19.7567\n",
            "epoch =  386   loss = 19.7942\n",
            "epoch =  388   loss = 19.7918\n",
            "epoch =  390   loss = 19.7984\n",
            "epoch =  392   loss = 19.8076\n",
            "epoch =  394   loss = 19.8168\n",
            "epoch =  396   loss = 19.7988\n",
            "epoch =  398   loss = 19.7781\n",
            "epoch =  400   loss = 19.7984\n",
            "epoch =  402   loss = 19.7873\n",
            "epoch =  404   loss = 19.7982\n",
            "epoch =  406   loss = 19.7956\n",
            "epoch =  408   loss = 19.8140\n",
            "epoch =  410   loss = 19.7803\n",
            "epoch =  412   loss = 19.7754\n",
            "epoch =  414   loss = 19.7802\n",
            "epoch =  416   loss = 19.8160\n",
            "epoch =  418   loss = 19.8015\n",
            "epoch =  420   loss = 19.8083\n",
            "epoch =  422   loss = 19.7979\n",
            "epoch =  424   loss = 19.7895\n",
            "epoch =  426   loss = 19.8068\n",
            "epoch =  428   loss = 19.8070\n",
            "epoch =  430   loss = 19.8087\n",
            "epoch =  432   loss = 19.7517\n",
            "epoch =  434   loss = 19.7804\n",
            "epoch =  436   loss = 19.7867\n",
            "epoch =  438   loss = 19.7845\n",
            "epoch =  440   loss = 19.7974\n",
            "epoch =  442   loss = 19.7708\n",
            "epoch =  444   loss = 19.7978\n",
            "epoch =  446   loss = 19.7782\n",
            "epoch =  448   loss = 19.7931\n",
            "epoch =  450   loss = 19.7824\n",
            "epoch =  452   loss = 19.7687\n",
            "epoch =  454   loss = 19.7825\n",
            "epoch =  456   loss = 19.7979\n",
            "epoch =  458   loss = 19.7916\n",
            "epoch =  460   loss = 19.7882\n",
            "epoch =  462   loss = 19.7780\n",
            "epoch =  464   loss = 19.7782\n",
            "epoch =  466   loss = 19.7862\n",
            "epoch =  468   loss = 19.7732\n",
            "epoch =  470   loss = 19.7946\n",
            "epoch =  472   loss = 19.7496\n",
            "epoch =  474   loss = 19.7897\n",
            "epoch =  476   loss = 19.7770\n",
            "epoch =  478   loss = 19.7981\n",
            "epoch =  480   loss = 19.7777\n",
            "epoch =  482   loss = 19.7712\n",
            "epoch =  484   loss = 19.7691\n",
            "epoch =  486   loss = 19.7814\n",
            "epoch =  488   loss = 19.7607\n",
            "epoch =  490   loss = 19.7841\n",
            "epoch =  492   loss = 19.7885\n",
            "epoch =  494   loss = 19.8018\n",
            "epoch =  496   loss = 19.7810\n",
            "epoch =  498   loss = 19.7660\n",
            "Done \n",
            "(1, 10)\n",
            "8\n",
            "Synthesized Data (Normed): \n",
            "   Outcomes1  Outcomes2  Pregnancies  ...       BMI  DiabetesPedigree       Age\n",
            "0   0.167758   0.832242    -0.005734  ...  0.036621          0.023808  0.017606\n",
            "\n",
            "[1 rows x 10 columns]\n",
            "Synthesized Data (De-Normed): \n",
            "   Outcomes Pregnancies  Glucose  ...     BMI DiabetesPedigree     Age\n",
            "0         1       [7.0]  [121.0]  ...  [32.0]            [0.0]  [34.0]\n",
            "\n",
            "[1 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gt9IIj4WDv0e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}