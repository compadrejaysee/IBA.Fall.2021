#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity

import warnings
warnings.filterwarnings(action="ignore")

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler, normalize
from sklearn.decomposition import PCA

import scipy.cluster.hierarchy as shc
from sklearn.cluster import DBSCAN 

from sklearn.neighbors import NearestNeighbors
from random import sample
from numpy.random import uniform
import numpy as np
from math import isnan


# In[7]:


# A CLASS IS DEFINED FOR ALL PREPROCESSING OF DATA AS WELL AS STANDARDIZATION AND NORMALIZATION OF DATA

class preprocessing():
    
    def describe(df):
        return df.describe()
        
    def info(df):
        return df.info(df)
    
    def mv_percent(df):
        return df.isna().mean()*100
        
    def drop_col(df, col):
        df.drop(columns=col, inplace=True)
        return df
    
    def drop_null_rows(df, row):
        df.dropna(subset=[row], inplace=True)
        return df
        
    def impute_col_median(df, col):
        df[col].fillna(df[col].median(), inplace=True)
        return df
        
    def impute_col_mean(df, col):
        df[col].fillna(df[col].mean(), inplace=True)
        return df
    
    # Standardize data
    def standardize(df):
        scaler = StandardScaler() 
        global scaled_df 
        scaled_df= scaler.fit_transform(df)
        print('variabe: scaled_df =')
        return scaled_df

    # Normalizing the Data

    def normalise(df):
        global normalized_df
        normalized_df = normalize(df)
        print('variabe normalized_df =')
        return 'normalized_df =', normalized_df

    # Converting the numpy array into a pandas DataFrame 

    def array2pd(data):
        global df
        df = pd.DataFrame(data)

        print ('below dataframe has been update to variable df')
        return df.head()


# In[3]:


#DIMENSIONALITY REDUCTION USING PRINCIPAL COMPONENT ANALYSIS (PCA)    
# Reducing the dimensions of the data. 2 Principals

def pca_2(data):
    pca = PCA(n_components = 2) 
    
    global df
    df = pca.fit_transform(data) 
    df = pd.DataFrame(df) 
    df.columns = ['P1', 'P2'] 

    print ('below 2 dimensional dataframe has been update to variable df')
    return df.head()

# 3 principals 
def pca_3(data):
    pca = PCA(n_components = 3) 
    
    global df
    df = pca.fit_transform(data) 
    df = pd.DataFrame(df) 
    df.columns = ['P1', 'P2', 'P3'] 

    print ('below 3 dimensional dataframe has been update to variable df')
    return df.head()

#4 principals
def pca_4(data):
    pca = PCA(n_components = 4) 
    
    global df
    df = pca.fit_transform(data) 
    df = pd.DataFrame(df) 
    df.columns = ['P1', 'P2', 'P3', 'P4'] 

    print ('below 4 dimensional dataframe has been update to variable df')
    return df.head()


# In[4]:


#A CLASS MADE FOR ALL VISUALIZATIONS RELATED TO UNSUPERVISED CLUSTERING

class visual():

    def skewness(df):
        plt.figure(figsize=(20,35))
        for i, col in enumerate(df.columns):
            if df[col].dtype != 'object':
                ax = plt.subplot(9, 2, i+1)
                sns.kdeplot(df[col], ax=ax)
                plt.xlabel(col)
        plt.show()
        
    def heatmap(df):
        plt.figure(figsize=(12,12))
        sns.heatmap(df.corr(), annot=True)
        plt.show()
        
    def dendogram(df):
        plt.figure(figsize =(6, 6)) 
        plt.title('Dendogram visualisation of the data')
        Dendrogram = shc.dendrogram((shc.linkage(df, method ='ward'))) 
        
    def cluster_plot(df, col1, col2):
        plt.figure(figsize=(10,6))
        sns.scatterplot(data=df, x=col1, y=col2, hue='cluster_id')
        plt.title('Distribution of clusters based on ' +col1+ ' and ' + col2)
        plt.show()
    
    def agglo_cluster(df, col1, col2, clusters):
        from sklearn.cluster import AgglomerativeClustering
        plt.scatter(df[col1], df[col2],  
        c = AgglomerativeClustering(n_clusters = clusters).fit_predict(df), cmap =plt.cm.winter) 
        plt.show() 


# In[5]:


#The Hopkins statistic (Lawson and Jurs 1990) is used to assess the clustering tendency of a data set 
#by measuring the probability that a given data set is generated by a uniform data distribution. 
#In other words, it tests the spatial randomness of the data.

class cluster_metrics():

    def hopkins (df):

        from sklearn.neighbors import NearestNeighbors
        from random import sample
        from numpy.random import uniform
        import numpy as np
        from math import isnan

        d = df.shape[1]
        #d = len(vars) # columns
        n = len(df) # rows
        m = int(0.1 * n) 
        nbrs = NearestNeighbors(n_neighbors=1).fit(df.values)

        rand_X = sample(range(0, n, 1), m)

        ujd = []
        wjd = []
        for j in range(0, m):
            u_dist, _ = nbrs.kneighbors(uniform(np.amin(df,axis=0),np.amax(df,axis=0),d).reshape(1, -1), 2, return_distance=True)
            ujd.append(u_dist[0][1])
            w_dist, _ = nbrs.kneighbors(df.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)
            wjd.append(w_dist[0][1])

        H = sum(ujd) / (sum(ujd) + sum(wjd))
        if isnan(H):
            print(ujd, wjd)
            H = 0

        print('Hopkins statistic =',H)


    def silhoutte_viz(df):
            kmeans_models = [KMeans(n_clusters=k, random_state=23).fit(df) for k in range (1, 10)]
            silhoutte_scores = [silhouette_score(df, model.labels_) for model in kmeans_models[1:4]]
            plt.plot(range(2,5), silhoutte_scores, "bo-")
            plt.xticks([2, 3, 4])
            plt.title('Silhoutte scores vs Number of clusters')
            plt.xlabel('Number of clusters')
            plt.ylabel('Silhoutte score')
            plt.show()

    def silhoutte(df, clusters):
        from sklearn.metrics import silhouette_score

        kmeans = KMeans(n_clusters=clusters, random_state=23)
        kmeans.fit(df)

        print('Silhoutte score of our model is ' + str(silhouette_score(df, kmeans.labels_)))


# In[8]:


#***CLUSTERING*****

#class for K_means clustering

class K_means():
    
    def elbow_method(df):
        kmeans_models = [KMeans(n_clusters=k, random_state=23).fit(df) for k in range (1, 10)]
        innertia = [model.inertia_ for model in kmeans_models]

        plt.plot(range(1, 10), innertia)
        plt.title('Elbow method')
        plt.xlabel('Number of Clusters')
        plt.ylabel('WCSS') #Within Cluster Summation of Square
        plt.show()
        
    #clusters and adds cluster column to the df   
    def cluster_col(df, clusters): 
        kmeans = KMeans(n_clusters=clusters, random_state=23)
        kmeans.fit(df)
        df['cluster_id'] = kmeans.labels_
        return df
    
#function for Agglomerative Clustering   
def Agglomerative_Clustering(df, clusters):
    
    from sklearn.cluster import AgglomerativeClustering 
    
    agg = AgglomerativeClustering(n_clusters=clusters)
    agg.fit(df)


class dbscan():
    #To identify optimal epsilon value
    def points_distance(df):
        neigh = NearestNeighbors(n_neighbors=2)
        nbrs = neigh.fit(df)
        distances, indices = nbrs.kneighbors(df)

        distances = np.sort(distances, axis=0)
        distances = distances[:,1]
        plt.plot(distances)
    
    #db scan clustering
    def db_scan(df, col1, col2, epsilon, samples):
        from sklearn.cluster import DBSCAN 

        db = DBSCAN(eps=epsilon, min_samples=samples).fit(df)

        df['Labels'] = db.labels_
        plt.figure(figsize=(12, 8))
        sns.scatterplot(df[col1], df[col2], hue=df['Labels'], 
                        palette=sns.color_palette('hls', np.unique(db.labels_).shape[0]))
        plt.title('DBSCAN clustering')
        plt.show()
        
#Function for mean shift clustering        
def mean_shift(df, col1, col2):

    from sklearn.cluster import MeanShift, estimate_bandwidth

    # The following bandwidth can be automatically detected using
    bandwidth = estimate_bandwidth(df, quantile=0.1)
    ms = MeanShift(bandwidth).fit(df)

    df['Labels'] = ms.labels_
    plt.figure(figsize=(12, 8))
    sns.scatterplot(df[col1], df[col2], hue=df['Labels'], 
                    palette=sns.color_palette('hls', np.unique(ms.labels_).shape[0]))
    plt.plot()
    plt.title('MeanShift')
    plt.show()

