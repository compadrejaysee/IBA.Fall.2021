{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db3aabf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier as XGBC\n",
    "import lightgbm as lgbm\n",
    "import seaborn as sb\n",
    "import pandas as pd \n",
    "from pandas.plotting import lag_plot, autocorrelation_plot as acplot\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as mano\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "import math\n",
    "import time\n",
    "from math import sqrt\n",
    "import warnings\n",
    "from scipy.stats import skew, kurtosis, spearmanr as spm, pearsonr as pe, chi2_contingency\n",
    "from scipy.special import boxcox, inv_boxcox\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA, ARMA\n",
    "from random import gauss, seed\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import pylab as py\n",
    "from sklearn.feature_selection import RFE, RFECV, mutual_info_regression\n",
    "from sklearn import svm, tree\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import OrdinalEncoder as ordenc, OneHotEncoder as ohot, StandardScaler as StdSclr, MinMaxScaler as MMS\n",
    "from sklearn.metrics import confusion_matrix as confm, accuracy_score as acs, classification_report as crep, log_loss as logloss, mean_absolute_error as MAE, mean_squared_error as MSE, r2_score as r2, precision_score as pscore, recall_score as rscore, roc_auc_score as rascore, f1_score as f1score, roc_curve as roccurve, auc \n",
    "from sklearn.model_selection import train_test_split as tts, KFold, RepeatedKFold as RKFold, StratifiedKFold as SKFold, StratifiedShuffleSplit as SSS, cross_val_score as cvl, LeaveOneOut as lvo, RepeatedStratifiedKFold as rskfold\n",
    "from sklearn.tree import DecisionTreeClassifier as dtc, DecisionTreeRegressor as dtr\n",
    "from sklearn.linear_model import LinearRegression as LR, LogisticRegression as LogR, Ridge as rdg, Lasso as lso\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC, GradientBoostingClassifier as GBC, AdaBoostClassifier as ABC, GradientBoostingRegressor as GBR, AdaBoostRegressor as ABR, VotingRegressor as VR, RandomForestRegressor as RFR\n",
    "from sklearn.naive_bayes import GaussianNB as GNB\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier as mlpc\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures as PF\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "from statistics import mean, median, mode\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import graphviz\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from collections import Counter\n",
    "from IPython.display import Image\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b1f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0bb71420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepcopy(dataset):\n",
    "    return dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3a2c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_analysis(dataset):\n",
    "    colname = str(input(\"which column do you want to analyze? Press enter if you want to analyze the entire dataset.\"))\n",
    "    if colname != '':\n",
    "        dataset[colname].plot()\n",
    "        plt.show()\n",
    "        print(dataset[colname].describe())\n",
    "        fit = str(input(\"what type of fit do you want the qqplot to be? (None, 45, s, r, or q)\\n\"))\n",
    "        sm.qqplot(dataset[colname], line = fit)\n",
    "        plt.show()\n",
    "        plt.hist(dataset[colname])\n",
    "        plt.show()\n",
    "        dataset[colname].plot.box()\n",
    "        plt.show()\n",
    "        skewvalue = skew(dataset[colname])\n",
    "        print('The skewvalue is', skewvalue)\n",
    "        if skewvalue>-2 and skewvalue<2:\n",
    "            print(\"Normally skewed data\")\n",
    "        else:\n",
    "            print(\"Not normally skewed data\")\n",
    "        kurtosis_value = kurtosis(dataset[colname])\n",
    "        print(kurtosis_value)\n",
    "        if kurtosis_value>-7 and kurtosis_value<7:\n",
    "            print(\"Normal Kurtosis\")\n",
    "        else:\n",
    "            print(\"Not normal Kurtosis\")\n",
    "    else:\n",
    "        dataset.plot()\n",
    "        plt.show()\n",
    "        dataset.describe()\n",
    "        fit = str(input(\"what type of fit do you want the qqplot to be? (None, 45, s, r, or q)\\n\"))\n",
    "        sm.qqplot(dataset, line = fit)\n",
    "        plt.show()\n",
    "        plt.hist(dataset)\n",
    "        plt.show()\n",
    "        dataset.plot.box()\n",
    "        plt.show()\n",
    "        skewvalue = skew(dataset)\n",
    "        print(skewvalue)\n",
    "        if (skewvalue>-2).all() and (skewvalue<2).all():\n",
    "            print(\"Normally skewed data\")\n",
    "        else:\n",
    "            print(\"Not normally skewed data\")\n",
    "        kurtosis_value = kurtosis(dataset)\n",
    "        print(kurtosis_value)\n",
    "        if (kurtosis_value>-7).all() and (kurtosis_value<7).all():\n",
    "            print(\"Normal Kurtosis\")\n",
    "        else:\n",
    "            print(\"Not normal Kurtosis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b5a0a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missval(dataset):\n",
    "    x = str(input(\"What analysis do you want to do? (missingval, totalmissingval, or correlation, dendogram)\"))\n",
    "    if x == 'missingval':\n",
    "        return mano.matrix(dataset)\n",
    "    elif x == 'totalmissingval':\n",
    "        return mano.bar(dataset)\n",
    "    elif x == 'correlation':\n",
    "        return mano.heatmap(dataset, figsize=(12,6))\n",
    "    elif x == 'dendo':\n",
    "        return mano.dendrogram(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34c6e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remspace(dataset):\n",
    "    colname=str(input(\"Which column do you want to remove white space in?\\n\"))\n",
    "    where = str(input(\"where do you want to remove the whitespace from?\"))\n",
    "    if colname == \"\":\n",
    "        if where == \"\":\n",
    "            dataset = dataset.str.replace(' ', '')\n",
    "        if where == \"beginning\":\n",
    "            dataset = dataset.str.lstrip()\n",
    "        if where == \"end\":\n",
    "            dataset = dataset.str.rstrip()\n",
    "        if where == \"both\":\n",
    "            dataset = dataset.str.strip()\n",
    "    else:\n",
    "        if where == \"\":\n",
    "            dataset[colname] = dataset[colname].str.replace(' ', '')\n",
    "        if where == \"beginning\":\n",
    "            dataset[colname] = dataset[colname].str.lstrip()\n",
    "        if where == \"end\":\n",
    "            dataset[colname] = dataset[colname].str.rstrip()\n",
    "        if where == \"both\":\n",
    "            dataset[colname] = dataset[colname].str.strip()\n",
    "    return dataset\n",
    "\n",
    "def repspace(dataset):\n",
    "    colname=str(input(\"Which column do you want to replace white space in?\\n\"))\n",
    "    newvalue = str(input(\"what do you want to replace the white space with?\"))\n",
    "    where = str(input(\"where do you want to put the new value?\"))\n",
    "    if colname !=\"\":\n",
    "        if where == \"beginning\":\n",
    "            dataset[colname].str.replace('^ +', newvalue)\n",
    "        elif where == \"end\":\n",
    "            dataset[colname].str.replace(' +$', newvalue)\n",
    "        elif where == \"both\":\n",
    "            dataset[colname].str.replace('^ +| +$', newvalue)\n",
    "        elif where == \"\":\n",
    "            dataset[colname].str.replace(' ', newvalue)\n",
    "    else:\n",
    "        if where == \"beginning\":\n",
    "            dataset.str.replace('^ +', newvalue)\n",
    "        elif where == \"end\":\n",
    "            dataset.str.replace(' +$', newvalue)\n",
    "        elif where == \"both\":\n",
    "            dataset.str.replace('^ +| +$', newvalue)\n",
    "        elif where == \"\":\n",
    "            dataset.str.replace(' ', newvalue)\n",
    "    return dataset\n",
    "\n",
    "def extdmy(dataset):\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    colname = str(input(\"What column do you want to plot against time?\"))\n",
    "    dataset['year'] = dataset.index.year\n",
    "    dataset['month'] = dataset.index.month\n",
    "    dataset['day']=dataset.index.day\n",
    "    dataset['weekday']=dataset.index.day_name()\n",
    "    dataset['hour']=dataset.index.hour\n",
    "    dataset['minute']=dataset.index.minute\n",
    "    dataset['second']=dataset.index.second\n",
    "    f, axes = plt.subplots(1,3)\n",
    "    sb.boxplot(data=dataset, x = 'weekday', y = colname, ax = axes[0]) \n",
    "    sb.boxplot(data=dataset, x = 'month', y = colname, ax = axes[1])\n",
    "    sb.boxplot(data=dataset, x = 'year', y = colname, ax = axes[2])\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c950d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def description(name, rows):\n",
    "    print(\"The columns and rows of the dataset are\", name.shape)\n",
    "    print(\"The number of values in the dataset is\", name.size)\n",
    "    print(name.head(rows))\n",
    "    print(name.tail(rows))\n",
    "    print(name.dtypes)\n",
    "    print(name.columns)\n",
    "    return name.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdafc7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remcol(dataset):\n",
    "    colname = str(input(\"which column do you want to remove?\\n\"))\n",
    "    if colname != \"\":\n",
    "        del(dataset[colname])\n",
    "        return remcol(dataset)\n",
    "    else:\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d791ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename(dataset):\n",
    "    colname = str(input(\"Which column do you want to rename?\\n\"))\n",
    "    if colname!='':\n",
    "        newcolname = str(input(\"New name of the column?\\n\"))\n",
    "        dataset.rename(columns = {colname:newcolname}, inplace = True)\n",
    "        return rename(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "217f1701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remrow(dataset):\n",
    "    colname = str(input(\"which column values do you want to remove?\\n\"))\n",
    "    if colname == 'all':\n",
    "        remrow.dataset = dataset.dropna(axis=0)\n",
    "    elif colname ==\"\":\n",
    "        row = (input(\"which row/s do you want to remove?\\n\"))\n",
    "        remrow.dataset = dataset[dataset[colname]!=row]\n",
    "        return remrow(dataset)\n",
    "    else:\n",
    "        return remrow.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa018d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadfile():\n",
    "    filetype = str(input(\"What is the file type? (csv or excel)\\n\"))\n",
    "    enc = str(input(\"What encoding do you want? (latin1 or unicode_escape)\\n\"))\n",
    "    if filetype == 'csv':\n",
    "        filepath = str(input(\"What is the filepath:\"))\n",
    "        data = pd.read_csv(filepath, index_col = None, encoding = enc)\n",
    "        return data\n",
    "    elif filetype == 'excel':\n",
    "        filepath = str(input(\"What is the filepath:\"))\n",
    "        data = pd.read_excel(filepath, index_col = None)\n",
    "        return data\n",
    "    else:\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95a54d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null(dataset):    \n",
    "    return dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec0e00f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillmissingvals(dataset):\n",
    "    method = str(input(\"which method do you want to use? (KNN, SimpleImp, Interpolation, mean, median, mode)\"))\n",
    "    colname = str(input(\"which column do you want to fill?\"))\n",
    "    if method == 'mode':\n",
    "        dataset[colname].value_counts()\n",
    "        mode = input(\"what is the most occurring value?\")\n",
    "        dataset[colname].fillna(mode, inplace=True)\n",
    "    elif method == 'mean':\n",
    "        dataset[colname].fillna(data[colname].mean)\n",
    "    elif method == 'median':\n",
    "        dataset[colname].fillna(data[colname].median)\n",
    "    elif method == 'knn':\n",
    "        colname1 = str(input('Which columns do you want to use?'))\n",
    "        colname2=str(input('Which columns do you want to use?')) \n",
    "        neighbours = int(input(\"how many neighbours?\\n\"))\n",
    "        imputer = KNNImputer(n_neighbors=neighbours)\n",
    "        dataset[[colname1, colname2]] = imputer.fit_transform(dataset[[colname1, colname2]])\n",
    "        return dataset\n",
    "    elif method == 'simpleimp':\n",
    "        newcolname = str(input(\"what is the new column name?\\n\"))\n",
    "        imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        dataset[[newcolname]] = imp_mean.fit(dataset[[colname]])\n",
    "    elif method == 'Interpol':\n",
    "        submethod = str(input(\"backwards or forwards?\"))\n",
    "        newcolname = str(input(\"what is the new column name?\\n\"))\n",
    "        if submethod == 'forwards':\n",
    "            dataset[newcolname] = dataset[colname].interpolate(method ='linear', limit_direction ='forward')\n",
    "        elif submethod == 'backwards':\n",
    "            dataset[newcolname] = dataset[colname].interpolate(method ='linear', limit_direction ='backward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2ee940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def changedtype(dataset):\n",
    "    colname = str(input(\"which column do you want to change the data type for? Press enter if you want to change the data type for the entire dataset. \"))\n",
    "    if colname != '':\n",
    "        changetype = str(input(\"what data type do you want to set the column to? (string, numbers, datetime)\"))\n",
    "        if changetype == 'string':\n",
    "            dataset[colname]= dataset[colname].astype(str)\n",
    "        elif changetype == 'float':\n",
    "            dataset[colname]= dataset[colname].astype(float) #converting to float to accomodate all types of numbers\n",
    "        elif changetype == 'boolean':\n",
    "            dataset[colname]= dataset[colname].astype(bool)\n",
    "        elif changetype == 'datetime':\n",
    "            dataset[colname]=pd.to_datetime(dataset[colname])\n",
    "        elif changetype == 'numbers':\n",
    "            dataset[colname]=dataset[colname].astype(int)\n",
    "    else:\n",
    "        changetype = str(input(\"what data type do you want to set the column to? (string, numbers, datetime)\"))\n",
    "        if changetype == 'string':\n",
    "            dataset= dataset.astype(str)\n",
    "        elif changetype == 'float':\n",
    "            dataset= dataset.astype(float) #converting to float to accomodate all types of numbers\n",
    "        elif changetype == 'boolean':\n",
    "            dataset= dataset.astype(bool)\n",
    "        elif changetype == 'datetime':\n",
    "            dataset=pd.to_datetime(dataset)\n",
    "        elif changetype == 'numbers':\n",
    "            dataset=dataset.astype(int)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61bfb286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(dataset):\n",
    "    print(dataset.dtypes)\n",
    "    method = str(input(\"what do you want to do?\"))\n",
    "    if method == \"onehot\":\n",
    "        colname = str(input(\"which column do you want to encode?\"))\n",
    "        if colname != '':\n",
    "            results = ohot.fit_transform(dataset[colname])\n",
    "            newdat = pd.DataFrame(results.toarray(), columns = ohot.categories_)\n",
    "            dataset = dataset.join(newdat)\n",
    "        else: \n",
    "            results = ohot.fit_transform(dataset)\n",
    "            newdat = pd.DataFrame(results.toarray(), columns = ohot.categories_)\n",
    "            return newdat\n",
    "    elif method == 'onehot2':\n",
    "        df = pd.get_dummies(dataset)\n",
    "        return df\n",
    "    elif method == \"ordinal\":\n",
    "        colname = str(input(\"which column do you want to encode?\"))\n",
    "        if colname !='':\n",
    "            newcolname = str(input(\"set a new column name for encoded values\"))\n",
    "            dataset[newcolname] = ordenc.fit_transform(dataset[colname])\n",
    "        else:\n",
    "            return ordenc.fit_transform(dataset)\n",
    "    elif method == 'labelencoder':\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(dataset[i])\n",
    "        dataset = le.transform(dataset)\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a92433a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNNalgo(X_train, X_test, y_train, y_test):  \n",
    "    neighbours = int(input(\"How many neighbours do you want?\"))\n",
    "    classifier = KNC(n_neighbors=neighbours)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(confm(y_test, y_pred))\n",
    "    print(crep(y_test, y_pred))\n",
    "    print(\"These are the predicted values: \", y_pred)\n",
    "    confmatrix = confm(y_test, y_pred)\n",
    "    print(\"The Confusion Matrix is\\n\", confmatrix)\n",
    "    Accuracy_Score = acs(y_test, y_pred)*100\n",
    "    print(\"The Accuracy Score is\\n\", Accuracy_Score)\n",
    "    precision = pscore(y_test, y_pred, pos_label = '1', labels = [0,1])*100\n",
    "    print(\"The Precision is\", precision)\n",
    "    CReport = crep(y_test, y_pred)\n",
    "    print(\"The Classification Report is\\n\", CReport)\n",
    "    LogLoss = logloss(y_test, y_pred)\n",
    "    print(\"The LogLoss is\", LogLoss)\n",
    "    fpr, tpr, _ = roccurve(y_test, y_pred)\n",
    "    AreaUnderCurve = auc(fpr, tpr)\n",
    "    print(\"The Area Under the Curve is\", AreaUnderCurve)\n",
    "    RecallScore = rscore(y_test, y_pred,pos_label='1',labels=[0,1])*100\n",
    "    print(\"The Recall Score is\", RecallScore)\n",
    "    print(\"The Specificity is\", fpr, \"and the Sensitivity is\", tpr)\n",
    "    FScore = f1score(y_test, y_pred)\n",
    "    print(\"The F-Score is\", FScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbc0cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NVB(X_train, X_test, y_train, y_test):\n",
    "    y_pred = GNB().fit(X_train, y_train).predict(X_test)\n",
    "    print(\"These are the predicted values: \", y_pred)\n",
    "    print(\"The number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "    confmatrix = confm(y_test, y_pred)\n",
    "    print(\"The Confusion Matrix is\\n\", confmatrix)\n",
    "    Accuracy_Score = acs(y_test, y_pred)*100\n",
    "    print(\"The Accuracy Score is\\n\", Accuracy_Score)\n",
    "    precision = pscore(y_test, y_pred, pos_label = '1', labels = [0,1])*100\n",
    "    print(\"The Precision is\", precision)\n",
    "    CReport = crep(y_test, y_pred)\n",
    "    print(\"The Classification Report is\\n\", CReport)\n",
    "    LogLoss = logloss(y_test, y_pred)\n",
    "    print(\"The LogLoss is\", LogLoss)\n",
    "    fpr, tpr, _ = roccurve(y_test, y_pred)\n",
    "    AreaUnderCurve = auc(fpr, tpr)\n",
    "    print(\"The Area Under the Curve is\", AreaUnderCurve)\n",
    "    RecallScore = rscore(y_test, y_pred,pos_label='1',labels=[0,1])*100\n",
    "    print(\"The Recall Score is\", RecallScore)\n",
    "    print(\"The Specificity is\", fpr, \"and the Sensitivity is\", tpr)\n",
    "    FScore = f1score(y_test, y_pred)\n",
    "    print(\"The F-Score is\", FScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5033c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perceptron(X_train, X_test, y_train, y_test):\n",
    "    clf = mlpc(tol=1e-3, random_state=0)\n",
    "    y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "    print(\"These are the predicted values: \", y_pred)\n",
    "    print('The Perceptron score is', clf.score(X_train, y_train))\n",
    "    confmatrix = confm(y_test, y_pred)\n",
    "    print(\"The Confusion Matrix is\\n\", confmatrix)\n",
    "    Accuracy_Score = acs(y_test, y_pred)*100\n",
    "    print(\"The Accuracy Score is\\n\", Accuracy_Score)\n",
    "    precision = pscore(y_test, y_pred, pos_label = '1', labels = [0,1])*100\n",
    "    print(\"The Precision is\", precision)\n",
    "    CReport = crep(y_test, y_pred)\n",
    "    print(\"The Classification Report is\\n\", CReport)\n",
    "    LogLoss = logloss(y_test, y_pred)\n",
    "    print(\"The LogLoss is\", LogLoss)\n",
    "    fpr, tpr, _ = roccurve(y_test, y_pred)\n",
    "    AreaUnderCurve = auc(fpr, tpr)\n",
    "    print(\"The Area Under the Curve is\", AreaUnderCurve)\n",
    "    RecallScore = rscore(y_test, y_pred,pos_label='1',labels=[0,1])*100\n",
    "    print(\"The Recall Score is\", RecallScore)\n",
    "    print(\"The Specificity is\", fpr, \"and the Sensitivity is\", tpr)\n",
    "    FScore = f1score(y_test, y_pred)\n",
    "    print(\"The F-Score is\", FScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b21adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supvec(X_train, X_test, y_train, y_test):\n",
    "    knl = str(input('Which kernel do you want to apply? (linear, rbf)'))\n",
    "    c= int(input('Value of c?' ))\n",
    "    clf = svm.SVC(kernel = knl, C = c, random_state = 42)\n",
    "    y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "    print(\"These are the predicted values: \", y_pred)\n",
    "    confmatrix = confm(y_test, y_pred)\n",
    "    print(\"The Confusion Matrix is\\n\", confmatrix)\n",
    "    Accuracy_Score = acs(y_test, y_pred)*100\n",
    "    print(\"The Accuracy Score is\\n\", Accuracy_Score)\n",
    "    precision = pscore(y_test, y_pred, average = 'macro', labels = [0,1])*100\n",
    "    print(\"The Precision is\", precision)\n",
    "    CReport = crep(y_test, y_pred)\n",
    "    print(\"The Classification Report is\\n\", CReport)\n",
    "    LogLoss = logloss(y_test, y_pred)\n",
    "    print(\"The LogLoss is\", LogLoss)\n",
    "    fpr, tpr, _ = roccurve(y_test, y_pred)\n",
    "    AreaUnderCurve = auc(fpr, tpr)\n",
    "    print(\"The Area Under the Curve is\", AreaUnderCurve)\n",
    "    RecallScore = rscore(y_test, y_pred, average = 'macro',labels=[0,1])*100\n",
    "    print(\"The Recall Score is\", RecallScore)\n",
    "    print(\"The Specificity is\", fpr, \"and the Sensitivity is\", tpr)\n",
    "    FScore = f1score(y_test, y_pred)\n",
    "    print(\"The F-Score is\", FScore)\n",
    "    \n",
    "def RandFor(X_train, X_test, y_train, y_test):\n",
    "    y_pred = RFC().fit(X_train, y_train).predict(X_test)\n",
    "    print(\"These are the predicted values: \", y_pred)\n",
    "    confmatrix = confm(y_test, y_pred)\n",
    "    print(\"The Confusion Matrix is\\n\", confmatrix)\n",
    "    Accuracy_Score = acs(y_test, y_pred)*100\n",
    "    print(\"The Accuracy Score is\\n\", Accuracy_Score)\n",
    "    precision = pscore(y_test, y_pred, average = 'macro', labels = [0,1])*100\n",
    "    print(\"The Precision is\", precision)\n",
    "    CReport = crep(y_test, y_pred)\n",
    "    print(\"The Classification Report is\\n\", CReport)\n",
    "    LogLoss = logloss(y_test, y_pred)\n",
    "    print(\"The LogLoss is\", LogLoss)\n",
    "    fpr, tpr, _ = roccurve(y_test, y_pred)\n",
    "    AreaUnderCurve = auc(fpr, tpr)\n",
    "    print(\"The Area Under the Curve is\", AreaUnderCurve)\n",
    "    RecallScore = rscore(y_test, y_pred,average = 'macro',labels=[0,1])*100\n",
    "    print(\"The Recall Score is\", RecallScore)\n",
    "    print(\"The Specificity is\", fpr, \"and the Sensitivity is\", tpr)\n",
    "    FScore = f1score(y_test, y_pred)\n",
    "    print(\"The F-Score is\", FScore)\n",
    "    \n",
    "def ada(X_train, X_test, y_train, y_test):\n",
    "    y_pred = ABC().fit(X_train, y_train).predict(X_test)\n",
    "    print(\"These are the predicted values: \", y_pred)\n",
    "    confmatrix = confm(y_test, y_pred)\n",
    "    print(\"The Confusion Matrix is\\n\", confmatrix)\n",
    "    Accuracy_Score = acs(y_test, y_pred)*100\n",
    "    print(\"The Accuracy Score is\\n\", Accuracy_Score)\n",
    "    precision = pscore(y_test, y_pred, average = 'macro', labels = [0,1])*100\n",
    "    print(\"The Precision is\", precision)\n",
    "    CReport = crep(y_test, y_pred)\n",
    "    print(\"The Classification Report is\\n\", CReport)\n",
    "    LogLoss = logloss(y_test, y_pred)\n",
    "    print(\"The LogLoss is\", LogLoss)\n",
    "    fpr, tpr, _ = roccurve(y_test, y_pred)\n",
    "    AreaUnderCurve = auc(fpr, tpr)\n",
    "    print(\"The Area Under the Curve is\", AreaUnderCurve)\n",
    "    RecallScore = rscore(y_test, y_pred,average = 'macro',labels=[0,1])*100\n",
    "    print(\"The Recall Score is\", RecallScore)\n",
    "    print(\"The Specificity is\", fpr, \"and the Sensitivity is\", tpr)\n",
    "    FScore = f1score(y_test, y_pred)\n",
    "    print(\"The F-Score is\", FScore)\n",
    "\n",
    "def boostaway(X_train, X_test, y_train, y_test):\n",
    "    xg_reg = XGBC(disable_default_eval_metric=1)\n",
    "    xg_reg.fit(X_train,y_train)\n",
    "    y_pred = xg_reg.predict(X_test)\n",
    "    print(\"These are the predicted values: \", y_pred)\n",
    "    confmatrix = confm(y_test, y_pred)\n",
    "    print(\"The Confusion Matrix is\\n\", confmatrix)\n",
    "    Accuracy_Score = acs(y_test, y_pred)*100\n",
    "    print(\"The Accuracy Score is\\n\", Accuracy_Score)\n",
    "    precision = pscore(y_test, y_pred,  average = 'macro')*100\n",
    "    print(\"The Precision is\", precision)\n",
    "    CReport = crep(y_test, y_pred)\n",
    "    print(\"The Classification Report is\\n\", CReport)\n",
    "    LogLoss = logloss(y_test, y_pred)\n",
    "    print(\"The LogLoss is\", LogLoss)\n",
    "    fpr, tpr, _ = roccurve(y_test, y_pred)\n",
    "    AreaUnderCurve = auc(fpr, tpr)\n",
    "    print(\"The Area Under the Curve is\", AreaUnderCurve)\n",
    "    RecallScore = rscore(y_test, y_pred, average = 'macro')*100\n",
    "    print(\"The Recall Score is\", RecallScore)\n",
    "    FScore = f1score(y_test, y_pred)\n",
    "    print(\"The F-Score is\", FScore)\n",
    "    return y_pred\n",
    "\n",
    "def light(X_train, X_test, y_train, y_test):\n",
    "    X_train = StdSclr().fit_transform(X_train, y_train)\n",
    "    X_test = StdSclr().fit_transform(X_test)\n",
    "    clf = clf = lgbm.LGBMClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"These are the predicted values: \", y_pred)\n",
    "    confmatrix = confm(y_test, y_pred)\n",
    "    print(\"The Confusion Matrix is\\n\", confmatrix)\n",
    "    Accuracy_Score = acs(y_test, y_pred)*100\n",
    "    print(\"The Accuracy Score is\\n\", Accuracy_Score)\n",
    "    precision = pscore(y_test, y_pred, pos_label = '1', labels = [0,1])*100\n",
    "    print(\"The Precision is\", precision)\n",
    "    CReport = crep(y_test, y_pred)\n",
    "    print(\"The Classification Report is\\n\", CReport)\n",
    "    LogLoss = logloss(y_test, y_pred)\n",
    "    print(\"The LogLoss is\", LogLoss)\n",
    "    fpr, tpr, _ = roccurve(y_test, y_pred)\n",
    "    AreaUnderCurve = auc(fpr, tpr)\n",
    "    print(\"The Area Under the Curve is\", AreaUnderCurve)\n",
    "    RecallScore = rscore(y_test, y_pred,pos_label='1',labels=[0,1])*100\n",
    "    print(\"The Recall Score is\", RecallScore)\n",
    "    print(\"The Specificity is\", fpr, \"and the Sensitivity is\", tpr)\n",
    "    FScore = f1score(y_test, y_pred)\n",
    "    print(\"The F-Score is\", FScore)\n",
    "\n",
    "def LogisticRegression(X_train, X_test, y_train, y_test):\n",
    "    y_pred = LogR().fit(X_train, y_train).predict(X_test)\n",
    "    print(\"These are the predicted values: \", y_pred)\n",
    "    confmatrix = confm(y_test, y_pred)\n",
    "    print(\"The Confusion Matrix is\\n\", confmatrix)\n",
    "    Accuracy_Score = acs(y_test, y_pred)*100\n",
    "    print(\"The Accuracy Score is\\n\", Accuracy_Score)\n",
    "    precision = pscore(y_test, y_pred, average = 'macro', labels = [0,1])*100\n",
    "    print(\"The Precision is\", precision)\n",
    "    CReport = crep(y_test, y_pred)\n",
    "    print(\"The Classification Report is\\n\", CReport)\n",
    "    LogLoss = logloss(y_test, y_pred)\n",
    "    print(\"The LogLoss is\", LogLoss)\n",
    "    fpr, tpr, _ = roccurve(y_test, y_pred)\n",
    "    AreaUnderCurve = auc(fpr, tpr)\n",
    "    print(\"The Area Under the Curve is\", AreaUnderCurve)\n",
    "    RecallScore = rscore(y_test, y_pred,average = 'macro',labels=[0,1])*100\n",
    "    print(\"The Recall Score is\", RecallScore)\n",
    "    print(\"The Specificity is\", fpr, \"and the Sensitivity is\", tpr)\n",
    "    FScore = f1score(y_test, y_pred)\n",
    "    print(\"The F-Score is\", FScore)\n",
    "    \n",
    "    \n",
    "def DecTree(X_train, X_test, y_train, y_test):\n",
    "    clf= dtc()\n",
    "    y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "    print(\"These are the predicted values: \", y_pred)\n",
    "    confmatrix = confm(y_test, y_pred)\n",
    "    print(\"The Confusion Matrix is\\n\", confmatrix)\n",
    "    Accuracy_Score = acs(y_test, y_pred)*100\n",
    "    print(\"The Accuracy Score is\\n\", Accuracy_Score)\n",
    "    precision = pscore(y_test, y_pred, pos_label = '1', labels = [0,1])*100\n",
    "    print(\"The Precision is\", precision)\n",
    "    CReport = crep(y_test, y_pred)\n",
    "    print(\"The Classification Report is\", CReport)\n",
    "    LogLoss = logloss(y_test, y_pred)\n",
    "    print(\"The LogLoss is\", LogLoss)\n",
    "    fpr, tpr, _ = roccurve(y_test, y_pred)\n",
    "    AreaUnderCurve = auc(fpr, tpr)\n",
    "    print(\"The Area Under the Curve is\", AreaUnderCurve)\n",
    "    RecallScore = rscore(y_test, y_pred,pos_label='1',labels=[0,1])*100\n",
    "    print(\"The Recall Score is\", RecallScore)\n",
    "    print(\"The Specificity is\", fpr, \"and the Sensitivity is\", tpr)\n",
    "    FScore = f1score(y_test, y_pred)\n",
    "    print(\"The F-Score is\", FScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96599112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(dataset):\n",
    "    y=str(input(\"What is the label column\"))\n",
    "    label=dataset[y]\n",
    "    X=dataset.drop(columns=[y])\n",
    "    X_train, X_test, y_train, y_test = tts(X, label, test_size=0.33, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae4e287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c142eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize(X_train, y_train, X_test, y_test, X, y):\n",
    "    method = str(input(\"Which regularization do you want to implement (lasso, ridge, lin regression, log regression)?\"))\n",
    "    scaler = StdSclr(copy=True, with_mean=True, with_std=True)\n",
    "    X_train = scaler.fit(X_train.fillna(0))\n",
    "    if method == 'lasso':\n",
    "        cross_val_scores_lasso = []\n",
    "        alpha = []\n",
    "        for i in range(1, 9):\n",
    "            model = lso(alpha = i * 0.25, tol=0.0925)\n",
    "            model.fit(X_train, y_train)\n",
    "            scores = cvl(model, X,y, cv = 10)\n",
    "            avg_cross_val_score = mean(scores)*100\n",
    "            cross_val_scores_lasso.append(avg_cross_val_score)\n",
    "            alpha.append(i * 0.25)\n",
    "        for i in range(0, len(alpha)):\n",
    "            print(str(alpha[i])+' : '+str(cross_val_scores_lasso[i]))\n",
    "        chosen = lso(alpha = alpha[cross_val_scores_lasso.index(max(cross_val_scores_lasso))], tol = 0.0925)\n",
    "        chosen.fit(X_train, y_train)\n",
    "        print('The regularization score of the Lasso model is', chosen.score(X_test, y_test))\n",
    "        y_pred = chosen.fit(X_train, y_train).predict(X_test)\n",
    "        print(\"These are the predicted values: \", y_pred)\n",
    "    if method == 'ridge':\n",
    "        cross_val_scores_ridge = []\n",
    "        alpha = []\n",
    "        for i in range(1, 9):\n",
    "            model = rdg(alpha = i * 0.25)\n",
    "            model.fit(X_train, y_train)\n",
    "            scores = cvl(model, X, y, cv = 10)\n",
    "            avg_cross_val_score = mean(scores)*100\n",
    "            cross_val_scores_ridge.append(avg_cross_val_score)\n",
    "            alpha.append(i * 0.25)\n",
    "        for i in range(0, len(alpha)):\n",
    "            print(str(alpha[i])+' : '+str(cross_val_scores_ridge[i]))\n",
    "        chosen = rdg(alpha = alpha[cross_val_scores_ridge.index(max(cross_val_scores_ridge))])\n",
    "        chosen.fit(X_train, y_train)\n",
    "        print('The regularization score of the Ridge model is', chosen.score(X_test, y_test))\n",
    "        y_pred = chosen.fit(X_train, y_train).predict(X_test)\n",
    "        print(\"These are the predicted values: \", y_pred)\n",
    "    if method == 'log regression':\n",
    "        model = LogR()\n",
    "        model.fit(X_train, y_train)\n",
    "        score = model.score(X_test, y_test)\n",
    "        print('The regularization score with Logarithmic Regression is', score, '.')\n",
    "        y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "    if method == 'lin regression':\n",
    "        model = LR()\n",
    "        model.fit(X_train, y_train)\n",
    "        score = model.score(X_test, y_test)\n",
    "        print('The regularization score with Linear Regression is', score, '.')\n",
    "        y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "        print(\"These are the predicted values: \", y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "67b87d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalscore(X_train, X_test, y_train, y_test):\n",
    "    testsize = float(input('What do you want the test size to be? Please input in decimal form.'))\n",
    "    knl = str(input('Which kernel do you want to apply? (linear, rbf)'))\n",
    "    clf = XGBC(disable_default_eval_metric=1)\n",
    "    score = cvl(clf, X_train, y_train, cv=5)\n",
    "    print('The normal cross validation score is', score)\n",
    "    print(\"For the normal cross validation score, there is a %0.2f accuracy with a standard deviation of %0.2f\" % (score.mean(), score.std()))\n",
    "    sscore = cvl(clf, X_train, y_train, cv=SSS(n_splits = 5, test_size=testsize, random_state=0))\n",
    "    print('The ShuffleSplit score is', sscore)\n",
    "    print(\"For the ShuffleSplit cross validation score, there is a %0.2f accuracy with a standard deviation of %0.2f\" % (sscore.mean(), sscore.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1479a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossval(dataset):\n",
    "    method = str(input(\"What Fold method do you want to use?(kfold, rkfold, skfold, lvo, sss, rskfold)\"))\n",
    "    if method == 'kfold':\n",
    "        k = int(input(\"What number-fold cross validation do you want to do?\"))\n",
    "        kf= KFold(n_splits=k)\n",
    "        for train_index, test_index in kf.split(dataset):\n",
    "            trainX, testX = X.iloc[train_index], X.iloc[test_index] \n",
    "            trainY, testY = y.iloc[train_index], y.iloc[test_index]\n",
    "            return trainX, testX, trainY, testY\n",
    "    if method == 'rkfold':\n",
    "        k = int(input(\"What number-fold cross validation do you want to do?\"))\n",
    "        r = int(input(\"How many times do you want to repeat the cross validation?\"))\n",
    "        kf = RKFold(n_splits=k, n_repeats=r, random_state=random_state)\n",
    "        for train_index, test_index in kf.split(dataset):\n",
    "            trainX, testX = X.iloc[train_index], X.iloc[test_index] \n",
    "            trainY, testY = y.iloc[train_index], y.iloc[test_index]\n",
    "            return trainX, testX, trainY, testY\n",
    "    if method == 'skfold':\n",
    "        k = int(input(\"What number-fold cross validation do you want to do?\"))\n",
    "        kf = SKFold(n_splits=k, random_state=None, shuffle=False)\n",
    "        label = str(input(\"What is the label of the dataset?\"))\n",
    "        y=dataset[label]\n",
    "        X=dataset.drop([label], axis=1)\n",
    "        kf.get_n_splits(X,y)\n",
    "        print(kf)\n",
    "        for train_index, test_index in kf.split(X,y):\n",
    "            trainX, testX = X.iloc[train_index], X.iloc[test_index] \n",
    "            trainY, testY = y.iloc[train_index], y.iloc[test_index]\n",
    "            return trainX, testX, trainY, testY\n",
    "    if method == 'rskfold':\n",
    "        k = int(input(\"What number-fold cross validation do you want to do?\"))\n",
    "        r = int(input(\"How many times do you want to repeat the cross validation?\"))\n",
    "        kf = rskfold(n_splits=k, n_repeats=r, random_state=36851234)\n",
    "        label = str(input(\"What is the label of the dataset?\"))\n",
    "        y=dataset[label]\n",
    "        X=dataset.drop([label], axis=1)\n",
    "        kf.get_n_splits(X,y)\n",
    "        print(kf)\n",
    "        for train_index, test_index in kf.split(X,y):\n",
    "            trainX, testX = X.iloc[train_index], X.iloc[test_index] \n",
    "            trainY, testY = y.iloc[train_index], y.iloc[test_index]\n",
    "            return trainX, testX, trainY, testY\n",
    "    if method == 'lvo':\n",
    "        for train_index, test_index in lvo.split(dataset):\n",
    "            trainX, testX = X.iloc[train_index], X.iloc[test_index] \n",
    "            trainY, testY = y.iloc[train_index], y.iloc[test_index]\n",
    "            return trainX, testX, trainY, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c7594c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(X,y):\n",
    "    estimator = SVR(kernel=\"linear\")\n",
    "    selector = RFE(estimator, n_features_to_select=5, step=1)\n",
    "    selector = selector.fit(X, y)\n",
    "    print(selector.support_)\n",
    "    print(selector.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cd2f8f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANOVA(dataset):\n",
    "    colname1 = str(input(\"first column you want to relate\"))\n",
    "    colname2 = str(input(\"second column you want to relate\"))\n",
    "    covariance = np.cov(dataset[colname1], dataset[colname2])\n",
    "    print(\"The Covariance is \",covariance)\n",
    "    \n",
    "    corr1, _ = spm(dataset[colname1], dataset[colname2])\n",
    "    print('Spearman correlation: %.2f' % corr1)\n",
    "    corr2, _ = pe(dataset[colname1], dataset[colname2])\n",
    "    print('Pearsons correlation: %.2f' % corr2)\n",
    "    print('The t-test value is', (dataset[colname1].mean()-dataset[colname2].mean())/(math.sqrt((dataset[colname1].std()**2)/dataset[colname1].count() + (dataset[colname2].std()**2)/dataset[colname2].count())))\n",
    "    obs = np.array(dataset)\n",
    "    obs[obs<0]=0\n",
    "    chi2_contingency(obs)\n",
    "    g, p, dof, expctd = chi2_contingency(obs, lambda_=\"log-likelihood\")\n",
    "    print('The critical value of Chi-Square is', g) \n",
    "    print('The p-value is', p)\n",
    "    print('The degrees of freedom are', dof)\n",
    "    print('The expected values using the Chi-Square method are', expctd)\n",
    "    alpha = 0.05\n",
    "    if p <= alpha:\n",
    "        print('Dependent (reject H0)')\n",
    "    else:\n",
    "        print('Independent (H0 holds true)')\n",
    "    return dataset.plot(x= colname1,y=colname2,kind='scatter',alpha=0.5,cmap='rainbow')\n",
    "\n",
    "def clheatmap(dataset):\n",
    "    corrmatrix = dataset.corr()\n",
    "    colour = str(input(\"Heatmap colour?\\n\"))\n",
    "    clmap = sb.clustermap(corrmatrix, cmap = colour, linewidths = 0.2); \n",
    "    f, axis = plt.subplots(figsize =(15, 10)) \n",
    "    plt.setp(clmap.ax_heatmap.yaxis.get_majorticklabels(), rotation = 0) \n",
    "    return clmap\n",
    "\n",
    "\n",
    "def correlation_analysis(dataset):\n",
    "    numcols = dataset.select_dtypes(include=np.number)\n",
    "    corr = numcols.corr()\n",
    "    ax = sb.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sb.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    "    )\n",
    "    \n",
    "    ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "84c13623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numanalysis(dataset):\n",
    "    colname = str(input(\"which column do you want to analyze? Press enter if you want to analyze the entire dataset. \"))\n",
    "    analysis_type = str(input(\"What analysis do you want to do? (bar charts, value counts?(barchart, value counts))\"))\n",
    "    if analysis_type == 'value counts':\n",
    "        if colname != '':\n",
    "            return dataset[colname].value_counts()\n",
    "        else:\n",
    "            return dataset.value_counts()\n",
    "    elif analysis_type == 'barchart':\n",
    "        if colname != '':\n",
    "            return dataset[colname].plot.bar()\n",
    "        else:\n",
    "            return dataset[colname].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "817d4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Usampler(X,y):\n",
    "    print('Original dataset shape %s' % Counter(y))\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_res, y_res = rus.fit_resample(X, y)\n",
    "    print('Resampled dataset shape %s' % Counter(y_res))\n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "06963d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Osampler(X,y):\n",
    "    print('Original dataset shape %s' % Counter(y))\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_res, y_res = ros.fit_resample(X, y)\n",
    "    print('Resampled dataset shape %s' % Counter(y_res))\n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f958bcc9",
   "metadata": {},
   "source": [
    "# TSF Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2a8b7762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadseries():\n",
    "    name = str(input('what is the name of file?'))\n",
    "    return pd.read_csv(name, header=0, index_col=0, parse_dates=True, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecf5d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagset(dataset):\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    steps = int(input('How many steps do you want to lag the dataset?'))\n",
    "    newdata = dataset.shift(steps)\n",
    "    newdata = pd.concat([newdata, dataset], axis=1)\n",
    "    newdata.fillna(0, inplace=True)\n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f1b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollingmean2(dataset):\n",
    "    data = pd.DataFrame(dataset.values)\n",
    "    window2 = data.shift(2).rolling(window=2)\n",
    "    means=window2.mean()\n",
    "    rwindow2 = pd.concat([means, data],axis=1)\n",
    "    rwindow2.columns = ['mean(t-2,t-1)', 't']\n",
    "    rwindow2.fillna(0, inplace=True)\n",
    "    return rwindow2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecbc423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollingmean3(dataset):\n",
    "    data = pd.DataFrame(dataset.values)\n",
    "    window3 = data.shift(3).rolling(window=3)\n",
    "    avg = window3.mean()\n",
    "    rwindow3 = pd.concat([avg, data], axis=1)\n",
    "    rwindow3.columns = ['mean(t-3, t-2, t-1)', 't']\n",
    "    rwindow3.fillna(0, inplace=True)\n",
    "    return rwindow3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19245ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expandingwindow(dataset):\n",
    "    expwindow = dataset.expanding()\n",
    "    dataframe = pd.concat([expwindow.min(), expwindow.mean(), expwindow.max(), shampsales.shift(-1)], axis=1)\n",
    "    dataframe.columns = ['min', 'mean', 'max', 't']\n",
    "    print(dataframe.head(5))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9445ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(dataset):\n",
    "    frequency = str(input(\"What is your resampling frequency? (quarterly, months, weeks, days, hours, minutes, seconds?)\"))\n",
    "    if frequency == 'quarterly':\n",
    "        resmpdata = dataset.resample('Q').mean()\n",
    "    elif frequency == 'months':\n",
    "        resmpdata = dataset.resample('M').mean()\n",
    "    elif frequency == 'weeks':\n",
    "        resmpdata = dataset.resample('W').mean()\n",
    "    elif frequency == 'days':\n",
    "        resmpdata = dataset.resample('D').mean()\n",
    "    elif frequency == 'hours':\n",
    "        resmpdata = dataset.resample('H').mean()\n",
    "    elif frequency == 'minutes':\n",
    "        resmpdata = dataset.resample('T').mean()\n",
    "    elif frequency == 'seconds':\n",
    "        resmpdata = dataset.resample('S').mean()\n",
    "    else:\n",
    "        print(\"error\")\n",
    "        return None \n",
    "    return resmpdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "149ff03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolseriesdata(dataset):\n",
    "    interpoldata = dataset.interpolate(method='linear')\n",
    "    print(interpoldata.head())\n",
    "    interpoldata.plot(marker = 'X', linestyle = '-', alpha = 0.8, figsize = (20,8), color='purple')\n",
    "    plt.show()\n",
    "    return interpoldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7a9c565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonaldecompose(dataset):\n",
    "    colname = str(input(\"What column do you want to decompose?\"))\n",
    "    timeperiod = str(input(\"What time period do you want to decompose the data over? (months, weeks, days)\"))\n",
    "    if timeperiod == 'months':\n",
    "        result = seasonal_decompose(dataset[colname], model='additive', freq=12)\n",
    "    elif timeperiod == 'weeks':\n",
    "        result = seasonal_decompose(dataset[colname], model='additive', freq=52)\n",
    "    elif timeperiod == 'days':\n",
    "        result = seasonal_decompose(dataset[colname], model='additive', freq=365)\n",
    "    else:\n",
    "        print(\"error\")\n",
    "    return result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f942da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationaritycheck(dataset):\n",
    "    colname = str(input(\"What column do you want to check the stationarity for?\"))\n",
    "    X = dataset[colname].values\n",
    "    split=int(len(X)/2)\n",
    "    X1,X2=X[0:split],X[split:]\n",
    "    mean1, mean2=X1.mean(),X2.mean()\n",
    "    var1, var2= X1.var(),X2.var()\n",
    "    print('mean1=%f,\\nmean2=%f'%(mean1,mean2))\n",
    "    print('var1=%f,\\nvar2=%f'%(var1,var2))\n",
    "    variance1 = abs(mean1-mean2)\n",
    "    variance2 = abs(var1-var2)\n",
    "    if variance1 > 0:\n",
    "        print(\"There is a difference of\", variance1, \"between the means of the two halves of the dataset.\")\n",
    "    else:\n",
    "        print(\"The data is stationary.\")\n",
    "    if variance2 > 0:\n",
    "        print(\"There is a difference of\", variance2, \"between the variances of the two halves of the dataset.\")\n",
    "    else:\n",
    "        print(\"The data is stationary.\")\n",
    "    ADF = str(input(\"Do you want to administer the ADFuller test for further clarity? (Yes (y) or No (n))\"))\n",
    "    if ADF == 'n':\n",
    "        return\n",
    "    else:\n",
    "        result1 = adfuller(X)\n",
    "        print('ADF Statistic for basic data: %f' % result1[0])\n",
    "        if result1[0]>1:\n",
    "            print('Very likely that null hypothesis is true, meaning we have a non-stationary dataset')\n",
    "        elif result1[0]<0:\n",
    "            print('Very likely that null hypothesis is false, meaning we have a stationary dataset')\n",
    "        else:\n",
    "            print('Hard to say if null hypothesis is true or false, it is complicated')\n",
    "        print('p-value : %f'%result1[1])\n",
    "        print(' Critical Values:\\n        Key   Value')\n",
    "        for key , value in result1[4].items():\n",
    "            print('\\t%s: %.6f' % (key, value))\n",
    "        print('\\n')\n",
    "        \n",
    "        result2 = adfuller(np.log(X))\n",
    "        print('ADF Statistic for log transformed data: %f' % result2[0])\n",
    "        if result2[0]>1:\n",
    "            print('Very likely that null hypothesis is true, meaning we have a non-stationary dataset')\n",
    "        elif result2[0]<0:\n",
    "            print('Very likely that null hypothesis is false, meaning we have a stationary dataset')\n",
    "        else:\n",
    "            print('Hard to say if null hypothesis is true or false, it is complicated')\n",
    "        print('p-value : %f'%result2[1])\n",
    "        print(' Critical Values:\\n        Key   Value')\n",
    "        for key , value in result2[4].items():\n",
    "            print('\\t%s: %.6f' % (key, value))\n",
    "        print('\\n')\n",
    "        \n",
    "        result = adfuller(np.sqrt(X))\n",
    "        print('ADF Statistic for square root transformed data: %f' % result[0])\n",
    "        if result[0]>1:\n",
    "            print('Very likely that null hypothesis is true, meaning we have a non-stationary dataset')\n",
    "        elif result[0]<0:\n",
    "            print('Very likely that null hypothesis is false, meaning we have a stationary dataset')\n",
    "        else:\n",
    "            print('Hard to say if null hypothesis is true or false, it is complicated')\n",
    "        print('p-value : %f'%result[1])\n",
    "        print(' Critical Values:\\n        Key   Value')\n",
    "        for key , value in result[4].items():\n",
    "            print('\\t%s: %.6f' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3827d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxcoxtransformation(dataset):\n",
    "    colname = str(input(\"What column do you want to transform?\"))\n",
    "    lambdavalue = int(input(\"Set a lambda value which transforms the data closest to a normal distribution. (What transformation gives the best ADFuller test result?)\"))\n",
    "    dataset[colname]=boxcox(dataset[colname],lmbda=lambdavalue) \n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.subplot(211)\n",
    "    plt.plot(dataset[colname])\n",
    "    plt.subplot(212)\n",
    "    plt.hist(dataset[colname],edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed140b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def differencing(dataset, interval):\n",
    "    differenced = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        differenced.append(value)\n",
    "    return pd.Series(differenced)\n",
    "\n",
    "def inversedifferencing(history, yhat, interval):\n",
    "    newdata = yhat + history[-interval]\n",
    "    return newdata\n",
    "\n",
    "def inversetransformation(differenceddataset, originaldata, yhat, interval):\n",
    "    inverteddataset = list()\n",
    "    for i in range(len(differenceddataset)):\n",
    "        value = inversedifferencing(originaldata, yhat, interval)\n",
    "        inverteddataset.append(value)\n",
    "    inverted = pd.Series(inverteddataset)\n",
    "    print(inverted.head(10))\n",
    "    return inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6c2199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(series):\n",
    "    X = series.values.reshape(len(X),1)\n",
    "    scaler = MMS(feature_range = (-1,1)).fit(X)\n",
    "    transformation = str(input(\"Type 's' for normal scaling and 'i' for inverse scaling.\"))\n",
    "    if transformation == 's':\n",
    "        scaled = scaler.transform(X)\n",
    "        scaled = pd.series(scaled[:,0])\n",
    "        print(scaled.head())\n",
    "        return scaled\n",
    "    else:\n",
    "        invertedscaled = scaler.inverse_transform(scaled)\n",
    "        inverted_series = pd.Series(invertedscaled[:, 0])\n",
    "        print(inverted_series.head(10))\n",
    "        return inverted_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3698afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkcorrelation(dataset):\n",
    "    colname = str(input(\"Which column do you want to plot?\"))\n",
    "    plt.figure(figsize = (11,5))\n",
    "    lag_plot(dataset[colname])\n",
    "    acplot(dataset[colname])\n",
    "    plot_acf(dataset[colname], lags=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7a5eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregression(dataset):\n",
    "    colname = str(input(\"Which column do you want to analyze?\"))\n",
    "    data = dataset[colname].values\n",
    "    train, test = data[0:len(data)*0.66], data[len(data)*0.66:]\n",
    "    train_X, train_y = train[:,0], train[:,1]\n",
    "    test_X, test_y = test[:,0], test[:,1]\n",
    "    pred = [x for x in data]\n",
    "    train_resid = [train_y[i]-pred[i] for i in range(len(pred))]\n",
    "    model = AR(train_resid).fit()\n",
    "    window = model.k_ar\n",
    "    prediction = model_fit.forecast(steps=5)\n",
    "    coef = model_fit.params\n",
    "    print('\\nLag=%d \\nCoef=%s' % (window, coef))\n",
    "    history = train_resid[len(train_resid)-window:]\n",
    "    history = [history[i] for i in range(len(history))]\n",
    "    predictions = list()\n",
    "    expected_error = list()\n",
    "    for t in range(len(test_y)):\n",
    "        yhat = test_X[t]\n",
    "        error = test_y[t] - yhat\n",
    "        expected_error.append(error)\n",
    "        length = len(history)\n",
    "        lag = [history[i] for i in range(length-window,length)]\n",
    "        pred_error = coef[0]\n",
    "        for d in range(window):\n",
    "            pred_error += coef[d+1] * lag[window-d-1]\n",
    "        predictions.append(pred_error)\n",
    "        history.append(error)\n",
    "        print('predicted error=%f, expected error=%f' % (pred_error, error))\n",
    "    \n",
    "    performance = MSE(test_y, predictions, squared = False)\n",
    "    print(\"\\n Test RMSE: %.4f\" % performance)\n",
    "    plt.plot(expected_error, 'red')\n",
    "    plt.plot(test_y, color = 'blue')\n",
    "    plt.plot(predictions, color = 'green')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2ffdd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARIMAmodel(dataset, colname):\n",
    "    model = ARIMA(dataset[colname], order=(3,1,3))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    print(model_fit.summary())\n",
    "    residuals = pd.DataFrame(model_fit.resid)\n",
    "    print(\"\\n Normal Residuals Plot\")\n",
    "    residuals.plot()\n",
    "    plt.show()\n",
    "    print(\"\\n Kernel Density Residuals Plot\")\n",
    "    residuals.plot(kind='kde')\n",
    "    plt.show()\n",
    "    print(\"\\n\", residuals.describe())\n",
    "    history = dataset[colname].values\n",
    "    model2 = ARIMA(history, order=(2,1,1))\n",
    "    model2_fit = model2.fit(disp=0)\n",
    "    prediction = pd.DataFram(model_fit.forecast(steps=5))\n",
    "    print(\"\\n\", prediction)\n",
    "    print(\"\\n Normal Predictions Plot\")\n",
    "    prediction.plot()\n",
    "    plt.show()\n",
    "    print(\"\\n Kernel Density Predictions Plot\")\n",
    "    prediction.plot(kind='kde')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94eda63",
   "metadata": {},
   "source": [
    "## Building a Dataset from Raw File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bc637e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for reading a dictionary standard words UNIX file\n",
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [word for i in range(len(content)) for word in content[i].split()]\n",
    "    content = np.array(content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "271ab2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building dictionary\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5cd1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aee082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a89c804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
