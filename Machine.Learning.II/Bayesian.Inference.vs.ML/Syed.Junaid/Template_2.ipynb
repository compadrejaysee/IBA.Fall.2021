{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filetype):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pyodbc\n",
    "    valid = {'sql','csv','excel'}\n",
    "    if filetype not in valid:\n",
    "        raise ValueError(\"filetype must be one of %r.\" % valid)\n",
    "    if filetype=='csv':\n",
    "        path=input('Enter path of file')\n",
    "        df=pd.read_csv(path)\n",
    "    elif filetype=='excel':\n",
    "        path=input('Enter path of file')\n",
    "        df=pd.read_excel(path)\n",
    "    else:\n",
    "        query=input('Enter sql query')\n",
    "        #connect_params=input('Enter connection parameters')\n",
    "        conn = pyodbc.connect('Driver={SQL Server};'\n",
    "                      'Server=DESKTOP-AIS9287\\SQLEXPRESS;'\n",
    "                      'Database=ML_PROJECT;'# Parameters can be altered accordingly\n",
    "                      'Trusted_Connection=yes;')\n",
    "        cursor = conn.cursor()\n",
    "        df = pd.read_sql_query(query,conn)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overview(df,head,n):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    print('The shape is :',df.shape)\n",
    "    print('The column data types are: \\n')\n",
    "    print(df.dtypes)\n",
    "    print('\\n')\n",
    "    valid = {'head','tail'}\n",
    "    if head not in valid:\n",
    "        raise ValueError(\"Either head or tail\")\n",
    "    if head=='head':\n",
    "        print('The first '+str(n)+' rows are :')\n",
    "        display(df.head(n))\n",
    "    else:\n",
    "        print('The last '+str(n)+' rows are :')\n",
    "        display(df.tail(n))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_details(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    b = pd.DataFrame()\n",
    "    b['Missing value, %'] = round(df.isnull().sum()/df.shape[0]*100)\n",
    "    b['Missing value count']=df.isnull().sum()\n",
    "    b['N unique value'] = df.nunique()\n",
    "    return b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest(group1,group2,related=False):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pyodbc\n",
    "    from scipy import stats\n",
    "    if related==False:\n",
    "        stat,p=stats.ttest_ind(group1,group2)\n",
    "        if p<0.05:\n",
    "            print('P value is:',p)\n",
    "            print('Sampling error does not exist')\n",
    "        else:\n",
    "            print('P value is:',p)\n",
    "            print('Sampling error exists')\n",
    "    else:\n",
    "        stat,p=stats.ttest_rel(group1,group2)\n",
    "        if p<0.05:\n",
    "            print('P value is:',p)\n",
    "            print('Sampling error does not exist')\n",
    "        else:\n",
    "            print('P value is:',p)\n",
    "            print('Sampling error exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi square test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_test(df,*cols):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pyodbc\n",
    "    from scipy.stats import chi2_contingency\n",
    "    x=[]\n",
    "    for i in cols:\n",
    "        x.append(i)\n",
    "    target=input('Enter target variable')\n",
    "    for i in x:\n",
    "        print(i)\n",
    "        data=pd.crosstab(df[i],df[target])\n",
    "        stat, p, dof, expected = chi2_contingency(data)\n",
    "\n",
    "        # interpret p-value\n",
    "        alpha = 0.05\n",
    "        print(\"p value is \" + str(p))\n",
    "        if p <= alpha:\n",
    "            print('Dependent (reject H0)')\n",
    "        else:\n",
    "            print('Independent (H0 holds true)')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova(df,continous,categorical):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.formula.api import ols\n",
    "    lm=ols(continous+'~'+categorical,data=df).fit()\n",
    "    table=sm.stats.anova_lm(lm)\n",
    "    display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_heatmap(df):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import statsmodels.api as sm\n",
    "    corr=df.corr()\n",
    "    plt.subplots(figsize=(12,12))\n",
    "    sns.heatmap(corr,annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normality_test(df,*cols):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import statsmodels.api as sm\n",
    "    import pylab as py\n",
    "    for i in cols:\n",
    "        x.append(i)\n",
    "    sm.qqplot(df[column], line ='s') \n",
    "    py.show()\n",
    "    from scipy.stats import skew\n",
    "    from scipy.stats import kurtosis\n",
    "    method=input('Enter normality test')\n",
    "    if method=='Shapiro Wilk Test':\n",
    "        for column in x:\n",
    "            print(column)\n",
    "            sm.qqplot(df[column], line ='s') \n",
    "            py.show()\n",
    "            display(df[column].describe())\n",
    "            from scipy.stats import skew\n",
    "            from scipy.stats import kurtosis\n",
    "            Skew=skew(df[column])\n",
    "            Kurtosis=kurtosis(df[column])\n",
    "            print('Skew is',Skew)\n",
    "            print('Kurtosis is',Kurtosis)\n",
    "            from scipy.stats import shapiro\n",
    "            stat,p=shapiro(df[column])\n",
    "            print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "            if p>0.05:\n",
    "                print('Normally distributed according to shapiro test')\n",
    "            else:\n",
    "                print('Not Normally distributed according to shapiro test')\n",
    "            print('\\n')\n",
    "    elif method=='Anderson':\n",
    "        for column in x:\n",
    "            print(column)\n",
    "            sm.qqplot(df[column], line ='s') \n",
    "            py.show()\n",
    "            display(df[column].describe())\n",
    "            from scipy.stats import skew\n",
    "            from scipy.stats import kurtosis\n",
    "            Skew=skew(df[column])\n",
    "            Kurtosis=kurtosis(df[column])\n",
    "            print('Skew is',Skew)\n",
    "            print('Kurtosis is',Kurtosis)\n",
    "            from scipy.stats import anderson\n",
    "            result=anderson(df[column])\n",
    "            print('stat=%.3f'%(result.statistic))\n",
    "            for i in range(len(result.critical_values)):\n",
    "                sig_level,cric_value=result.significance_level[i],result.critical_values[i]\n",
    "                if result.statistic<cric_value:\n",
    "                    print(f\"Probably Normally distributed {cric_value} critical value at {sig_level} level of significance\")\n",
    "                else:\n",
    "                    print(f\"Probably Not Normally distributed {cric_value} critical value at {sig_level} level of significance\")\n",
    "            print('\\n')\n",
    "    elif method=='Chi square':\n",
    "        for column in x:\n",
    "            print(column)\n",
    "            sm.qqplot(df[column], line ='s') \n",
    "            py.show()\n",
    "            display(df[column].describe())\n",
    "            from scipy.stats import skew\n",
    "            from scipy.stats import kurtosis\n",
    "            Skew=skew(df[column])\n",
    "            Kurtosis=kurtosis(df[column])\n",
    "            print('Skew is',Skew)\n",
    "            print('Kurtosis is',Kurtosis)\n",
    "            from scipy.stats import chisquare\n",
    "            stat,p=chisquare(df[column])\n",
    "            print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "            if p>0.05:\n",
    "                print('Normally distributed according to chisquare test')\n",
    "            else:\n",
    "                print('Not Normally distributed according to chisquare test')\n",
    "            print('\\n')\n",
    "    elif method=='Lilliefors':\n",
    "        for column in x:\n",
    "\n",
    "            print(column)\n",
    "            sm.qqplot(df[column], line ='s') \n",
    "            py.show()\n",
    "            display(df[column].describe())\n",
    "            from scipy.stats import skew\n",
    "            from scipy.stats import kurtosis\n",
    "            Skew=skew(df[column])\n",
    "            Kurtosis=kurtosis(df[column])\n",
    "            print('Skew is',Skew)\n",
    "            print('Kurtosis is',Kurtosis)\n",
    "            from statsmodels.stats.diagnostic import lilliefors\n",
    "            stat,p=lilliefors(df[column])\n",
    "            print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "            if p>0.05:\n",
    "                print('Normally distributed according to lilliefors test')\n",
    "            else:\n",
    "                print('Not Normally distributed according to lilliefors test')\n",
    "            print('\\n')\n",
    "    elif method=='Kolmogorov':\n",
    "        for column in x:\n",
    "            print(column)\n",
    "            sm.qqplot(df[column], line ='s') \n",
    "            py.show()\n",
    "            display(df[column].describe())\n",
    "            from scipy.stats import skew\n",
    "            from scipy.stats import kurtosis\n",
    "            Skew=skew(df[column])\n",
    "            Kurtosis=kurtosis(df[column])\n",
    "            print('Skew is',Skew)\n",
    "            print('Kurtosis is',Kurtosis)\n",
    "            from scipy.stats import kstest\n",
    "            stat,p=kstest(df[column],'norm')\n",
    "            print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "            if p>0.05:\n",
    "                print('Normally distributed according to Kolmogorov-Smirnov test')\n",
    "            else:\n",
    "                print('Not normally distributed according to Kolmogorov-Smirnov test')\n",
    "            print('\\n')\n",
    "    else:\n",
    "        raise ValueError('Method must be one of Shapiro Wilk Test,Anderson,Chi square,Lilliefors,Kolmogorov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_mv(df,column,method):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    if method=='mean':\n",
    "        df[column].fillna(df[column].mean(),inplace=True)\n",
    "    elif method=='median':\n",
    "        df[column].fillna(df[column].median(),inplace=True)\n",
    "    elif method=='mode':\n",
    "        df[column].fillna(df[column].mode()[0],inplace=True)\n",
    "    elif method=='value':\n",
    "        x=input('Enter value to replace null values')\n",
    "        df[column].fillna(x,inplace=True)\n",
    "    elif method=='interpolation':\n",
    "        x=input('forward or backward interpolation?')\n",
    "        if x=='forward':\n",
    "            df[column] = df['column'].interpolate(method ='linear', limit_direction ='forward')\n",
    "        elif x=='backward':\n",
    "            df[column] = df['column'].interpolate(method ='linear', limit_direction ='backward')\n",
    "        else:\n",
    "            raise ValueError('Select on from forward or backward')\n",
    "    elif method=='KNN':\n",
    "        from sklearn.impute import KNNImputer\n",
    "        neighbors=int(input('Enter number of number of neighbors'))\n",
    "        df2=df.copy()\n",
    "        imputer = KNNImputer (n_neighbors=neighbors)\n",
    "        y=[]\n",
    "        n=int(input('Enter the number of columns to fit KNN imputer'))\n",
    "        for i in range(n):\n",
    "            x=input('Enter name of column')\n",
    "            y.append(x)\n",
    "        y.append(column)\n",
    "        df[y] = imputer.fit_transform(df[y])\n",
    "        x_axis=input('x axis label')\n",
    "        y_axis=input('y axis label')\n",
    "        nulls=df2[x_axis].isna()+df2[y_axis].isna()\n",
    "        df.plot(x=x_axis,y=y_axis,kind='scatter',alpha=0.5,c=nulls,cmap='rainbow')\n",
    "        plt.show()\n",
    "    elif method=='group_mode':        \n",
    "        x=input('Enter name of second column')\n",
    "        df2=df[[column,x]]\n",
    "        df2[column]=df2.groupby(x).transform(lambda group:group.fillna(group.mode()[0]) )\n",
    "        df[column]=df2[column]\n",
    "    elif method=='drop': \n",
    "        df.dropna(axis=0, inplace=True)\n",
    "    else:\n",
    "        raise ValueError('method must be one of mean,median,mode,value,interpolation,KNN,group_mode or drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols(df,*columns):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    cols=[]\n",
    "    for i in columns:\n",
    "        cols.append(i)\n",
    "    df.drop(columns=cols,inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows(df,column,value):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    df.drop(df.loc[df[column]==value].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_details(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    b = pd.DataFrame()\n",
    "    b['Missing value, %'] = round(df.isnull().sum()/df.shape[0]*100)\n",
    "    b['Missing value count']=df.isnull().sum()\n",
    "    b['N unique value'] = df.nunique()\n",
    "    return b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_analysis(df,*cols):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import missingno as mn\n",
    "    x=[]\n",
    "    for i in cols:\n",
    "        x.append(i)\n",
    "    df2=df[x]\n",
    "    chart_type=input('Enter the type of chart required')\n",
    "    if chart_type=='bar':\n",
    "        mn.bar(df2)\n",
    "    elif chart_type=='matrix':\n",
    "        mn.matrix(df2)\n",
    "    elif chart_type=='heatmap':\n",
    "        mn.heatmap(df2)\n",
    "    elif chart_type=='dendrogram':\n",
    "        mn.dendrogram(df2)\n",
    "    else:\n",
    "        raise ValueError('Chart type must be on of bar,matrix,heatmap or dendrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_analysis(df,column):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    print(df[column].value_counts())\n",
    "    plt.subplots(figsize=(12,6))\n",
    "    sns.countplot(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_analysis(df,column):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import statsmodels.api as sm\n",
    "    import pylab as py\n",
    "    sns.distplot(df[column])\n",
    "    plt.show()\n",
    "    sns.boxplot(df[column])\n",
    "    plt.show()\n",
    "    sm.qqplot(df[column], line ='s') \n",
    "    py.show()\n",
    "    display(df[column].describe())\n",
    "    from scipy.stats import skew\n",
    "    from scipy.stats import kurtosis\n",
    "    Skew=skew(df[column])\n",
    "    Kurtosis=kurtosis(df[column])\n",
    "    print('Skew is',Skew)\n",
    "    print('Kurtosis is',Kurtosis)\n",
    "    if Skew>-2 and Skew<2 and Kurtosis>-7 and Kurtosis<7:\n",
    "        print('Can be considered normally distributed on the basis of skewness and kurtosis')\n",
    "    else:\n",
    "        print('Cannot be considered normally distributed on the basis of skewness and kurtosis')\n",
    "    print('\\n')\n",
    "    print('Shapiro Wilk Test for Normality')\n",
    "    from scipy.stats import shapiro\n",
    "    stat,p=shapiro(df[column])\n",
    "    print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "    if p>0.05:\n",
    "        print('Normally distributed according to shapiro test')\n",
    "    else:\n",
    "        print('Not Normally distributed according to shapiro test')\n",
    "    print('\\n')\n",
    "    print('Anderson Test for Normality')\n",
    "    from scipy.stats import anderson\n",
    "    result=anderson(df[column])\n",
    "    print('stat=%.3f'%(result.statistic))\n",
    "    for i in range(len(result.critical_values)):\n",
    "        sig_level,cric_value=result.significance_level[i],result.critical_values[i]\n",
    "        if result.statistic<cric_value:\n",
    "            print(f\"Probably Normally distributed {cric_value} critical value at {sig_level} level of significance\")\n",
    "        else:\n",
    "            print(f\"Probably Not Normally distributed {cric_value} critical value at {sig_level} level of significance\")\n",
    "    print('\\n')\n",
    "    print('Chi square Normality test')\n",
    "    from scipy.stats import chisquare\n",
    "    stat,p=chisquare(df[column])\n",
    "    print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "    if p>0.05:\n",
    "        print('Normally distributed according to chisquare test')\n",
    "    else:\n",
    "        print('Not Normally distributed according to chisquare test')\n",
    "    print('\\n')\n",
    "    print('Lilliefors Test for Normality')\n",
    "    from statsmodels.stats.diagnostic import lilliefors\n",
    "    stat,p=lilliefors(df[column])\n",
    "    print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "    if p>0.05:\n",
    "        print('Normally distributed according to lilliefors test')\n",
    "    else:\n",
    "        print('Not Normally distributed according to lilliefors test')\n",
    "    print('\\n')\n",
    "    print('Kolmogorov-Smirnov test')\n",
    "    from scipy.stats import kstest\n",
    "    stat,p=kstest(df[column],'norm')\n",
    "    print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "    if p>0.05:\n",
    "        print('Normally distributed according to Kolmogorov-Smirnov test')\n",
    "    else:\n",
    "        print('Not normally distributed according to Kolmogorov-Smirnov test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_type(df,column,t):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    if t=='integer':\n",
    "        df[column]=pd.to_numeric(df[column],downcast='integer')\n",
    "    elif t=='float':\n",
    "        df[column]=pd.to_numeric(df[column],downcast='float')\n",
    "    elif t=='datetime':\n",
    "        df[column]=pd.to_datetime(df[column])\n",
    "    elif t=='object':\n",
    "        df[column]=df[column].astype('object')\n",
    "    else:\n",
    "        raise ValueError('Must be one of integer,float,datetime,object')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_descr(df,column,value=None,new_value=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    x=input('Remove white space? Y or N')\n",
    "    if x=='Y':\n",
    "        df[column]=df[column].str.replace(' ','')\n",
    "    elif x=='N':\n",
    "        df[column]=df[column].replace(value,new_value)\n",
    "    else:\n",
    "        raise ValueError('Y or N only')\n",
    "    y=input('change column name? Y or N')\n",
    "    if y=='Y':\n",
    "        w=input('Remove with space? Y or N')\n",
    "        if w=='N':\n",
    "            z=input('Enter new column name')\n",
    "            df.rename(columns={column:z},inplace=True)\n",
    "        elif w=='Y':\n",
    "            for i in df.columns:\n",
    "                \n",
    "                c=i.replace(' ','')\n",
    "                df.rename(columns={i:c},inplace=True)\n",
    "                #df.rename(columns=lambda x: x.strip())\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_copy(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    df2=df.copy(deep=True)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df,*columns):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    cols=[]\n",
    "    for i in columns:\n",
    "        cols.append(i)\n",
    "    encode_type=input('Enter encode type')\n",
    "    if encode_type=='label':\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le=LabelEncoder()\n",
    "        for i in cols:\n",
    "            df[i]=le.fit_transform(df[[i]])\n",
    "        return df\n",
    "    elif encode_type=='ordinal':\n",
    "        for i in cols:\n",
    "\n",
    "            ord_dict={}\n",
    "            print(i)\n",
    "            n=int(input(\"enter a number of unique values present in the column: \"))\n",
    "            for j in range(n):\n",
    "                key=input('Unique value for the column')\n",
    "                value=int(input('Enter ordinal number for the value'))\n",
    "                ord_dict[key]=value\n",
    "            x=df[i].map(ord_dict)\n",
    "            df[i]=x\n",
    "        return df\n",
    "    elif encode_type=='one-hot':\n",
    "        df2=pd.get_dummies(df,columns=cols)\n",
    "        \n",
    "        return df2\n",
    "    else:\n",
    "        raise Error('Invalid encode type, select one from label,ordinal,one-hot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = LogisticRegression()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def KNN(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = KNeighborsClassifier()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def GadientBoosting(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = GradientBoostingClassifier()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def AdaBoost(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = AdaBoostClassifier()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn import svm\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        if svm_type == \"Linear\":\n",
    "            clf = svm.LinearSVC()\n",
    "        else:\n",
    "            clf = svm.SVC()\n",
    "    clf.fit(xtrain,ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    accuracy=accuracy_score(ytest, y_pred)*100\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    AUC=auc(fpr, tpr)*100\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    precision=precision_score(ytest, y_pred)*100\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    recall=recall_score(ytest,y_pred)*100\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    f1=f1_score(ytest,y_pred)*100\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    log=log_loss(ytest,y_pred)\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    class_report=classification_report(ytest,y_pred)\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "    return accuracy,AUC,precision,recall,f1,log,class_report\n",
    "\n",
    "\n",
    "def DecisionTree(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = DecisionTreeClassifier()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def RandomForest(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = RandomForestClassifier()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def NaiveBayes(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = GaussianNB()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def MultiLayerPerceptron(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = MLPClassifier(hidden_layer_sizes=5)\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def XgBoost(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = XGBClassifier()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def LightGbm(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from lightgbm import LGBMClassifier\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    \n",
    "    if not clf:\n",
    "        clf  = LGBMClassifier()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    if not clf:\n",
    "        clf  = LinearRegression()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "    print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "    print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('The RMSE is ',rmse)\n",
    "\n",
    "    r2=metrics.r2_score(ytest,y_pred)\n",
    "    n=len(xtrain)\n",
    "    k=len(x.columns)\n",
    "    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "    print('The adjusted R Squared Error is',r2_adj)\n",
    "    plt.scatter(ytest, y_pred, c = 'green') \n",
    "    plt.xlabel(\"True Value\") \n",
    "    plt.ylabel(\"Predicted value\") \n",
    "    plt.title(\"True value vs predicted value\") \n",
    "    plt.show()\n",
    "\n",
    "def RandomForestReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    if not clf:\n",
    "        clf  = RandomForestRegressor(n_estimators=100)\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "    print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "    print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('The RMSE is ',rmse)\n",
    "\n",
    "    r2=metrics.r2_score(ytest,y_pred)\n",
    "    n=len(xtrain)\n",
    "    k=len(x.columns)\n",
    "    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "    print('The adjusted R Squared Error is',r2_adj)\n",
    "    plt.scatter(ytest, y_pred, c = 'green') \n",
    "    plt.xlabel(\"True Value\") \n",
    "    plt.ylabel(\"Predicted value\") \n",
    "    plt.title(\"True value vs predicted value\") \n",
    "    plt.show()\n",
    "\n",
    "def PolynomialReg(xtrain, xtest, ytrain, ytest,degree=3, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    poly = PolynomialFeatures(degree = degree)\n",
    "    X_poly = poly.fit_transform(xtrain)\n",
    "    poly.fit(X_poly, ytrain)\n",
    "    if not clf:\n",
    "        clf = LinearRegression() \n",
    "    clf.fit(X_poly, ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "    print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "    print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('The RMSE is ',rmse)\n",
    "\n",
    "    r2=metrics.r2_score(ytest,y_pred)\n",
    "    n=len(xtrain)\n",
    "    k=len(x.columns)\n",
    "    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "    print('The adjusted R Squared Error is',r2_adj)\n",
    "    plt.scatter(ytest, y_pred, c = 'green') \n",
    "    plt.xlabel(\"True Value\") \n",
    "    plt.ylabel(\"Predicted value\") \n",
    "    plt.title(\"True value vs predicted value\") \n",
    "    plt.show()\n",
    "\n",
    "def SupportVectorRegression(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    if not clf:\n",
    "        clf  = SVR(kernel=\"rbf\")\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "    print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "    print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('The RMSE is ',rmse)\n",
    "\n",
    "    r2=metrics.r2_score(ytest,y_pred)\n",
    "    n=len(xtrain)\n",
    "    k=len(x.columns)\n",
    "    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "    print('The adjusted R Squared Error is',r2_adj)\n",
    "    plt.scatter(ytest, y_pred, c = 'green') \n",
    "    plt.xlabel(\"True Value\") \n",
    "    plt.ylabel(\"Predicted value\") \n",
    "    plt.title(\"True value vs predicted value\") \n",
    "    plt.show()\n",
    "\n",
    "def DecisionTreeReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    if not clf:\n",
    "        clf  = DecisionTreeRegressor()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "    print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "    print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('The RMSE is ',rmse)\n",
    "\n",
    "    r2=metrics.r2_score(ytest,y_pred)\n",
    "    n=len(xtrain)\n",
    "    k=len(x.columns)\n",
    "    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "    print('The adjusted R Squared Error is',r2_adj)\n",
    "    plt.scatter(ytest, y_pred, c = 'green') \n",
    "    plt.xlabel(\"True Value\") \n",
    "    plt.ylabel(\"Predicted value\") \n",
    "    plt.title(\"True value vs predicted value\") \n",
    "    plt.show()\n",
    "\n",
    "def GradientBoostingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    if not clf:\n",
    "        clf  = GradientBoostingRegressor()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "    print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "    print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('The RMSE is ',rmse)\n",
    "\n",
    "    r2=metrics.r2_score(ytest,y_pred)\n",
    "    n=len(xtrain)\n",
    "    k=len(x.columns)\n",
    "    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "    print('The adjusted R Squared Error is',r2_adj)\n",
    "    plt.scatter(ytest, y_pred, c = 'green') \n",
    "    plt.xlabel(\"True Value\") \n",
    "    plt.ylabel(\"Predicted value\") \n",
    "    plt.title(\"True value vs predicted value\") \n",
    "    plt.show()\n",
    "\n",
    "def AdaBooostReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    if not clf:\n",
    "        clf  = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "    print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "    print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('The RMSE is ',rmse)\n",
    "\n",
    "    r2=metrics.r2_score(ytest,y_pred)\n",
    "    n=len(xtrain)\n",
    "    k=len(x.columns)\n",
    "    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "    print('The adjusted R Squared Error is',r2_adj)\n",
    "    plt.scatter(ytest, y_pred, c = 'green') \n",
    "    plt.xlabel(\"True Value\") \n",
    "    plt.ylabel(\"Predicted value\") \n",
    "    plt.title(\"True value vs predicted value\") \n",
    "    plt.show()\n",
    "\n",
    "def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    lr = LinearRegression()\n",
    "    rf = RandomForestRegressor(n_estimators=100)\n",
    "    sv = SVR(kernel=\"rbf\")\n",
    "    dt = DecisionTreeRegressor()\n",
    "    gb = GradientBoostingRegressor()\n",
    "    ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "    if not clf:\n",
    "        clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "    print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "    print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('The RMSE is ',rmse)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "    r2=metrics.r2_score(ytest,y_pred)\n",
    "    n=len(xtrain)\n",
    "    k=len(x.columns)\n",
    "    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "    print('The adjusted R Squared Error is',r2_adj)\n",
    "    plt.scatter(ytest, y_pred, c = 'green') \n",
    "    plt.xlabel(\"True Value\") \n",
    "    plt.ylabel(\"Predicted value\") \n",
    "    plt.title(\"True value vs predicted value\") \n",
    "    plt.show()\n",
    "    return mse,mae,rmse,r2,r2_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_kfold(x, y, split=10, random=None, shuffle=False):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import KFold \n",
    "    kf = KFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "        ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "        yield xtrain,ytrain,xtest,ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_repeated_kf(x, y, split=10, random=None, repeat=10):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    kf = RepeatedKFold(n_splits=split, random_state=random, n_repeats=repeat)\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "        ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "        yield xtrain,ytrain,xtest,ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "    for train_index, test_index in kf.split(x, y):\n",
    "        xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "        ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "        yield xtrain,ytrain,xtest,ytest\n",
    "\n",
    "def cross_valid_strat_shuffle_kf(x, y, split=10, random=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    sss = StratifiedShuffleSplit(n_splits=split, random_state=random)\n",
    "    for train_index, test_index in sss.split(x, y):\n",
    "        xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "        ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "        yield xtrain,ytrain,xtest,ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    if regression:\n",
    "        clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "    else:\n",
    "        clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "    clf.fit(xtrain,ytrain)\n",
    "    #validationmetrics(clf,testX,testY)\n",
    "    res = pd.Series(clf.feature_importances_, index=df.columns.values).sort_values(ascending=False)*100\n",
    "    print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_plot(df,label,start,end,gap,ib=True,kernel='rbf'):\n",
    "    from tqdm import tqdm\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import model_selection\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostClassifier(),\n",
    "    #\"SVM\": svm.SVC(kernel=kernel),\n",
    "    'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'light Gradient Boosting':LGBMClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier()}\n",
    "    from sklearn.preprocessing import scale \n",
    "    for key,classifier in dict_classifiers.items():\n",
    "        X = df.drop([label], axis=1)\n",
    "        y = df[[label]]\n",
    "\n",
    "        #scale predictor variables\n",
    "        pca = PCA()\n",
    "        X_reduced = pca.fit_transform(scale(X))\n",
    "        X_reduced=pd.DataFrame(X_reduced)\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in X_reduced.columns:\n",
    "            X_reduced[i] = ss.fit_transform(X_reduced[[i]])\n",
    "        #display(X_reduced)\n",
    "        X_reduced=X_reduced.to_numpy()\n",
    "\n",
    "        #define cross validation method\n",
    "        print(key)\n",
    "\n",
    "        clf = classifier\n",
    "        acc= []\n",
    "        pre=[]\n",
    "        re=[]\n",
    "        f1=[]\n",
    "        results=pd.DataFrame(columns=['Number of components','Accuracy','Precision','Recall','F1 score']) \n",
    "        for idx,i in tqdm(enumerate(np.arange(start,end,gap))):\n",
    "            results.loc[idx,'Number of components']=i   \n",
    "            accuracy = cross_val_score(clf,X_reduced[:,:i], y, cv=10, scoring='accuracy').mean()\n",
    "            acc.append(accuracy)\n",
    "            results.loc[idx,'Accuracy']=accuracy\n",
    "            precision = cross_val_score(clf,X_reduced[:,:i], y, cv=10, scoring='precision').mean()\n",
    "            pre.append(precision)\n",
    "            results.loc[idx,'Precision']=precision\n",
    "\n",
    "            recall = cross_val_score(clf,X_reduced[:,:i], y, cv=10, scoring='recall').mean()\n",
    "            re.append(recall)\n",
    "            results.loc[idx,'Recall']=recall\n",
    "            f1score = cross_val_score(clf,X_reduced[:,:i], y, cv=10, scoring='f1').mean()\n",
    "            f1.append(f1score)\n",
    "            results.loc[idx,'F1 score']=f1score\n",
    "        display(results)\n",
    "\n",
    "        # Plot cross-validation results \n",
    "        if ib==True:\n",
    "            plt.subplots(figsize=(15, 5))\n",
    "            plt.subplot(1,3,1)\n",
    "            plt.plot(np.arange(start,end,gap),f1)\n",
    "            plt.xlim(start-1,end)\n",
    "            plt.xlabel('Number of Principal Components')\n",
    "            plt.ylabel('f1 score')\n",
    "            plt.title('target')\n",
    "        else:\n",
    "            \n",
    "            plt.subplots(figsize=(15, 5))\n",
    "            plt.subplot(1,3,1)\n",
    "            plt.plot(np.arange(start,end,gap),acc)\n",
    "            plt.xlim(start-1,end)\n",
    "            plt.xlabel('Number of Principal Components')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.title('target')\n",
    "        #plt.show()\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.plot(np.arange(start,end,gap),pre)\n",
    "        plt.xlim(start-1,end)\n",
    "        plt.xlabel('Number of Principal Components')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('target')\n",
    "        #plt.show()\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.plot(np.arange(start,end,gap),re)\n",
    "        plt.xlim(start-1,end)\n",
    "        plt.xlabel('Number of Principal Components')\n",
    "        plt.ylabel('Recall')\n",
    "        plt.title('target')\n",
    "        #plt.tight_layout(4)\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transform(df,label,n):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    x = df.drop([label], axis=1)\n",
    "    y = df[[label]]\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    ss=StandardScaler()\n",
    "    x = ss.fit_transform(x)\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca=PCA(n_components=n)\n",
    "    pca_df = pca.fit_transform(x)\n",
    "    pca_df=pd.DataFrame(pca_df)\n",
    "    pca_df[label]=y\n",
    "    return pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(df,label,n):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    x = df.drop([label], axis=1)\n",
    "    y = df[[label]]\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    ss=StandardScaler()\n",
    "    for i in x.columns:\n",
    "        x[i]=ss.fit_transform(x[[i]])\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    lda=LinearDiscriminantAnalysis(n_components=n)\n",
    "    lda.fit(x,y)\n",
    "    X=lda.transform(x)\n",
    "    X=pd.DataFrame(X)\n",
    "    X.rename(columns={0:'Feature 1'},inplace=True)\n",
    "    X[label]=y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Imbalance Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sample(df,label,ratio=0.5):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    oversample = RandomOverSampler(sampling_strategy=ratio)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "    return x_res,y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_sample(df,label,ratio=0.5):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    oversample = RandomUnderSampler(sampling_strategy=ratio)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "    return x_res,y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_plot(df,label,classification=True,over_sample=False):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    ss=StandardScaler()\n",
    "    for i in x.columns:\n",
    "        x[i] = ss.fit_transform(x[[i]])\n",
    "    if over_sample==False:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "        \n",
    "            \n",
    "    else:\n",
    "        from imblearn.over_sampling import RandomOverSampler\n",
    "        oversample = RandomOverSampler(sampling_strategy=0.5)\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "        x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "        xtrain=x_res\n",
    "        ytrain=y_res\n",
    "    if classification==True:\n",
    "\n",
    "        error=[]\n",
    "        for j in range(1,35):\n",
    "            knn=KNeighborsClassifier(n_neighbors=j)\n",
    "            knn.fit(xtrain,ytrain)\n",
    "            pred_i=knn.predict(xtest)\n",
    "            error.append(np.mean(pred_i!=ytest))\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(range(1,35),error,color='red',marker='o',markersize=10)\n",
    "        plt.title('Error rate K value')\n",
    "        plt.xlabel('K value')\n",
    "        plt.ylabel('Mean Error')\n",
    "    else:\n",
    "        error=[]\n",
    "        for j in range(1,35):\n",
    "            knn=KNeighborsRegressor(n_neighbors=j)\n",
    "            knn.fit(xtrain,ytrain)\n",
    "            pred_i=knn.predict(xtest)\n",
    "            error.append(np.mean(pred_i!=ytest))\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(range(1,35),error,color='red',marker='o',markersize=10)\n",
    "        plt.title('Error rate K value')\n",
    "        plt.xlabel('K value')\n",
    "        plt.ylabel('Mean Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithms without FS,CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(df,label,classification=True,min_max=False,svm_type = \"Linear\",kernel='poly'):\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "        print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "        print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "        print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "        print('The adjusted R Squared Error is',r2_adj)\n",
    "        plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC(probability=True,kernel=kernel)\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "        AUC=metrics.auc(fpr, tpr)*100\n",
    "        print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        print(classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    #\"Decision Tree\": DecisionTreeClassifier(),\n",
    "    #\"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    #\"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    #'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    #'light Gradient Boosting':LGBMClassifier(),\n",
    "    #\"Gradient Boosting Classifier\": GradientBoostingClassifier()\n",
    "    }\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                clf.fit(xtrain,ytrain)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                \n",
    "\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "               # fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                \n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                if svm_type == \"Linear\":\n",
    "                    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "                    auc_score=auc(fpr, tpr)*100\n",
    "                    df_results.loc[count,'AUC']= auc_score\n",
    "                else:\n",
    "                    \n",
    "                    ypred_prob=clf.predict_proba(xtest)[:,1]\n",
    "                    fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                    random_probs = [0 for i in range(len(ytest))]\n",
    "                    p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                    auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                    df_results.loc[count,'AUC']= auc_score\n",
    "                    print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                    ix = np.argmax(gmeans)\n",
    "                    print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                if svm_type == \"Linear\":\n",
    "                    pass\n",
    "                else:\n",
    "                    \n",
    "                    plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                    plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                    plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                    # title\n",
    "                    plt.title('ROC curve')\n",
    "                    # x label\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    # y label\n",
    "                    plt.ylabel('True Positive rate')\n",
    "\n",
    "                    plt.legend(loc='best')\n",
    "\n",
    "                    plt.show()\n",
    "            else:\n",
    "                classifier.fit(xtrain,ytrain)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                #fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                #print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "               # df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                ypred_prob=classifier.predict_proba(xtest)[:,1]\n",
    "                fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                random_probs = [0 for i in range(len(ytest))]\n",
    "                p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                df_results.loc[count,'AUC']= auc_score\n",
    "                print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                ix = np.argmax(gmeans)\n",
    "                print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                # title\n",
    "                plt.title('ROC curve')\n",
    "                # x label\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                # y label\n",
    "                plt.ylabel('True Positive rate')\n",
    "\n",
    "                plt.legend(loc='best')\n",
    "\n",
    "                plt.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        #df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_without_CV_FS.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                mse,mae,rmse,r2,r2_adj=VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            else:\n",
    "                \n",
    "                regressor.fit(xtrain,ytrain)\n",
    "                y_pred=regressor.predict(xtest)\n",
    "                print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "                print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "                print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "                rmse = math.sqrt(mse)\n",
    "                r2=metrics.r2_score(ytest,y_pred)\n",
    "                rmse = math.sqrt(mse)\n",
    "                mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                n=len(xtrain)\n",
    "                k=len(x.columns)\n",
    "                r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                print('The adjusted R Squared Error is',r2_adj)\n",
    "                plt.scatter(ytest, y_pred, c = 'green') \n",
    "                plt.xlabel(\"True Value\") \n",
    "                plt.ylabel(\"Predicted value\") \n",
    "                plt.title(\"True value vs predicted value\") \n",
    "                plt.show()\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithm with FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_RFS(df,label,threshold=5,classification=True,min_max=False,svm_type='Linear',kernel='poly'):\n",
    "    def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        if regression:\n",
    "            clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "        else:\n",
    "            clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        #validationmetrics(clf,testX,testY)\n",
    "        res = pd.Series(clf.feature_importances_, index=xtrain.columns.values).sort_values(ascending=False)*100\n",
    "        print(res)\n",
    "        return res\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "        print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "        print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "        print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "        print('The adjusted R Squared Error is',r2_adj)\n",
    "        plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        print(classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        print('false positive rate',fpr)\n",
    "        print('true positive rate',tpr)\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    #\"Decision Tree\": DecisionTreeClassifier(),\n",
    "    #\"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    #\"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    #'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    #'light Gradient Boosting':LGBMClassifier(),\n",
    "    #\"Gradient Boosting Classifier\": GradientBoostingClassifier()\n",
    "    }\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    if classification==True:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    else:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=True)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x2,y,test_size=0.20,random_state=42 )\n",
    "    \n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                clf.fit(xtrain,ytrain)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "               # fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                \n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                if svm_type == \"Linear\":\n",
    "                    print('AUC score cannot be calculated for linear SVM')\n",
    "                else:\n",
    "                    \n",
    "                    ypred_prob=clf.predict_proba(xtest)[:,1]\n",
    "                    fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                    random_probs = [0 for i in range(len(ytest))]\n",
    "                    p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                    auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                    df_results.loc[count,'AUC']= auc_score\n",
    "                    print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                    ix = np.argmax(gmeans)\n",
    "                    print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                if svm_type == \"Linear\":\n",
    "                    pass\n",
    "                else:\n",
    "                    \n",
    "                    plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                    plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                    plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                    # title\n",
    "                    plt.title('ROC curve')\n",
    "                    # x label\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    # y label\n",
    "                    plt.ylabel('True Positive rate')\n",
    "\n",
    "                    plt.legend(loc='best')\n",
    "\n",
    "                    plt.show()\n",
    "            else:\n",
    "                classifier.fit(xtrain,ytrain)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                #fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                #print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "               # df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                ypred_prob=classifier.predict_proba(xtest)[:,1]\n",
    "                fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                random_probs = [0 for i in range(len(ytest))]\n",
    "                p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                df_results.loc[count,'AUC']= auc_score\n",
    "                print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                ix = np.argmax(gmeans)\n",
    "                print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                # title\n",
    "                plt.title('ROC curve')\n",
    "                # x label\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                # y label\n",
    "                plt.ylabel('True Positive rate')\n",
    "\n",
    "                plt.legend(loc='best')\n",
    "\n",
    "                plt.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        #df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_FS.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                mse,mae,rmse,r2,r2_adj=VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            else:\n",
    "                \n",
    "                regressor.fit(xtrain,ytrain)\n",
    "                y_pred=regressor.predict(xtest)\n",
    "                print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "                print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "                print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "                rmse = math.sqrt(mse)\n",
    "                r2=metrics.r2_score(ytest,y_pred)\n",
    "                rmse = math.sqrt(mse)\n",
    "                mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                n=len(xtrain)\n",
    "                k=len(x.columns)\n",
    "                r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                print('The adjusted R Squared Error is',r2_adj)\n",
    "                plt.scatter(ytest, y_pred, c = 'green') \n",
    "                plt.xlabel(\"True Value\") \n",
    "                plt.ylabel(\"Predicted value\") \n",
    "                plt.title(\"True value vs predicted value\") \n",
    "                plt.show()\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithm with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_CV(df,label,split=10,classification=True,min_max=False,svm_type='Linear',kernel='poly'):\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "        print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "        print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "        print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "        print('The adjusted R Squared Error is',r2_adj)\n",
    "        plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC(kernel='poly')\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        print(classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    \n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    #\"Decision Tree\": DecisionTreeClassifier(),\n",
    "    #\"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    #\"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    #'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    #'light Gradient Boosting':LGBMClassifier(),\n",
    "    #\"Gradient Boosting Classifier\": GradientBoostingClassifier()\n",
    "    }\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xr , ytrain, yr =train_test_split(x,y,test_size=0.50,random_state=42 )\n",
    "    \n",
    "    xcv , xtest , ycv, ytest =train_test_split(xr,yr,test_size=0.40,random_state=42 )\n",
    "    print('_____Cross Validation_____')\n",
    "    if classification==True:\n",
    "        #df_results = pd.DataFrame(data=np.zeros(shape=(11,6)), columns = ['classifier','Accuracy','Precision','Recall','f1score','log_loss'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                accuracy=cross_val_score(clf, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(clf, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(clf, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(clf, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(clf, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "            else:\n",
    "                accuracy=cross_val_score(classifier, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(classifier, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(classifier, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(classifier, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(classifier, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                \n",
    "            count+=1\n",
    "        #return df_results\n",
    "    print('______Test_______')\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                clf.fit(xtrain,ytrain)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "               # fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                \n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                if svm_type == \"Linear\":\n",
    "                    print('AUC score cannot be calculated for linear SVM')\n",
    "                else:\n",
    "                    \n",
    "                    ypred_prob=clf.predict_proba(xtest)[:,1]\n",
    "                    fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                    random_probs = [0 for i in range(len(ytest))]\n",
    "                    p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                    auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                    df_results.loc[count,'AUC']= auc_score\n",
    "                    print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                    ix = np.argmax(gmeans)\n",
    "                    print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                if svm_type == \"Linear\":\n",
    "                    pass\n",
    "                else:\n",
    "                    \n",
    "                    plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                    plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                    plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                    # title\n",
    "                    plt.title('ROC curve')\n",
    "                    # x label\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    # y label\n",
    "                    plt.ylabel('True Positive rate')\n",
    "\n",
    "                    plt.legend(loc='best')\n",
    "\n",
    "                    plt.show()\n",
    "            else:\n",
    "                classifier.fit(xtrain,ytrain)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                #fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                #print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "               # df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                ypred_prob=classifier.predict_proba(xtest)[:,1]\n",
    "                fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                random_probs = [0 for i in range(len(ytest))]\n",
    "                p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                df_results.loc[count,'AUC']= auc_score\n",
    "                print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                ix = np.argmax(gmeans)\n",
    "                print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                # title\n",
    "                plt.title('ROC curve')\n",
    "                # x label\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                # y label\n",
    "                plt.ylabel('True Positive rate')\n",
    "\n",
    "                plt.legend(loc='best')\n",
    "\n",
    "                plt.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        #df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_CV.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                mse,mae,rmse,r2,r2_adj=VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            else:\n",
    "                \n",
    "                regressor.fit(xtrain,ytrain)\n",
    "                y_pred=regressor.predict(xtest)\n",
    "                print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "                print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "                print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "                rmse = math.sqrt(mse)\n",
    "                r2=metrics.r2_score(ytest,y_pred)\n",
    "                rmse = math.sqrt(mse)\n",
    "                mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                n=len(xtrain)\n",
    "                k=len(x.columns)\n",
    "                r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                print('The adjusted R Squared Error is',r2_adj)\n",
    "                plt.scatter(ytest, y_pred, c = 'green') \n",
    "                plt.xlabel(\"True Value\") \n",
    "                plt.ylabel(\"Predicted value\") \n",
    "                plt.title(\"True value vs predicted value\") \n",
    "                plt.show()\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithm with CV and FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "def fit_model_CV_FS(df,label,split=10,classification=True,cross_valid_method=cross_valid_stratified_kf,threshold=5,min_max=False,svm_type='Linear',kernel='poly'):\n",
    "    def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "    def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        if regression:\n",
    "            clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "        else:\n",
    "            clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        #validationmetrics(clf,testX,testY)\n",
    "        res = pd.Series(clf.feature_importances_, index=xtrain.columns.values).sort_values(ascending=False)*100\n",
    "        print(res)\n",
    "        return res\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "       # print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "       # print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "       # print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "       # print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "       # print('The adjusted R Squared Error is',r2_adj)\n",
    "        '''plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()'''\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        #print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        #print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        #print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        #print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        #print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        #print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        '''visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()'''\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report,fpr,tpr\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    #\"Decision Tree\": DecisionTreeClassifier(),\n",
    "    #\"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    #\"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    #'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    #'light Gradient Boosting':LGBMClassifier(),\n",
    "    #\"Gradient Boosting Classifier\": GradientBoostingClassifier()\n",
    "    }\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    if classification==True:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    else:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=True)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    xtrain , xr , ytrain, yr =train_test_split(x2,y,test_size=0.50,random_state=42 )\n",
    "    \n",
    "    xcv , xtest , ycv, ytest =train_test_split(xr,yr,test_size=0.40,random_state=42 )\n",
    "    print('_____Cross Validation_____')\n",
    "    if classification==True:\n",
    "        #df_results = pd.DataFrame(data=np.zeros(shape=(11,6)), columns = ['classifier','Accuracy','Precision','Recall','f1score','log_loss'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                accuracy=cross_val_score(clf, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(clf, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(clf, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(clf, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(clf, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "            else:\n",
    "                accuracy=cross_val_score(classifier, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(classifier, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(classifier, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(classifier, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(classifier, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                \n",
    "            count+=1\n",
    "        #return df_results\n",
    "    print('______Test_______')\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                clf.fit(xtrain,ytrain)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "               # fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                \n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                if svm_type == \"Linear\":\n",
    "                    print('AUC score cannot be calculated for linear SVM')\n",
    "                else:\n",
    "                    \n",
    "                    ypred_prob=clf.predict_proba(xtest)[:,1]\n",
    "                    fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                    random_probs = [0 for i in range(len(ytest))]\n",
    "                    p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                    auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                    df_results.loc[count,'AUC']= auc_score\n",
    "                    print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                    ix = np.argmax(gmeans)\n",
    "                    print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                if svm_type == \"Linear\":\n",
    "                    pass\n",
    "                else:\n",
    "                    \n",
    "                    plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                    plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                    plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                    # title\n",
    "                    plt.title('ROC curve')\n",
    "                    # x label\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    # y label\n",
    "                    plt.ylabel('True Positive rate')\n",
    "\n",
    "                    plt.legend(loc='best')\n",
    "\n",
    "                    plt.show()\n",
    "            else:\n",
    "                classifier.fit(xtrain,ytrain)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                #fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                #print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "               # df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                ypred_prob=classifier.predict_proba(xtest)[:,1]\n",
    "                fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                random_probs = [0 for i in range(len(ytest))]\n",
    "                p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                df_results.loc[count,'AUC']= auc_score\n",
    "                print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                ix = np.argmax(gmeans)\n",
    "                print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                # title\n",
    "                plt.title('ROC curve')\n",
    "                # x label\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                # y label\n",
    "                plt.ylabel('True Positive rate')\n",
    "\n",
    "                plt.legend(loc='best')\n",
    "\n",
    "                plt.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        #df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_CV_FS.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain , ytrain,xtest, ytest in cross_valid_method(x2,y,split=split):\n",
    "                    mse,mae,rmse,r2,r2_adj=VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None)\n",
    "                    s.append(mse)\n",
    "                    a.append(mae)\n",
    "                    rm.append(rmse)\n",
    "                    r.append(r2)\n",
    "                    ra.append(r2_adj)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "            else:\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain , ytrain,xtest, ytest in cross_valid_method(x2,y,split=split):\n",
    "                    \n",
    "                    regressor.fit(xtrain,ytrain)\n",
    "                    y_pred=regressor.predict(xtest)\n",
    "                    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                    s.append(mse)\n",
    "                    mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                    a,append(mae)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    rm.append(rmse)\n",
    "                    r2=metrics.r2_score(ytest,y_pred)\n",
    "                    r.append(r2)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    \n",
    "                    n=len(xtrain)\n",
    "                    k=len(x.columns)\n",
    "                    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                    ra.append(r2_adj)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                \n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithms with oversampling without FS,CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_os(df,label,ratio=0.5,classification=True,min_max=False,svm_type='Linear',kernel='poly'):\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "        print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "        print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "        print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "        print('The adjusted R Squared Error is',r2_adj)\n",
    "        plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        print(classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    \n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    #\"Decision Tree\": DecisionTreeClassifier(),\n",
    "    #\"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    #\"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    #'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    #'light Gradient Boosting':LGBMClassifier(),\n",
    "    #\"Gradient Boosting Classifier\": GradientBoostingClassifier()\n",
    "    }\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    oversample = RandomOverSampler(sampling_strategy=ratio)\n",
    "    x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                clf.fit(x_res,y_res)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "               # fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                \n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                if svm_type == \"Linear\":\n",
    "                    print('AUC score cannot be calculated for linear SVM')\n",
    "                else:\n",
    "                    \n",
    "                    ypred_prob=clf.predict_proba(xtest)[:,1]\n",
    "                    fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                    random_probs = [0 for i in range(len(ytest))]\n",
    "                    p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                    auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                    df_results.loc[count,'AUC']= auc_score\n",
    "                    print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                    ix = np.argmax(gmeans)\n",
    "                    print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(x_res, y_res)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                if svm_type == \"Linear\":\n",
    "                    pass\n",
    "                else:\n",
    "                    \n",
    "                    plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                    plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                    plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                    # title\n",
    "                    plt.title('ROC curve')\n",
    "                    # x label\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    # y label\n",
    "                    plt.ylabel('True Positive rate')\n",
    "\n",
    "                    plt.legend(loc='best')\n",
    "\n",
    "                    plt.show()\n",
    "            else:\n",
    "                classifier.fit(x_res,y_res)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                #fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                #print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "               # df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                ypred_prob=classifier.predict_proba(xtest)[:,1]\n",
    "                fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                random_probs = [0 for i in range(len(ytest))]\n",
    "                p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                df_results.loc[count,'AUC']= auc_score\n",
    "                print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                ix = np.argmax(gmeans)\n",
    "                print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(x_res,y_res)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                # title\n",
    "                plt.title('ROC curve')\n",
    "                # x label\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                # y label\n",
    "                plt.ylabel('True Positive rate')\n",
    "\n",
    "                plt.legend(loc='best')\n",
    "\n",
    "                plt.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        #df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_OS.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                mse,mae,rmse,r2,r2_adj=VotingReg(x_res, xtest, y_res, ytest, verbose=True, clf=None)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            else:\n",
    "                \n",
    "                regressor.fit(x_res,y_res)\n",
    "                y_pred=regressor.predict(xtest)\n",
    "                print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "                print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "                print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "                rmse = math.sqrt(mse)\n",
    "                r2=metrics.r2_score(ytest,y_pred)\n",
    "                rmse = math.sqrt(mse)\n",
    "                mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                n=len(xtrain)\n",
    "                k=len(x.columns)\n",
    "                r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                print('The adjusted R Squared Error is',r2_adj)\n",
    "                plt.scatter(ytest, y_pred, c = 'green') \n",
    "                plt.xlabel(\"True Value\") \n",
    "                plt.ylabel(\"Predicted value\") \n",
    "                plt.title(\"True value vs predicted value\") \n",
    "                plt.show()\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithms with undersampling without FS,CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_us(df,label,ratio=0.5,classification=True,min_max=False,svm_type='Linear',kernel='poly'):\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "        print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "        print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "        print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "        print('The adjusted R Squared Error is',r2_adj)\n",
    "        plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        print(classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    #\"Decision Tree\": DecisionTreeClassifier(),\n",
    "    #\"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    #\"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    #'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    #'light Gradient Boosting':LGBMClassifier(),\n",
    "    #\"Gradient Boosting Classifier\": GradientBoostingClassifier()\n",
    "    }\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    undersample = RandomUnderSampler(sampling_strategy=ratio)\n",
    "    x_res, y_res = undersample.fit_resample(xtrain, ytrain)\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                clf.fit(x_res,y_res)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "               # fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                \n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                if svm_type == \"Linear\":\n",
    "                    print('AUC score cannot be calculated for linear SVM')\n",
    "                else:\n",
    "                    \n",
    "                    ypred_prob=clf.predict_proba(xtest)[:,1]\n",
    "                    fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                    random_probs = [0 for i in range(len(ytest))]\n",
    "                    p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                    auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                    df_results.loc[count,'AUC']= auc_score\n",
    "                    print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                    ix = np.argmax(gmeans)\n",
    "                    print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(x_res, y_res)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                if svm_type == \"Linear\":\n",
    "                    pass\n",
    "                else:\n",
    "                    \n",
    "                    plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                    plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                    plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                    # title\n",
    "                    plt.title('ROC curve')\n",
    "                    # x label\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    # y label\n",
    "                    plt.ylabel('True Positive rate')\n",
    "\n",
    "                    plt.legend(loc='best')\n",
    "\n",
    "                    plt.show()\n",
    "            else:\n",
    "                classifier.fit(x_res,y_res)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                #fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                #print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "               # df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                ypred_prob=classifier.predict_proba(xtest)[:,1]\n",
    "                fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                random_probs = [0 for i in range(len(ytest))]\n",
    "                p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                df_results.loc[count,'AUC']= auc_score\n",
    "                print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                ix = np.argmax(gmeans)\n",
    "                print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(x_res, y_res)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                # title\n",
    "                plt.title('ROC curve')\n",
    "                # x label\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                # y label\n",
    "                plt.ylabel('True Positive rate')\n",
    "\n",
    "                plt.legend(loc='best')\n",
    "\n",
    "                plt.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        #df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_US.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                mse,mae,rmse,r2,r2_adj=VotingReg(x_res, xtest, y_res, ytest, verbose=True, clf=None)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            else:\n",
    "                \n",
    "                regressor.fit(x_res,y_res)\n",
    "                y_pred=regressor.predict(xtest)\n",
    "                print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "                print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "                print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "                rmse = math.sqrt(mse)\n",
    "                r2=metrics.r2_score(ytest,y_pred)\n",
    "                rmse = math.sqrt(mse)\n",
    "                mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                n=len(xtrain)\n",
    "                k=len(x.columns)\n",
    "                r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                print('The adjusted R Squared Error is',r2_adj)\n",
    "                plt.scatter(ytest, y_pred, c = 'green') \n",
    "                plt.xlabel(\"True Value\") \n",
    "                plt.ylabel(\"Predicted value\") \n",
    "                plt.title(\"True value vs predicted value\") \n",
    "                plt.show()\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithm with oversampling and FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_RFS_os(df,label,ratio=0.5,threshold=5,classification=True,min_max=False,svm_type='Linear',kernel='poly'):\n",
    "    def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        if regression:\n",
    "            clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "        else:\n",
    "            clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        #validationmetrics(clf,testX,testY)\n",
    "        res = pd.Series(clf.feature_importances_, index=xtrain.columns.values).sort_values(ascending=False)*100\n",
    "        print(res)\n",
    "        return res\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "        print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "        print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "        print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "        print('The adjusted R Squared Error is',r2_adj)\n",
    "        plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        print(classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    #\"Decision Tree\": DecisionTreeClassifier(),\n",
    "    #\"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    #\"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    #'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    #'light Gradient Boosting':LGBMClassifier(),\n",
    "    #\"Gradient Boosting Classifier\": GradientBoostingClassifier()\n",
    "    }\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    \n",
    "    if classification==True:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    else:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=True)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x2,y,test_size=0.20,random_state=42 )\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    oversample = RandomOverSampler(sampling_strategy=ratio)\n",
    "    x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                clf.fit(x_res,y_res)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "               # fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                \n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                if svm_type == \"Linear\":\n",
    "                    print('AUC score cannot be calculated for linear SVM')\n",
    "                else:\n",
    "                    \n",
    "                    ypred_prob=clf.predict_proba(xtest)[:,1]\n",
    "                    fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                    random_probs = [0 for i in range(len(ytest))]\n",
    "                    p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                    auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                    df_results.loc[count,'AUC']= auc_score\n",
    "                    print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                    ix = np.argmax(gmeans)\n",
    "                    print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(x_res, y_res)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                if svm_type == \"Linear\":\n",
    "                    pass\n",
    "                else:\n",
    "                    \n",
    "                    plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                    plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                    plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                    # title\n",
    "                    plt.title('ROC curve')\n",
    "                    # x label\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    # y label\n",
    "                    plt.ylabel('True Positive rate')\n",
    "\n",
    "                    plt.legend(loc='best')\n",
    "\n",
    "                    plt.show()\n",
    "            else:\n",
    "                classifier.fit(x_res,y_res)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                #fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                #print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "               # df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                ypred_prob=classifier.predict_proba(xtest)[:,1]\n",
    "                fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                random_probs = [0 for i in range(len(ytest))]\n",
    "                p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                df_results.loc[count,'AUC']= auc_score\n",
    "                print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                ix = np.argmax(gmeans)\n",
    "                print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(x_res, y_res)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                # title\n",
    "                plt.title('ROC curve')\n",
    "                # x label\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                # y label\n",
    "                plt.ylabel('True Positive rate')\n",
    "\n",
    "                plt.legend(loc='best')\n",
    "\n",
    "                plt.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        #df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_RFS_OS.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                mse,mae,rmse,r2,r2_adj=VotingReg(x_res, xtest, y_res, ytest, verbose=True, clf=None)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            else:\n",
    "                \n",
    "                regressor.fit(x_res,y_res)\n",
    "                y_pred=regressor.predict(xtest)\n",
    "                print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "                print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "                print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "                rmse = math.sqrt(mse)\n",
    "                r2=metrics.r2_score(ytest,y_pred)\n",
    "                rmse = math.sqrt(mse)\n",
    "                mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                n=len(x_res)\n",
    "                k=len(x.columns)\n",
    "                r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                print('The adjusted R Squared Error is',r2_adj)\n",
    "                plt.scatter(ytest, y_pred, c = 'green') \n",
    "                plt.xlabel(\"True Value\") \n",
    "                plt.ylabel(\"Predicted value\") \n",
    "                plt.title(\"True value vs predicted value\") \n",
    "                plt.show()\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithm with undersampling and FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_RFS_us(df,label,ratio=0.5,threshold=5,classification=True,min_max=False,svm_type='Linear',kernel='poly'):\n",
    "    def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        if regression:\n",
    "            clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "        else:\n",
    "            clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        #validationmetrics(clf,testX,testY)\n",
    "        res = pd.Series(clf.feature_importances_, index=xtrain.columns.values).sort_values(ascending=False)*100\n",
    "        print(res)\n",
    "        return res\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "        print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "        print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "        print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "        print('The adjusted R Squared Error is',r2_adj)\n",
    "        plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        print(classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    #\"Decision Tree\": DecisionTreeClassifier(),\n",
    "    #\"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    #\"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    #'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    #'light Gradient Boosting':LGBMClassifier(),\n",
    "    #\"Gradient Boosting Classifier\": GradientBoostingClassifier()\n",
    "    }\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    \n",
    "    if classification==True:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    else:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=True)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x2,y,test_size=0.20,random_state=42 )\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    undersample = RandomUnderSampler(sampling_strategy=ratio)\n",
    "    x_res, y_res = undersample.fit_resample(xtrain, ytrain)\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                clf.fit(x_res,y_res)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "               # fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                \n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                if svm_type == \"Linear\":\n",
    "                    print('AUC score cannot be calculated for linear SVM')\n",
    "                else:\n",
    "                    \n",
    "                    ypred_prob=clf.predict_proba(xtest)[:,1]\n",
    "                    fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                    random_probs = [0 for i in range(len(ytest))]\n",
    "                    p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                    auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                    df_results.loc[count,'AUC']= auc_score\n",
    "                    print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                    ix = np.argmax(gmeans)\n",
    "                    print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(x_res, y_res)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                if svm_type == \"Linear\":\n",
    "                    pass\n",
    "                else:\n",
    "                    \n",
    "                    plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                    plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                    plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                    # title\n",
    "                    plt.title('ROC curve')\n",
    "                    # x label\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    # y label\n",
    "                    plt.ylabel('True Positive rate')\n",
    "\n",
    "                    plt.legend(loc='best')\n",
    "\n",
    "                    plt.show()\n",
    "            else:\n",
    "                classifier.fit(x_res,y_res)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                #fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                #print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "               # df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                ypred_prob=classifier.predict_proba(xtest)[:,1]\n",
    "                fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                random_probs = [0 for i in range(len(ytest))]\n",
    "                p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                df_results.loc[count,'AUC']= auc_score\n",
    "                print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                ix = np.argmax(gmeans)\n",
    "                print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(x_res, y_res)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                # title\n",
    "                plt.title('ROC curve')\n",
    "                # x label\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                # y label\n",
    "                plt.ylabel('True Positive rate')\n",
    "\n",
    "                plt.legend(loc='best')\n",
    "\n",
    "                plt.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        #df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_RFS_US.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                mse,mae,rmse,r2,r2_adj=VotingReg(x_res, xtest, y_res, ytest, verbose=True, clf=None)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            else:\n",
    "                \n",
    "                regressor.fit(x_res,y_res)\n",
    "                y_pred=regressor.predict(xtest)\n",
    "                print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "                print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "                print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "                rmse = math.sqrt(mse)\n",
    "                r2=metrics.r2_score(ytest,y_pred)\n",
    "                rmse = math.sqrt(mse)\n",
    "                mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                n=len(x_res)\n",
    "                k=len(x.columns)\n",
    "                r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                print('The adjusted R Squared Error is',r2_adj)\n",
    "                plt.scatter(ytest, y_pred, c = 'green') \n",
    "                plt.xlabel(\"True Value\") \n",
    "                plt.ylabel(\"Predicted value\") \n",
    "                plt.title(\"True value vs predicted value\") \n",
    "                plt.show()\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ml algorithm with oversampling and CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_stratified_kf(x, y,ratio=0.5, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "def fit_model_CV_OS(df,label,ratio=0.5,split=10,classification=True,cross_valid_method=cross_valid_stratified_kf,min_max=False,svm_type='Linear',kernel='poly'):\n",
    "    def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "    def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        if regression:\n",
    "            clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "        else:\n",
    "            clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        #validationmetrics(clf,testX,testY)\n",
    "        res = pd.Series(clf.feature_importances_, index=xtrain.columns.values).sort_values(ascending=False)*100\n",
    "        print(res)\n",
    "        return res\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "       # print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "       # print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "       # print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "       # print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "       # print('The adjusted R Squared Error is',r2_adj)\n",
    "        '''plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()'''\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        #print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        #print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        #print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        #print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        #print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        #print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        '''visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()'''\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report,fpr,tpr\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    #\"Decision Tree\": DecisionTreeClassifier(),\n",
    "    #\"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    #\"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    #'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    #'light Gradient Boosting':LGBMClassifier(),\n",
    "    #\"Gradient Boosting Classifier\": GradientBoostingClassifier()\n",
    "    }\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    #xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "   # for xtrain , xtest , ytrain, ytest in cross_valid_method(x,y,split=split):\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    oversample = RandomOverSampler(sampling_strategy=ratio)\n",
    "    x_res, y_res = oversample.fit_resample(xtrain, ytrain)    \n",
    "    \n",
    "    xtrain , xr , ytrain, yr =train_test_split(x_res,y_res,test_size=0.50,random_state=42 )\n",
    "    \n",
    "    xcv , x_test , ycv, y_test =train_test_split(xr,yr,test_size=0.40,random_state=42 )\n",
    "    print('_____Cross Validation_____')\n",
    "    if classification==True:\n",
    "        #df_results = pd.DataFrame(data=np.zeros(shape=(11,6)), columns = ['classifier','Accuracy','Precision','Recall','f1score','log_loss'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                accuracy=cross_val_score(clf, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(clf, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(clf, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(clf, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(clf, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "            else:\n",
    "                accuracy=cross_val_score(classifier, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(classifier, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(classifier, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(classifier, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(classifier, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                \n",
    "            count+=1\n",
    "        #return df_results\n",
    "    print('______Test_______')\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                clf.fit(xtrain,ytrain)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "               # fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                \n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                if svm_type == \"Linear\":\n",
    "                    print('AUC score cannot be calculated for linear SVM')\n",
    "                else:\n",
    "                    \n",
    "                    ypred_prob=clf.predict_proba(xtest)[:,1]\n",
    "                    fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                    random_probs = [0 for i in range(len(ytest))]\n",
    "                    p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                    auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                    df_results.loc[count,'AUC']= auc_score\n",
    "                    print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                    ix = np.argmax(gmeans)\n",
    "                    print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                if svm_type == \"Linear\":\n",
    "                    pass\n",
    "                else:\n",
    "                    \n",
    "                    plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                    plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                    plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                    # title\n",
    "                    plt.title('ROC curve')\n",
    "                    # x label\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    # y label\n",
    "                    plt.ylabel('True Positive rate')\n",
    "\n",
    "                    plt.legend(loc='best')\n",
    "\n",
    "                    plt.show()\n",
    "            else:\n",
    "                classifier.fit(xtrain,ytrain)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                #fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                #print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "               # df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                ypred_prob=classifier.predict_proba(xtest)[:,1]\n",
    "                fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                random_probs = [0 for i in range(len(ytest))]\n",
    "                p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                df_results.loc[count,'AUC']= auc_score\n",
    "                print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                ix = np.argmax(gmeans)\n",
    "                print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                # title\n",
    "                plt.title('ROC curve')\n",
    "                # x label\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                # y label\n",
    "                plt.ylabel('True Positive rate')\n",
    "\n",
    "                plt.legend(loc='best')\n",
    "\n",
    "                plt.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        #df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_CV_OS.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain  , ytrain,xtest, ytest in cross_valid_method(x,y,split=split):\n",
    "                    from imblearn.over_sampling import RandomOverSampler\n",
    "                    oversample = RandomOverSampler(sampling_strategy=ratio)\n",
    "                    x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "                    mse,mae,rmse,r2,r2_adj=VotingReg(x_res, xtest, y_res, ytest, verbose=True, clf=None)\n",
    "                    s.append(mse)\n",
    "                    a.append(mae)\n",
    "                    rm.append(rmse)\n",
    "                    r.append(r2)\n",
    "                    ra.append(r2_adj)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "            else:\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain , ytrain,xtest, ytest in cross_valid_method(x,y,split=split):\n",
    "                    from imblearn.over_sampling import RandomOverSampler\n",
    "                    oversample = RandomOverSampler(sampling_strategy=ratio)\n",
    "                    x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "                    regressor.fit(x_res,y_res)\n",
    "                    y_pred=regressor.predict(xtest)\n",
    "                    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                    s.append(mse)\n",
    "                    mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                    a,append(mae)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    rm.append(rmse)\n",
    "                    r2=metrics.r2_score(ytest,y_pred)\n",
    "                    r.append(r2)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    \n",
    "                    n=len(xtrain)\n",
    "                    k=len(x.columns)\n",
    "                    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                    ra.append(r2_adj)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                \n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithm with undersampling and CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_stratified_kf(x, y,ratio=0.5, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "def fit_model_CV_US(df,label,ratio=0.5,split=10,classification=True,cross_valid_method=cross_valid_stratified_kf,min_max=False,svm_type='Linear',kernel='poly'):\n",
    "    def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "    def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        if regression:\n",
    "            clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "        else:\n",
    "            clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        #validationmetrics(clf,testX,testY)\n",
    "        res = pd.Series(clf.feature_importances_, index=xtrain.columns.values).sort_values(ascending=False)*100\n",
    "        print(res)\n",
    "        return res\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "       # print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "       # print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "       # print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "       # print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "       # print('The adjusted R Squared Error is',r2_adj)\n",
    "        '''plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()'''\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        #print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        #print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        #print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        #print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        #print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        #print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        '''visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()'''\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report,fpr,tpr\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    #\"Decision Tree\": DecisionTreeClassifier(),\n",
    "    #\"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    #\"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    #'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    #'light Gradient Boosting':LGBMClassifier(),\n",
    "    #\"Gradient Boosting Classifier\": GradientBoostingClassifier()\n",
    "    }\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    #from sklearn.model_selection import train_test_split\n",
    "    #xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "   # for xtrain , xtest , ytrain, ytest in cross_valid_method(x,y,split=split):\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    undersample = RandomUnderSampler(sampling_strategy=ratio)\n",
    "    x_res, y_res = undersample.fit_resample(xtrain, ytrain)   \n",
    "    \n",
    "    xtrain , xr , ytrain, yr =train_test_split(x_res,y_res,test_size=0.50,random_state=42 )\n",
    "    \n",
    "    xcv , x_test , ycv, y_test =train_test_split(xr,yr,test_size=0.40,random_state=42 )\n",
    "    print('_____Cross Validation_____')\n",
    "    if classification==True:\n",
    "        #df_results = pd.DataFrame(data=np.zeros(shape=(11,6)), columns = ['classifier','Accuracy','Precision','Recall','f1score','log_loss'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                accuracy=cross_val_score(clf, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(clf, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(clf, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(clf, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(clf, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "            else:\n",
    "                accuracy=cross_val_score(classifier, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(classifier, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(classifier, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(classifier, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(classifier, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                \n",
    "            count+=1\n",
    "        #return df_results\n",
    "    print('______Test_______')\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                clf.fit(xtrain,ytrain)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "               # fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                \n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                if svm_type == \"Linear\":\n",
    "                    print('AUC score cannot be calculated for linear SVM')\n",
    "                else:\n",
    "                    \n",
    "                    ypred_prob=clf.predict_proba(xtest)[:,1]\n",
    "                    fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                    random_probs = [0 for i in range(len(ytest))]\n",
    "                    p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                    auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                    df_results.loc[count,'AUC']= auc_score\n",
    "                    print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                    ix = np.argmax(gmeans)\n",
    "                    print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                if svm_type == \"Linear\":\n",
    "                    pass\n",
    "                else:\n",
    "                    \n",
    "                    plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                    plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                    plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                    # title\n",
    "                    plt.title('ROC curve')\n",
    "                    # x label\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    # y label\n",
    "                    plt.ylabel('True Positive rate')\n",
    "\n",
    "                    plt.legend(loc='best')\n",
    "\n",
    "                    plt.show()\n",
    "            else:\n",
    "                classifier.fit(xtrain,ytrain)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                #fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                #print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "               # df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                ypred_prob=classifier.predict_proba(xtest)[:,1]\n",
    "                fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                random_probs = [0 for i in range(len(ytest))]\n",
    "                p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                df_results.loc[count,'AUC']= auc_score\n",
    "                print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                ix = np.argmax(gmeans)\n",
    "                print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                # title\n",
    "                plt.title('ROC curve')\n",
    "                # x label\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                # y label\n",
    "                plt.ylabel('True Positive rate')\n",
    "\n",
    "                plt.legend(loc='best')\n",
    "\n",
    "                plt.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        #df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_CV_US.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain  , ytrain,xtest, ytest in cross_valid_method(x,y,split=split):\n",
    "                    from imblearn.over_sampling import RandomUnderSampler\n",
    "                    oversample = RandomUnderSampler(sampling_strategy=ratio)\n",
    "                    x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "                    mse,mae,rmse,r2,r2_adj=VotingReg(x_res, xtest, y_res, ytest, verbose=True, clf=None)\n",
    "                    s.append(mse)\n",
    "                    a.append(mae)\n",
    "                    rm.append(rmse)\n",
    "                    r.append(r2)\n",
    "                    ra.append(r2_adj)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "            else:\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain , ytrain,xtest, ytest in cross_valid_method(x,y,split=split):\n",
    "                    from imblearn.over_sampling import RandomUnderSampler\n",
    "                    oversample = RandomUnderSampler(sampling_strategy=ratio)\n",
    "                    x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "                    regressor.fit(x_res,y_res)\n",
    "                    y_pred=regressor.predict(xtest)\n",
    "                    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                    s.append(mse)\n",
    "                    mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                    a,append(mae)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    rm.append(rmse)\n",
    "                    r2=metrics.r2_score(ytest,y_pred)\n",
    "                    r.append(r2)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    \n",
    "                    n=len(xtrain)\n",
    "                    k=len(x.columns)\n",
    "                    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                    ra.append(r2_adj)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                \n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Algorithm with oversampling,FS and CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "def fit_model_CV_FS_OS(df,label,ratio=0.5,split=10,classification=True,cross_valid_method=cross_valid_stratified_kf,threshold=5,min_max=False,svm_type='Linear',kernel='poly'):\n",
    "    def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "    def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        if regression:\n",
    "            clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "        else:\n",
    "            clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        #validationmetrics(clf,testX,testY)\n",
    "        res = pd.Series(clf.feature_importances_, index=xtrain.columns.values).sort_values(ascending=False)*100\n",
    "        print(res)\n",
    "        return res\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "       # print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "       # print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "       # print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "       # print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "       # print('The adjusted R Squared Error is',r2_adj)\n",
    "        '''plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()'''\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        #print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        #print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        #print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        #print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        #print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        #print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        '''visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()'''\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report,fpr,tpr\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    #\"Decision Tree\": DecisionTreeClassifier(),\n",
    "    #\"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    #\"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    #'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    #'light Gradient Boosting':LGBMClassifier(),\n",
    "    #\"Gradient Boosting Classifier\": GradientBoostingClassifier()\n",
    "    }\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    if classification==True:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    else:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=True)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "   # for xtrain , xtest , ytrain, ytest in cross_valid_method(x,y,split=split):\n",
    "        \n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x2,y,test_size=0.20,random_state=42 )\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    oversample = RandomOverSampler(sampling_strategy=ratio)\n",
    "    x_res, y_res = oversample.fit_resample(xtrain, ytrain)    \n",
    "    \n",
    "    xtrain , xr , ytrain, yr =train_test_split(x_res,y_res,test_size=0.50,random_state=42 )\n",
    "    \n",
    "    xcv , x_test , ycv, y_test =train_test_split(xr,yr,test_size=0.40,random_state=42 )\n",
    "    print('_____Cross Validation_____')\n",
    "    if classification==True:\n",
    "        #df_results = pd.DataFrame(data=np.zeros(shape=(11,6)), columns = ['classifier','Accuracy','Precision','Recall','f1score','log_loss'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                accuracy=cross_val_score(clf, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(clf, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(clf, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(clf, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(clf, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "            else:\n",
    "                accuracy=cross_val_score(classifier, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(classifier, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(classifier, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(classifier, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(classifier, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                \n",
    "            count+=1\n",
    "        #return df_results\n",
    "    print('______Test_______')\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                clf.fit(xtrain,ytrain)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "               # fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                \n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                if svm_type == \"Linear\":\n",
    "                    print('AUC score cannot be calculated for linear SVM')\n",
    "                else:\n",
    "                    \n",
    "                    ypred_prob=clf.predict_proba(xtest)[:,1]\n",
    "                    fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                    random_probs = [0 for i in range(len(ytest))]\n",
    "                    p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                    auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                    df_results.loc[count,'AUC']= auc_score\n",
    "                    print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                    ix = np.argmax(gmeans)\n",
    "                    print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                if svm_type == \"Linear\":\n",
    "                    pass\n",
    "                else:\n",
    "                    \n",
    "                    plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                    plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                    plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                    # title\n",
    "                    plt.title('ROC curve')\n",
    "                    # x label\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    # y label\n",
    "                    plt.ylabel('True Positive rate')\n",
    "\n",
    "                    plt.legend(loc='best')\n",
    "\n",
    "                    plt.show()\n",
    "            else:\n",
    "                classifier.fit(xtrain,ytrain)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                #fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                #print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "               # df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                ypred_prob=classifier.predict_proba(xtest)[:,1]\n",
    "                fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                random_probs = [0 for i in range(len(ytest))]\n",
    "                p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                df_results.loc[count,'AUC']= auc_score\n",
    "                print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                ix = np.argmax(gmeans)\n",
    "                print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                # title\n",
    "                plt.title('ROC curve')\n",
    "                # x label\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                # y label\n",
    "                plt.ylabel('True Positive rate')\n",
    "\n",
    "                plt.legend(loc='best')\n",
    "\n",
    "                plt.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        #df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_CV_OS_FS.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.item():\n",
    "            if key=='Voting Regression':\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain  , ytrain,xtest, ytest in cross_valid_method(x2,y,split=split):\n",
    "                    mse,mae,rmse,r2,r2_adj=VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None)\n",
    "                    s.append(mse)\n",
    "                    a.append(mae)\n",
    "                    rm.append(rmse)\n",
    "                    r.append(r2)\n",
    "                    ra.append(r2_adj)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "            else:\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain, ytrain,xtest, ytest in cross_valid_method(x2,y,split=split):\n",
    "                    \n",
    "                    regressor.fit(xtrain,ytrain)\n",
    "                    y_pred=regressor.predict(xtest)\n",
    "                    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                    s.append(mse)\n",
    "                    mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                    a,append(mae)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    rm.append(rmse)\n",
    "                    r2=metrics.r2_score(ytest,y_pred)\n",
    "                    r.append(r2)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    \n",
    "                    n=len(xtrain)\n",
    "                    k=len(x.columns)\n",
    "                    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                    ra.append(r2_adj)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                \n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Algorithm with undersampling,FS and CV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "def fit_model_CV_FS_US(df,label,ratio=0.5,split=10,classification=True,cross_valid_method=cross_valid_stratified_kf,threshold=5,min_max=False,svm_type='Linear',kernel='poly'):\n",
    "    def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "    def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        if regression:\n",
    "            clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "        else:\n",
    "            clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        #validationmetrics(clf,testX,testY)\n",
    "        res = pd.Series(clf.feature_importances_, index=xtrain.columns.values).sort_values(ascending=False)*100\n",
    "        print(res)\n",
    "        return res\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "       # print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "       # print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "       # print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "       # print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "       # print('The adjusted R Squared Error is',r2_adj)\n",
    "        '''plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()'''\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        #print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        #print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        #print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        #print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        #print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        #print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        '''visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()'''\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report,fpr,tpr\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    #\"Decision Tree\": DecisionTreeClassifier(),\n",
    "    #\"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    #\"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    #'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    #'light Gradient Boosting':LGBMClassifier(),\n",
    "    #\"Gradient Boosting Classifier\": GradientBoostingClassifier()\n",
    "    }\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    if classification==True:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    else:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=True)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "   # for xtrain , xtest , ytrain, ytest in cross_valid_method(x,y,split=split):\n",
    "        \n",
    "    \n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x2,y,test_size=0.20,random_state=42 )\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    undersample = RandomUnderSampler(sampling_strategy=ratio)\n",
    "    x_res, y_res = undersample.fit_resample(xtrain, ytrain)   \n",
    "    \n",
    "    xtrain , xr , ytrain, yr =train_test_split(x_res,y_res,test_size=0.50,random_state=42 )\n",
    "    \n",
    "    xcv , x_test , ycv, y_test =train_test_split(xr,yr,test_size=0.40,random_state=42 )\n",
    "    print('_____Cross Validation_____')\n",
    "    if classification==True:\n",
    "        #df_results = pd.DataFrame(data=np.zeros(shape=(11,6)), columns = ['classifier','Accuracy','Precision','Recall','f1score','log_loss'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                accuracy=cross_val_score(clf, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(clf, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(clf, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(clf, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(clf, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "            else:\n",
    "                accuracy=cross_val_score(classifier, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(classifier, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(classifier, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(classifier, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(classifier, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                \n",
    "            count+=1\n",
    "        #return df_results\n",
    "    print('______Test_______')\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(probability=True,kernel=kernel)\n",
    "                clf.fit(xtrain,ytrain)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "               # fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                \n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                if svm_type == \"Linear\":\n",
    "                    print('AUC score cannot be calculated for linear SVM')\n",
    "                else:\n",
    "                    \n",
    "                    ypred_prob=clf.predict_proba(xtest)[:,1]\n",
    "                    fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                    random_probs = [0 for i in range(len(ytest))]\n",
    "                    p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                    auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                    df_results.loc[count,'AUC']= auc_score\n",
    "                    print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                    ix = np.argmax(gmeans)\n",
    "                    print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                if svm_type == \"Linear\":\n",
    "                    pass\n",
    "                else:\n",
    "                    \n",
    "                    plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                    plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                    plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                    # title\n",
    "                    plt.title('ROC curve')\n",
    "                    # x label\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    # y label\n",
    "                    plt.ylabel('True Positive rate')\n",
    "\n",
    "                    plt.legend(loc='best')\n",
    "\n",
    "                    plt.show()\n",
    "            else:\n",
    "                classifier.fit(xtrain,ytrain)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                #fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                #print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                #AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "               # df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "                ypred_prob=classifier.predict_proba(xtest)[:,1]\n",
    "                fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "                random_probs = [0 for i in range(len(ytest))]\n",
    "                p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "                auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "                df_results.loc[count,'AUC']= auc_score\n",
    "                print(\"AUC  (%): \\n\",auc_score*100)\n",
    "                gmeans = np.sqrt(tpr * (1-fpr))\n",
    "                ix = np.argmax(gmeans)\n",
    "                print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "                plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "                plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "                # title\n",
    "                plt.title('ROC curve')\n",
    "                # x label\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                # y label\n",
    "                plt.ylabel('True Positive rate')\n",
    "\n",
    "                plt.legend(loc='best')\n",
    "\n",
    "                plt.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        #df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_CV_US_FS.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain  , ytrain,xtest, ytest in cross_valid_method(x2,y,split=split):\n",
    "                    mse,mae,rmse,r2,r2_adj=VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None)\n",
    "                    s.append(mse)\n",
    "                    a.append(mae)\n",
    "                    rm.append(rmse)\n",
    "                    r.append(r2)\n",
    "                    ra.append(r2_adj)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "            else:\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain, ytrain,xtest, ytest in cross_valid_method(x2,y,split=split):\n",
    "                    \n",
    "                    regressor.fit(xtrain,ytrain)\n",
    "                    y_pred=regressor.predict(xtest)\n",
    "                    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                    s.append(mse)\n",
    "                    mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                    a,append(mae)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    rm.append(rmse)\n",
    "                    r2=metrics.r2_score(ytest,y_pred)\n",
    "                    r.append(r2)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    \n",
    "                    n=len(xtrain)\n",
    "                    k=len(x.columns)\n",
    "                    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                    ra.append(r2_adj)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                \n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_classifier(df,label,kernel='poly' ,svmtype=\"SVC\",over_sample=False,under_sample=False,name=None,clf=None,min_max=False):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn import svm\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        if svmtype == \"Linear\":\n",
    "            clf = svm.LinearSVC()\n",
    "        else:\n",
    "            clf = svm.SVC(probability=True,kernel=kernel)\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    if over_sample==True:\n",
    "        from imblearn.over_sampling import RandomOverSampler\n",
    "        oversample = RandomOverSampler(sampling_strategy=0.5)\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.50,random_state=42 )\n",
    "        x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "        x_train=x_res\n",
    "        y_train=y_res\n",
    "    elif under_sample==True:\n",
    "        from imblearn.under_sampling import RandomUnderSampler\n",
    "        undersample = RandomUnderSampler(sampling_strategy=0.5)\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.50,random_state=42 )\n",
    "        x_res, y_res = undersample.fit_resample(xtrain, ytrain)\n",
    "        x_train=x_res\n",
    "        y_train=y_res\n",
    "    else:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.50,random_state=42 )\n",
    "\n",
    "    clf.fit(xtrain,ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    accuracy=accuracy_score(ytest, y_pred)*100\n",
    "    #fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    #print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    #AUC=auc(fpr, tpr)*100\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    precision=precision_score(ytest, y_pred)*100\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    recall=recall_score(ytest,y_pred)*100\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    f1=f1_score(ytest,y_pred)*100\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    log=log_loss(ytest,y_pred)\n",
    "    print(classification_report(ytest,y_pred))\n",
    "    class_report=classification_report(ytest,y_pred)\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    '''visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()'''\n",
    "    import pickle\n",
    "    filename = name\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    if svm_type == \"Linear\":\n",
    "        print('AUC score cannot be calculated for linear SVM')\n",
    "    else:\n",
    "\n",
    "        ypred_prob=clf.predict_proba(xtest)[:,1]\n",
    "        fpr, tpr, thresh = roc_curve(ytest, ypred_prob, pos_label=1)\n",
    "        random_probs = [0 for i in range(len(ytest))]\n",
    "        p_fpr, p_tpr, _ = roc_curve(ytest, random_probs, pos_label=1)\n",
    "        auc_score = roc_auc_score(ytest, ypred_prob)\n",
    "       \n",
    "        print(\"AUC  (%): \\n\",auc_score*100)\n",
    "        gmeans = np.sqrt(tpr * (1-fpr))\n",
    "        ix = np.argmax(gmeans)\n",
    "        print('Best Threshold=%f, G-Mean=%.3f' % (thresh[ix], gmeans[ix]))\n",
    "\n",
    "    #df_results.loc[count,'f1score'] = f1\n",
    "    #df_results.loc[count,'log_loss'] = log\n",
    "    #print(class_report)\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    if svm_type == \"Linear\":\n",
    "        pass\n",
    "    else:\n",
    "\n",
    "        plt.plot(fpr, tpr,label=key)\n",
    "\n",
    "        plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "        plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "        # title\n",
    "        plt.title('ROC curve')\n",
    "        # x label\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        # y label\n",
    "        plt.ylabel('True Positive rate')\n",
    "\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.show()\n",
    "    return accuracy,auc_score,precision,recall,f1,log,class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter(df,column1,column2):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    plt.subplots(figsize=(12,12))\n",
    "    sns.scatterplot(df[column1],df[column2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_plot_kmeans(df3,yellow=True):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "    from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "    if yellow==True:\n",
    "        model = KMeans()\n",
    "        visualizer = KElbowVisualizer(model, k=(4,12))\n",
    "\n",
    "        visualizer.fit(df3)       \n",
    "        visualizer.show()    \n",
    "    else:\n",
    "        Sum_of_squared_distances = []\n",
    "        K = range(1,21)\n",
    "        for k in K:\n",
    "            km = KMeans(n_clusters=k)\n",
    "            km = km.fit(df3)\n",
    "            Sum_of_squared_distances.append(km.inertia_)\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel('Sum_of_squared_distances')\n",
    "        plt.title('Elbow Method For Optimal k')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(n,data):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.cluster import KMeans\n",
    "    model = KMeans(n_clusters=n)\n",
    "    model.fit(data)\n",
    "    kmeans_labels = model.labels_\n",
    "    return model,kmeans_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SI_score(data,labels):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    print('Silhouette score: {}'.format(silhouette_score(data, labels, \n",
    "                                           metric='euclidean')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_viz(df,label):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.manifold import TSNE\n",
    "    t=TSNE(n_components=2)\n",
    "    X_2d = t.fit_transform(df)\n",
    "    sns.scatterplot(X_2d[:,0], X_2d[:,1],hue=label)\n",
    "    plt.show()\n",
    "    t2=TSNE(n_components=3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_viz3d(df,label):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.manifold import TSNE\n",
    "    t2=TSNE(n_components=3)\n",
    "    X_3d = t2.fit_transform(df)\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    fig = plt.figure(num=None, figsize=(15, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "    ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "    ax.scatter3D(X_3d[:,0], X_3d[:,1], X_3d[:,2],c=label, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dendrogram(df2):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import scipy.cluster.hierarchy as sch\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    dendrogram = sch.dendrogram(sch.linkage(df2, method='complete'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_clustering(n):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    model = AgglomerativeClustering(n_clusters=n) \n",
    "    model.fit(df2)\n",
    "    hac_labels = model.labels_\n",
    "    return model,hac_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan(df2,eps=0.5,min_samples=5):\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    # Compute DBSCAN\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    db.fit(df2)\n",
    "    dblabels = db.labels_\n",
    "    return db,dblabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanshift(df2,quantile=0.3):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "    from sklearn import cluster\n",
    "\n",
    "    bandwidth=estimate_bandwidth(df2, quantile=quantile)\n",
    "\n",
    "    ms = MeanShift(bandwidth = bandwidth) \n",
    "    ms.fit(df2)\n",
    "    mslabels = ms.labels_\n",
    "    return ms,mslabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optics(df2):\n",
    "    from sklearn.cluster import OPTICS\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    op = OPTICS()\n",
    "    op.fit(df2)\n",
    "    oplabels= op.labels_\n",
    "    return op,oplabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def birch(df2):\n",
    "    from sklearn.cluster import Birch\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    bi = Birch()\n",
    "    bi.fit(df2)\n",
    "    bilabels= bi.labels_\n",
    "    return bi,bilabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap(df2):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.cluster import AffinityPropagation\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "    preference=euclidean_distances(df2, df2).max()\n",
    "    af = AffinityPropagation()\n",
    "    clustering = af.fit(df2)\n",
    "    af.get_params()\n",
    "    aflabels = af.predict(df2)\n",
    "    return af,aflabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm(df2,n):\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    gmm = GaussianMixture(n_components=n, covariance_type='full', random_state=42 )\n",
    "    gmm.fit(df2)\n",
    "    gmmlabels = gmm.predict(df2)\n",
    "    return gmm,gmmlabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ppc(model,trace):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pymc3 as pm\n",
    "    import arviz as az\n",
    "    prior_predictive = pm.sample_prior_predictive(model=model)\n",
    "\n",
    "    posterior_predictive = pm.sample_posterior_predictive(model=model,trace=trace)\n",
    "    dataset = az.from_pymc3(trace=trace, posterior_predictive=posterior_predictive,prior=prior_predictive)\n",
    "    az.plot_ppc(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(model,trace,xtest,ytest):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pymc3 as pm\n",
    "    import arviz as az\n",
    "    with model:\n",
    "        pm.set_data({'x': xtest})\n",
    "        y_pred = pm.sample_posterior_predictive(trace)\n",
    "    predictions=np.mean(y_pred['yl'],axis=0)\n",
    "    predictions2=[]\n",
    "    for i in predictions:\n",
    "        if i>0.5:\n",
    "            predictions2.append(1)\n",
    "        else:\n",
    "            predictions2.append(0)\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(ytest, predictions2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
